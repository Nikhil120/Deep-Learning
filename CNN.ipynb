{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"datasets/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'X:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input variable\n",
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.placeholder(shape=[None, 10], dtype=tf.float32, name=\"y\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = tf.placeholder(dtype=tf.float32, name=\"prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv1/Relu:0' shape=(?, 28, 28, 64) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first convolutional layer\n",
    "conv1 = tf.layers.conv2d(X, filters=64, kernel_size=3, strides=1, padding=\"same\", activation=tf.nn.relu, name=\"conv1\")\n",
    "conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pool1/MaxPool:0' shape=(?, 14, 14, 64) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first pooling layer\n",
    "pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], padding=\"same\", strides=2, name=\"pool1\")\n",
    "pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2/Relu:0' shape=(?, 14, 14, 128) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second convolutional layer\n",
    "conv2 = tf.layers.conv2d(pool1, filters=128, kernel_size=3, strides=1, padding=\"same\", activation=tf.nn.relu, name=\"conv2\")\n",
    "conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pool2/MaxPool:0' shape=(?, 7, 7, 128) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second pooling layer\n",
    "pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=2, padding=\"same\", name=\"pool2\")\n",
    "pool2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv3/Relu:0' shape=(?, 7, 7, 256) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third convolutional layer\n",
    "conv3 = tf.layers.conv2d(pool2, filters=256, kernel_size=3, strides=1, padding=\"same\", activation=tf.nn.relu, name=\"conv3\")\n",
    "conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pool3/MaxPool:0' shape=(?, 4, 4, 256) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third pooling layer\n",
    "pool3 = tf.layers.max_pooling2d(conv3, pool_size=[2, 2], strides=2, padding=\"same\", name=\"pool3\")\n",
    "pool3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pool2_flat:0' shape=(?, 4096) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense layer\n",
    "pool2_flat = tf.reshape(pool3, [-1, 4 * 4 * 256], name=\"pool2_flat\")\n",
    "pool2_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Relu:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = tf.layers.dense(pool2_flat, units=1024, activation=tf.nn.relu, name=\"dense\")\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout/Identity:0' shape=(?, 1024) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = tf.layers.dropout(dense, prob, name=\"dropout\")\n",
    "dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_2/BiasAdd:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.layers.dense(dropout, 10)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y_pred:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tf.nn.softmax(logits, name=\"y_pred\")\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training operation\n",
    "training_op = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize and save\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/1718 (0.1%)  Accuracy:15.62%  Loss: 2.302060\n",
      "Iteration: 2/1718 (0.1%)  Accuracy:6.25%  Loss: 2.453395\n",
      "Iteration: 3/1718 (0.2%)  Accuracy:37.50%  Loss: 2.250383\n",
      "Iteration: 4/1718 (0.2%)  Accuracy:12.50%  Loss: 2.213284\n",
      "Iteration: 5/1718 (0.3%)  Accuracy:9.38%  Loss: 2.252246\n",
      "Iteration: 6/1718 (0.3%)  Accuracy:21.88%  Loss: 2.253464\n",
      "Iteration: 7/1718 (0.4%)  Accuracy:21.88%  Loss: 2.244262\n",
      "Iteration: 8/1718 (0.5%)  Accuracy:37.50%  Loss: 2.127113\n",
      "Iteration: 9/1718 (0.5%)  Accuracy:53.12%  Loss: 2.048895\n",
      "Iteration: 10/1718 (0.6%)  Accuracy:46.88%  Loss: 1.881180\n",
      "Iteration: 11/1718 (0.6%)  Accuracy:50.00%  Loss: 1.792043\n",
      "Iteration: 12/1718 (0.7%)  Accuracy:50.00%  Loss: 1.634714\n",
      "Iteration: 13/1718 (0.8%)  Accuracy:53.12%  Loss: 1.634261\n",
      "Iteration: 14/1718 (0.8%)  Accuracy:34.38%  Loss: 1.472763\n",
      "Iteration: 15/1718 (0.9%)  Accuracy:62.50%  Loss: 1.130898\n",
      "Iteration: 16/1718 (0.9%)  Accuracy:81.25%  Loss: 0.782785\n",
      "Iteration: 17/1718 (1.0%)  Accuracy:65.62%  Loss: 1.095692\n",
      "Iteration: 18/1718 (1.0%)  Accuracy:71.88%  Loss: 0.819811\n",
      "Iteration: 19/1718 (1.1%)  Accuracy:81.25%  Loss: 0.641495\n",
      "Iteration: 20/1718 (1.2%)  Accuracy:71.88%  Loss: 0.933875\n",
      "Iteration: 21/1718 (1.2%)  Accuracy:87.50%  Loss: 0.568724\n",
      "Iteration: 22/1718 (1.3%)  Accuracy:65.62%  Loss: 1.193658\n",
      "Iteration: 23/1718 (1.3%)  Accuracy:75.00%  Loss: 0.843501\n",
      "Iteration: 24/1718 (1.4%)  Accuracy:78.12%  Loss: 0.699040\n",
      "Iteration: 25/1718 (1.5%)  Accuracy:87.50%  Loss: 0.437727\n",
      "Iteration: 26/1718 (1.5%)  Accuracy:78.12%  Loss: 0.457292\n",
      "Iteration: 27/1718 (1.6%)  Accuracy:81.25%  Loss: 0.738010\n",
      "Iteration: 28/1718 (1.6%)  Accuracy:68.75%  Loss: 0.784270\n",
      "Iteration: 29/1718 (1.7%)  Accuracy:93.75%  Loss: 0.241093\n",
      "Iteration: 30/1718 (1.7%)  Accuracy:84.38%  Loss: 0.582722\n",
      "Iteration: 31/1718 (1.8%)  Accuracy:90.62%  Loss: 0.406086\n",
      "Iteration: 32/1718 (1.9%)  Accuracy:87.50%  Loss: 0.366328\n",
      "Iteration: 33/1718 (1.9%)  Accuracy:81.25%  Loss: 0.461668\n",
      "Iteration: 34/1718 (2.0%)  Accuracy:84.38%  Loss: 0.583921\n",
      "Iteration: 35/1718 (2.0%)  Accuracy:75.00%  Loss: 0.918936\n",
      "Iteration: 36/1718 (2.1%)  Accuracy:96.88%  Loss: 0.219970\n",
      "Iteration: 37/1718 (2.2%)  Accuracy:84.38%  Loss: 0.562666\n",
      "Iteration: 38/1718 (2.2%)  Accuracy:93.75%  Loss: 0.299168\n",
      "Iteration: 39/1718 (2.3%)  Accuracy:93.75%  Loss: 0.333830\n",
      "Iteration: 40/1718 (2.3%)  Accuracy:87.50%  Loss: 0.377561\n",
      "Iteration: 41/1718 (2.4%)  Accuracy:87.50%  Loss: 0.422612\n",
      "Iteration: 42/1718 (2.4%)  Accuracy:87.50%  Loss: 0.416205\n",
      "Iteration: 43/1718 (2.5%)  Accuracy:90.62%  Loss: 0.255162\n",
      "Iteration: 44/1718 (2.6%)  Accuracy:90.62%  Loss: 0.299167\n",
      "Iteration: 45/1718 (2.6%)  Accuracy:84.38%  Loss: 0.323033\n",
      "Iteration: 46/1718 (2.7%)  Accuracy:81.25%  Loss: 0.453821\n",
      "Iteration: 47/1718 (2.7%)  Accuracy:84.38%  Loss: 0.632837\n",
      "Iteration: 48/1718 (2.8%)  Accuracy:90.62%  Loss: 0.366011\n",
      "Iteration: 49/1718 (2.9%)  Accuracy:87.50%  Loss: 0.257538\n",
      "Iteration: 50/1718 (2.9%)  Accuracy:90.62%  Loss: 0.289479\n",
      "Iteration: 51/1718 (3.0%)  Accuracy:96.88%  Loss: 0.141499\n",
      "Iteration: 52/1718 (3.0%)  Accuracy:93.75%  Loss: 0.182892\n",
      "Iteration: 53/1718 (3.1%)  Accuracy:93.75%  Loss: 0.191213\n",
      "Iteration: 54/1718 (3.1%)  Accuracy:87.50%  Loss: 0.286454\n",
      "Iteration: 55/1718 (3.2%)  Accuracy:93.75%  Loss: 0.131508\n",
      "Iteration: 56/1718 (3.3%)  Accuracy:90.62%  Loss: 0.211742\n",
      "Iteration: 57/1718 (3.3%)  Accuracy:84.38%  Loss: 0.358540\n",
      "Iteration: 58/1718 (3.4%)  Accuracy:87.50%  Loss: 0.447292\n",
      "Iteration: 59/1718 (3.4%)  Accuracy:100.00%  Loss: 0.067459\n",
      "Iteration: 60/1718 (3.5%)  Accuracy:93.75%  Loss: 0.161921\n",
      "Iteration: 61/1718 (3.6%)  Accuracy:93.75%  Loss: 0.186880\n",
      "Iteration: 62/1718 (3.6%)  Accuracy:90.62%  Loss: 0.312435\n",
      "Iteration: 63/1718 (3.7%)  Accuracy:87.50%  Loss: 0.417876\n",
      "Iteration: 64/1718 (3.7%)  Accuracy:96.88%  Loss: 0.141446\n",
      "Iteration: 65/1718 (3.8%)  Accuracy:93.75%  Loss: 0.154677\n",
      "Iteration: 66/1718 (3.8%)  Accuracy:90.62%  Loss: 0.633865\n",
      "Iteration: 67/1718 (3.9%)  Accuracy:100.00%  Loss: 0.116684\n",
      "Iteration: 68/1718 (4.0%)  Accuracy:96.88%  Loss: 0.152838\n",
      "Iteration: 69/1718 (4.0%)  Accuracy:100.00%  Loss: 0.061671\n",
      "Iteration: 70/1718 (4.1%)  Accuracy:93.75%  Loss: 0.161593\n",
      "Iteration: 71/1718 (4.1%)  Accuracy:90.62%  Loss: 0.151239\n",
      "Iteration: 72/1718 (4.2%)  Accuracy:100.00%  Loss: 0.057557\n",
      "Iteration: 73/1718 (4.2%)  Accuracy:87.50%  Loss: 0.228625\n",
      "Iteration: 74/1718 (4.3%)  Accuracy:96.88%  Loss: 0.179914\n",
      "Iteration: 75/1718 (4.4%)  Accuracy:90.62%  Loss: 0.196688\n",
      "Iteration: 76/1718 (4.4%)  Accuracy:96.88%  Loss: 0.139375\n",
      "Iteration: 77/1718 (4.5%)  Accuracy:93.75%  Loss: 0.157707\n",
      "Iteration: 78/1718 (4.5%)  Accuracy:93.75%  Loss: 0.190428\n",
      "Iteration: 79/1718 (4.6%)  Accuracy:93.75%  Loss: 0.134198\n",
      "Iteration: 80/1718 (4.7%)  Accuracy:96.88%  Loss: 0.273728\n",
      "Iteration: 81/1718 (4.7%)  Accuracy:96.88%  Loss: 0.140169\n",
      "Iteration: 82/1718 (4.8%)  Accuracy:100.00%  Loss: 0.041207\n",
      "Iteration: 83/1718 (4.8%)  Accuracy:93.75%  Loss: 0.164662\n",
      "Iteration: 84/1718 (4.9%)  Accuracy:96.88%  Loss: 0.083877\n",
      "Iteration: 85/1718 (4.9%)  Accuracy:93.75%  Loss: 0.153572\n",
      "Iteration: 86/1718 (5.0%)  Accuracy:87.50%  Loss: 0.487141\n",
      "Iteration: 87/1718 (5.1%)  Accuracy:90.62%  Loss: 0.331975\n",
      "Iteration: 88/1718 (5.1%)  Accuracy:87.50%  Loss: 0.291242\n",
      "Iteration: 89/1718 (5.2%)  Accuracy:90.62%  Loss: 0.123944\n",
      "Iteration: 90/1718 (5.2%)  Accuracy:100.00%  Loss: 0.042050\n",
      "Iteration: 91/1718 (5.3%)  Accuracy:100.00%  Loss: 0.039321\n",
      "Iteration: 92/1718 (5.4%)  Accuracy:96.88%  Loss: 0.066845\n",
      "Iteration: 93/1718 (5.4%)  Accuracy:100.00%  Loss: 0.023529\n",
      "Iteration: 94/1718 (5.5%)  Accuracy:93.75%  Loss: 0.180061\n",
      "Iteration: 95/1718 (5.5%)  Accuracy:90.62%  Loss: 0.345280\n",
      "Iteration: 96/1718 (5.6%)  Accuracy:93.75%  Loss: 0.241852\n",
      "Iteration: 97/1718 (5.6%)  Accuracy:96.88%  Loss: 0.073905\n",
      "Iteration: 98/1718 (5.7%)  Accuracy:93.75%  Loss: 0.270797\n",
      "Iteration: 99/1718 (5.8%)  Accuracy:96.88%  Loss: 0.128715\n",
      "Iteration: 100/1718 (5.8%)  Accuracy:84.38%  Loss: 0.433395\n",
      "Iteration: 101/1718 (5.9%)  Accuracy:100.00%  Loss: 0.018394\n",
      "Iteration: 102/1718 (5.9%)  Accuracy:96.88%  Loss: 0.098432\n",
      "Iteration: 103/1718 (6.0%)  Accuracy:93.75%  Loss: 0.182279\n",
      "Iteration: 104/1718 (6.1%)  Accuracy:84.38%  Loss: 0.479678\n",
      "Iteration: 105/1718 (6.1%)  Accuracy:90.62%  Loss: 0.217895\n",
      "Iteration: 106/1718 (6.2%)  Accuracy:87.50%  Loss: 0.235350\n",
      "Iteration: 107/1718 (6.2%)  Accuracy:87.50%  Loss: 0.642990\n",
      "Iteration: 108/1718 (6.3%)  Accuracy:93.75%  Loss: 0.249378\n",
      "Iteration: 109/1718 (6.3%)  Accuracy:84.38%  Loss: 0.418084\n",
      "Iteration: 110/1718 (6.4%)  Accuracy:96.88%  Loss: 0.148568\n",
      "Iteration: 111/1718 (6.5%)  Accuracy:96.88%  Loss: 0.098379\n",
      "Iteration: 112/1718 (6.5%)  Accuracy:90.62%  Loss: 0.265665\n",
      "Iteration: 113/1718 (6.6%)  Accuracy:96.88%  Loss: 0.098713\n",
      "Iteration: 114/1718 (6.6%)  Accuracy:93.75%  Loss: 0.213091\n",
      "Iteration: 115/1718 (6.7%)  Accuracy:93.75%  Loss: 0.146642\n",
      "Iteration: 116/1718 (6.8%)  Accuracy:87.50%  Loss: 0.354495\n",
      "Iteration: 117/1718 (6.8%)  Accuracy:84.38%  Loss: 0.211840\n",
      "Iteration: 118/1718 (6.9%)  Accuracy:93.75%  Loss: 0.294091\n",
      "Iteration: 119/1718 (6.9%)  Accuracy:93.75%  Loss: 0.289906\n",
      "Iteration: 120/1718 (7.0%)  Accuracy:90.62%  Loss: 0.262346\n",
      "Iteration: 121/1718 (7.0%)  Accuracy:96.88%  Loss: 0.175660\n",
      "Iteration: 122/1718 (7.1%)  Accuracy:93.75%  Loss: 0.312362\n",
      "Iteration: 123/1718 (7.2%)  Accuracy:96.88%  Loss: 0.230602\n",
      "Iteration: 124/1718 (7.2%)  Accuracy:100.00%  Loss: 0.031668\n",
      "Iteration: 125/1718 (7.3%)  Accuracy:93.75%  Loss: 0.255964\n",
      "Iteration: 126/1718 (7.3%)  Accuracy:100.00%  Loss: 0.067346\n",
      "Iteration: 127/1718 (7.4%)  Accuracy:93.75%  Loss: 0.183873\n",
      "Iteration: 128/1718 (7.5%)  Accuracy:87.50%  Loss: 0.374749\n",
      "Iteration: 129/1718 (7.5%)  Accuracy:93.75%  Loss: 0.317337\n",
      "Iteration: 130/1718 (7.6%)  Accuracy:87.50%  Loss: 0.364745\n",
      "Iteration: 131/1718 (7.6%)  Accuracy:96.88%  Loss: 0.140893\n",
      "Iteration: 132/1718 (7.7%)  Accuracy:100.00%  Loss: 0.055408\n",
      "Iteration: 133/1718 (7.7%)  Accuracy:100.00%  Loss: 0.015051\n",
      "Iteration: 134/1718 (7.8%)  Accuracy:93.75%  Loss: 0.157396\n",
      "Iteration: 135/1718 (7.9%)  Accuracy:93.75%  Loss: 0.161461\n",
      "Iteration: 136/1718 (7.9%)  Accuracy:93.75%  Loss: 0.193227\n",
      "Iteration: 137/1718 (8.0%)  Accuracy:96.88%  Loss: 0.150144\n",
      "Iteration: 138/1718 (8.0%)  Accuracy:96.88%  Loss: 0.059895\n",
      "Iteration: 139/1718 (8.1%)  Accuracy:100.00%  Loss: 0.042535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 140/1718 (8.1%)  Accuracy:90.62%  Loss: 0.375120\n",
      "Iteration: 141/1718 (8.2%)  Accuracy:96.88%  Loss: 0.093316\n",
      "Iteration: 142/1718 (8.3%)  Accuracy:96.88%  Loss: 0.090609\n",
      "Iteration: 143/1718 (8.3%)  Accuracy:100.00%  Loss: 0.019166\n",
      "Iteration: 144/1718 (8.4%)  Accuracy:100.00%  Loss: 0.016287\n",
      "Iteration: 145/1718 (8.4%)  Accuracy:93.75%  Loss: 0.185084\n",
      "Iteration: 146/1718 (8.5%)  Accuracy:96.88%  Loss: 0.164537\n",
      "Iteration: 147/1718 (8.6%)  Accuracy:96.88%  Loss: 0.075493\n",
      "Iteration: 148/1718 (8.6%)  Accuracy:93.75%  Loss: 0.159070\n",
      "Iteration: 149/1718 (8.7%)  Accuracy:96.88%  Loss: 0.133236\n",
      "Iteration: 150/1718 (8.7%)  Accuracy:96.88%  Loss: 0.107008\n",
      "Iteration: 151/1718 (8.8%)  Accuracy:100.00%  Loss: 0.042334\n",
      "Iteration: 152/1718 (8.8%)  Accuracy:96.88%  Loss: 0.167820\n",
      "Iteration: 153/1718 (8.9%)  Accuracy:90.62%  Loss: 0.226575\n",
      "Iteration: 154/1718 (9.0%)  Accuracy:100.00%  Loss: 0.047141\n",
      "Iteration: 155/1718 (9.0%)  Accuracy:93.75%  Loss: 0.150210\n",
      "Iteration: 156/1718 (9.1%)  Accuracy:100.00%  Loss: 0.090271\n",
      "Iteration: 157/1718 (9.1%)  Accuracy:87.50%  Loss: 0.409344\n",
      "Iteration: 158/1718 (9.2%)  Accuracy:93.75%  Loss: 0.212730\n",
      "Iteration: 159/1718 (9.3%)  Accuracy:100.00%  Loss: 0.027011\n",
      "Iteration: 160/1718 (9.3%)  Accuracy:96.88%  Loss: 0.080654\n",
      "Iteration: 161/1718 (9.4%)  Accuracy:100.00%  Loss: 0.032036\n",
      "Iteration: 162/1718 (9.4%)  Accuracy:96.88%  Loss: 0.203989\n",
      "Iteration: 163/1718 (9.5%)  Accuracy:100.00%  Loss: 0.013953\n",
      "Iteration: 164/1718 (9.5%)  Accuracy:100.00%  Loss: 0.046735\n",
      "Iteration: 165/1718 (9.6%)  Accuracy:93.75%  Loss: 0.161779\n",
      "Iteration: 166/1718 (9.7%)  Accuracy:100.00%  Loss: 0.049117\n",
      "Iteration: 167/1718 (9.7%)  Accuracy:96.88%  Loss: 0.085053\n",
      "Iteration: 168/1718 (9.8%)  Accuracy:96.88%  Loss: 0.085613\n",
      "Iteration: 169/1718 (9.8%)  Accuracy:93.75%  Loss: 0.206626\n",
      "Iteration: 170/1718 (9.9%)  Accuracy:100.00%  Loss: 0.046679\n",
      "Iteration: 171/1718 (10.0%)  Accuracy:100.00%  Loss: 0.030036\n",
      "Iteration: 172/1718 (10.0%)  Accuracy:96.88%  Loss: 0.049864\n",
      "Iteration: 173/1718 (10.1%)  Accuracy:93.75%  Loss: 0.105637\n",
      "Iteration: 174/1718 (10.1%)  Accuracy:100.00%  Loss: 0.005326\n",
      "Iteration: 175/1718 (10.2%)  Accuracy:93.75%  Loss: 0.115237\n",
      "Iteration: 176/1718 (10.2%)  Accuracy:93.75%  Loss: 0.239585\n",
      "Iteration: 177/1718 (10.3%)  Accuracy:100.00%  Loss: 0.018326\n",
      "Iteration: 178/1718 (10.4%)  Accuracy:96.88%  Loss: 0.066294\n",
      "Iteration: 179/1718 (10.4%)  Accuracy:100.00%  Loss: 0.057791\n",
      "Iteration: 180/1718 (10.5%)  Accuracy:93.75%  Loss: 0.217956\n",
      "Iteration: 181/1718 (10.5%)  Accuracy:93.75%  Loss: 0.059091\n",
      "Iteration: 182/1718 (10.6%)  Accuracy:93.75%  Loss: 0.161485\n",
      "Iteration: 183/1718 (10.7%)  Accuracy:90.62%  Loss: 0.181328\n",
      "Iteration: 184/1718 (10.7%)  Accuracy:96.88%  Loss: 0.156549\n",
      "Iteration: 185/1718 (10.8%)  Accuracy:93.75%  Loss: 0.249323\n",
      "Iteration: 186/1718 (10.8%)  Accuracy:96.88%  Loss: 0.047873\n",
      "Iteration: 187/1718 (10.9%)  Accuracy:90.62%  Loss: 0.404203\n",
      "Iteration: 188/1718 (10.9%)  Accuracy:93.75%  Loss: 0.182420\n",
      "Iteration: 189/1718 (11.0%)  Accuracy:100.00%  Loss: 0.012392\n",
      "Iteration: 190/1718 (11.1%)  Accuracy:93.75%  Loss: 0.086463\n",
      "Iteration: 191/1718 (11.1%)  Accuracy:93.75%  Loss: 0.117352\n",
      "Iteration: 192/1718 (11.2%)  Accuracy:100.00%  Loss: 0.052842\n",
      "Iteration: 193/1718 (11.2%)  Accuracy:93.75%  Loss: 0.519039\n",
      "Iteration: 194/1718 (11.3%)  Accuracy:93.75%  Loss: 0.205054\n",
      "Iteration: 195/1718 (11.4%)  Accuracy:93.75%  Loss: 0.160793\n",
      "Iteration: 196/1718 (11.4%)  Accuracy:93.75%  Loss: 0.248416\n",
      "Iteration: 197/1718 (11.5%)  Accuracy:87.50%  Loss: 0.232943\n",
      "Iteration: 198/1718 (11.5%)  Accuracy:93.75%  Loss: 0.092918\n",
      "Iteration: 199/1718 (11.6%)  Accuracy:93.75%  Loss: 0.232049\n",
      "Iteration: 200/1718 (11.6%)  Accuracy:96.88%  Loss: 0.070200\n",
      "Iteration: 201/1718 (11.7%)  Accuracy:87.50%  Loss: 0.269802\n",
      "Iteration: 202/1718 (11.8%)  Accuracy:100.00%  Loss: 0.042774\n",
      "Iteration: 203/1718 (11.8%)  Accuracy:93.75%  Loss: 0.199736\n",
      "Iteration: 204/1718 (11.9%)  Accuracy:96.88%  Loss: 0.109826\n",
      "Iteration: 205/1718 (11.9%)  Accuracy:96.88%  Loss: 0.115120\n",
      "Iteration: 206/1718 (12.0%)  Accuracy:93.75%  Loss: 0.154266\n",
      "Iteration: 207/1718 (12.0%)  Accuracy:100.00%  Loss: 0.023500\n",
      "Iteration: 208/1718 (12.1%)  Accuracy:90.62%  Loss: 0.302515\n",
      "Iteration: 209/1718 (12.2%)  Accuracy:96.88%  Loss: 0.179637\n",
      "Iteration: 210/1718 (12.2%)  Accuracy:96.88%  Loss: 0.117208\n",
      "Iteration: 211/1718 (12.3%)  Accuracy:93.75%  Loss: 0.357781\n",
      "Iteration: 212/1718 (12.3%)  Accuracy:100.00%  Loss: 0.041518\n",
      "Iteration: 213/1718 (12.4%)  Accuracy:90.62%  Loss: 0.213468\n",
      "Iteration: 214/1718 (12.5%)  Accuracy:96.88%  Loss: 0.089714\n",
      "Iteration: 215/1718 (12.5%)  Accuracy:96.88%  Loss: 0.104842\n",
      "Iteration: 216/1718 (12.6%)  Accuracy:93.75%  Loss: 0.182690\n",
      "Iteration: 217/1718 (12.6%)  Accuracy:100.00%  Loss: 0.021065\n",
      "Iteration: 218/1718 (12.7%)  Accuracy:100.00%  Loss: 0.051661\n",
      "Iteration: 219/1718 (12.7%)  Accuracy:96.88%  Loss: 0.079874\n",
      "Iteration: 220/1718 (12.8%)  Accuracy:90.62%  Loss: 0.182466\n",
      "Iteration: 221/1718 (12.9%)  Accuracy:100.00%  Loss: 0.040766\n",
      "Iteration: 222/1718 (12.9%)  Accuracy:96.88%  Loss: 0.086337\n",
      "Iteration: 223/1718 (13.0%)  Accuracy:100.00%  Loss: 0.021212\n",
      "Iteration: 224/1718 (13.0%)  Accuracy:96.88%  Loss: 0.127530\n",
      "Iteration: 225/1718 (13.1%)  Accuracy:96.88%  Loss: 0.223941\n",
      "Iteration: 226/1718 (13.2%)  Accuracy:96.88%  Loss: 0.061178\n",
      "Iteration: 227/1718 (13.2%)  Accuracy:96.88%  Loss: 0.049874\n",
      "Iteration: 228/1718 (13.3%)  Accuracy:93.75%  Loss: 0.241674\n",
      "Iteration: 229/1718 (13.3%)  Accuracy:96.88%  Loss: 0.171195\n",
      "Iteration: 230/1718 (13.4%)  Accuracy:100.00%  Loss: 0.010265\n",
      "Iteration: 231/1718 (13.4%)  Accuracy:100.00%  Loss: 0.031727\n",
      "Iteration: 232/1718 (13.5%)  Accuracy:96.88%  Loss: 0.048538\n",
      "Iteration: 233/1718 (13.6%)  Accuracy:93.75%  Loss: 0.213705\n",
      "Iteration: 234/1718 (13.6%)  Accuracy:100.00%  Loss: 0.028367\n",
      "Iteration: 235/1718 (13.7%)  Accuracy:96.88%  Loss: 0.119994\n",
      "Iteration: 236/1718 (13.7%)  Accuracy:96.88%  Loss: 0.054074\n",
      "Iteration: 237/1718 (13.8%)  Accuracy:100.00%  Loss: 0.008800\n",
      "Iteration: 238/1718 (13.9%)  Accuracy:100.00%  Loss: 0.014937\n",
      "Iteration: 239/1718 (13.9%)  Accuracy:96.88%  Loss: 0.053960\n",
      "Iteration: 240/1718 (14.0%)  Accuracy:96.88%  Loss: 0.106215\n",
      "Iteration: 241/1718 (14.0%)  Accuracy:96.88%  Loss: 0.053612\n",
      "Iteration: 242/1718 (14.1%)  Accuracy:93.75%  Loss: 0.114534\n",
      "Iteration: 243/1718 (14.1%)  Accuracy:96.88%  Loss: 0.085758\n",
      "Iteration: 244/1718 (14.2%)  Accuracy:96.88%  Loss: 0.100251\n",
      "Iteration: 245/1718 (14.3%)  Accuracy:96.88%  Loss: 0.110654\n",
      "Iteration: 246/1718 (14.3%)  Accuracy:100.00%  Loss: 0.056287\n",
      "Iteration: 247/1718 (14.4%)  Accuracy:81.25%  Loss: 0.621365\n",
      "Iteration: 248/1718 (14.4%)  Accuracy:90.62%  Loss: 0.126979\n",
      "Iteration: 249/1718 (14.5%)  Accuracy:96.88%  Loss: 0.043271\n",
      "Iteration: 250/1718 (14.6%)  Accuracy:96.88%  Loss: 0.092161\n",
      "Iteration: 251/1718 (14.6%)  Accuracy:100.00%  Loss: 0.045969\n",
      "Iteration: 252/1718 (14.7%)  Accuracy:100.00%  Loss: 0.076018\n",
      "Iteration: 253/1718 (14.7%)  Accuracy:93.75%  Loss: 0.274143\n",
      "Iteration: 254/1718 (14.8%)  Accuracy:96.88%  Loss: 0.068420\n",
      "Iteration: 255/1718 (14.8%)  Accuracy:96.88%  Loss: 0.232838\n",
      "Iteration: 256/1718 (14.9%)  Accuracy:96.88%  Loss: 0.189360\n",
      "Iteration: 257/1718 (15.0%)  Accuracy:93.75%  Loss: 0.219162\n",
      "Iteration: 258/1718 (15.0%)  Accuracy:96.88%  Loss: 0.077128\n",
      "Iteration: 259/1718 (15.1%)  Accuracy:96.88%  Loss: 0.106016\n",
      "Iteration: 260/1718 (15.1%)  Accuracy:96.88%  Loss: 0.073336\n",
      "Iteration: 261/1718 (15.2%)  Accuracy:100.00%  Loss: 0.033115\n",
      "Iteration: 262/1718 (15.3%)  Accuracy:100.00%  Loss: 0.026847\n",
      "Iteration: 263/1718 (15.3%)  Accuracy:100.00%  Loss: 0.005484\n",
      "Iteration: 264/1718 (15.4%)  Accuracy:93.75%  Loss: 0.130436\n",
      "Iteration: 265/1718 (15.4%)  Accuracy:96.88%  Loss: 0.161546\n",
      "Iteration: 266/1718 (15.5%)  Accuracy:96.88%  Loss: 0.160612\n",
      "Iteration: 267/1718 (15.5%)  Accuracy:96.88%  Loss: 0.201004\n",
      "Iteration: 268/1718 (15.6%)  Accuracy:100.00%  Loss: 0.070117\n",
      "Iteration: 269/1718 (15.7%)  Accuracy:100.00%  Loss: 0.030488\n",
      "Iteration: 270/1718 (15.7%)  Accuracy:93.75%  Loss: 0.073600\n",
      "Iteration: 271/1718 (15.8%)  Accuracy:90.62%  Loss: 0.230216\n",
      "Iteration: 272/1718 (15.8%)  Accuracy:93.75%  Loss: 0.146113\n",
      "Iteration: 273/1718 (15.9%)  Accuracy:96.88%  Loss: 0.088096\n",
      "Iteration: 274/1718 (15.9%)  Accuracy:96.88%  Loss: 0.050221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 275/1718 (16.0%)  Accuracy:96.88%  Loss: 0.182282\n",
      "Iteration: 276/1718 (16.1%)  Accuracy:96.88%  Loss: 0.159097\n",
      "Iteration: 277/1718 (16.1%)  Accuracy:87.50%  Loss: 0.142917\n",
      "Iteration: 278/1718 (16.2%)  Accuracy:96.88%  Loss: 0.131359\n",
      "Iteration: 279/1718 (16.2%)  Accuracy:93.75%  Loss: 0.196086\n",
      "Iteration: 280/1718 (16.3%)  Accuracy:93.75%  Loss: 0.285650\n",
      "Iteration: 281/1718 (16.4%)  Accuracy:100.00%  Loss: 0.023236\n",
      "Iteration: 282/1718 (16.4%)  Accuracy:96.88%  Loss: 0.048967\n",
      "Iteration: 283/1718 (16.5%)  Accuracy:87.50%  Loss: 0.438673\n",
      "Iteration: 284/1718 (16.5%)  Accuracy:100.00%  Loss: 0.017204\n",
      "Iteration: 285/1718 (16.6%)  Accuracy:93.75%  Loss: 0.195146\n",
      "Iteration: 286/1718 (16.6%)  Accuracy:96.88%  Loss: 0.058649\n",
      "Iteration: 287/1718 (16.7%)  Accuracy:93.75%  Loss: 0.077348\n",
      "Iteration: 288/1718 (16.8%)  Accuracy:96.88%  Loss: 0.128147\n",
      "Iteration: 289/1718 (16.8%)  Accuracy:93.75%  Loss: 0.197669\n",
      "Iteration: 290/1718 (16.9%)  Accuracy:93.75%  Loss: 0.092982\n",
      "Iteration: 291/1718 (16.9%)  Accuracy:90.62%  Loss: 0.291344\n",
      "Iteration: 292/1718 (17.0%)  Accuracy:96.88%  Loss: 0.126348\n",
      "Iteration: 293/1718 (17.1%)  Accuracy:100.00%  Loss: 0.022888\n",
      "Iteration: 294/1718 (17.1%)  Accuracy:100.00%  Loss: 0.065162\n",
      "Iteration: 295/1718 (17.2%)  Accuracy:100.00%  Loss: 0.052845\n",
      "Iteration: 296/1718 (17.2%)  Accuracy:100.00%  Loss: 0.028648\n",
      "Iteration: 297/1718 (17.3%)  Accuracy:100.00%  Loss: 0.049226\n",
      "Iteration: 298/1718 (17.3%)  Accuracy:93.75%  Loss: 0.282683\n",
      "Iteration: 299/1718 (17.4%)  Accuracy:96.88%  Loss: 0.121375\n",
      "Iteration: 300/1718 (17.5%)  Accuracy:100.00%  Loss: 0.034766\n",
      "Iteration: 301/1718 (17.5%)  Accuracy:93.75%  Loss: 0.294706\n",
      "Iteration: 302/1718 (17.6%)  Accuracy:100.00%  Loss: 0.016516\n",
      "Iteration: 303/1718 (17.6%)  Accuracy:100.00%  Loss: 0.057296\n",
      "Iteration: 304/1718 (17.7%)  Accuracy:96.88%  Loss: 0.113538\n",
      "Iteration: 305/1718 (17.8%)  Accuracy:96.88%  Loss: 0.123082\n",
      "Iteration: 306/1718 (17.8%)  Accuracy:90.62%  Loss: 0.213049\n",
      "Iteration: 307/1718 (17.9%)  Accuracy:93.75%  Loss: 0.177610\n",
      "Iteration: 308/1718 (17.9%)  Accuracy:100.00%  Loss: 0.066315\n",
      "Iteration: 309/1718 (18.0%)  Accuracy:93.75%  Loss: 0.155343\n",
      "Iteration: 310/1718 (18.0%)  Accuracy:96.88%  Loss: 0.063733\n",
      "Iteration: 311/1718 (18.1%)  Accuracy:100.00%  Loss: 0.042451\n",
      "Iteration: 312/1718 (18.2%)  Accuracy:93.75%  Loss: 0.333625\n",
      "Iteration: 313/1718 (18.2%)  Accuracy:96.88%  Loss: 0.099498\n",
      "Iteration: 314/1718 (18.3%)  Accuracy:87.50%  Loss: 0.386311\n",
      "Iteration: 315/1718 (18.3%)  Accuracy:87.50%  Loss: 0.540887\n",
      "Iteration: 316/1718 (18.4%)  Accuracy:93.75%  Loss: 0.240806\n",
      "Iteration: 317/1718 (18.5%)  Accuracy:96.88%  Loss: 0.104469\n",
      "Iteration: 318/1718 (18.5%)  Accuracy:100.00%  Loss: 0.029133\n",
      "Iteration: 319/1718 (18.6%)  Accuracy:93.75%  Loss: 0.162959\n",
      "Iteration: 320/1718 (18.6%)  Accuracy:100.00%  Loss: 0.075263\n",
      "Iteration: 321/1718 (18.7%)  Accuracy:93.75%  Loss: 0.192830\n",
      "Iteration: 322/1718 (18.7%)  Accuracy:84.38%  Loss: 0.638849\n",
      "Iteration: 323/1718 (18.8%)  Accuracy:96.88%  Loss: 0.061780\n",
      "Iteration: 324/1718 (18.9%)  Accuracy:84.38%  Loss: 0.383556\n",
      "Iteration: 325/1718 (18.9%)  Accuracy:93.75%  Loss: 0.275205\n",
      "Iteration: 326/1718 (19.0%)  Accuracy:100.00%  Loss: 0.052640\n",
      "Iteration: 327/1718 (19.0%)  Accuracy:96.88%  Loss: 0.072512\n",
      "Iteration: 328/1718 (19.1%)  Accuracy:81.25%  Loss: 0.328986\n",
      "Iteration: 329/1718 (19.2%)  Accuracy:90.62%  Loss: 0.182794\n",
      "Iteration: 330/1718 (19.2%)  Accuracy:87.50%  Loss: 0.207266\n",
      "Iteration: 331/1718 (19.3%)  Accuracy:93.75%  Loss: 0.214508\n",
      "Iteration: 332/1718 (19.3%)  Accuracy:96.88%  Loss: 0.146952\n",
      "Iteration: 333/1718 (19.4%)  Accuracy:93.75%  Loss: 0.094179\n",
      "Iteration: 334/1718 (19.4%)  Accuracy:96.88%  Loss: 0.276995\n",
      "Iteration: 335/1718 (19.5%)  Accuracy:96.88%  Loss: 0.080110\n",
      "Iteration: 336/1718 (19.6%)  Accuracy:93.75%  Loss: 0.237749\n",
      "Iteration: 337/1718 (19.6%)  Accuracy:100.00%  Loss: 0.033339\n",
      "Iteration: 338/1718 (19.7%)  Accuracy:96.88%  Loss: 0.135860\n",
      "Iteration: 339/1718 (19.7%)  Accuracy:96.88%  Loss: 0.067704\n",
      "Iteration: 340/1718 (19.8%)  Accuracy:100.00%  Loss: 0.053491\n",
      "Iteration: 341/1718 (19.8%)  Accuracy:96.88%  Loss: 0.072861\n",
      "Iteration: 342/1718 (19.9%)  Accuracy:93.75%  Loss: 0.125031\n",
      "Iteration: 343/1718 (20.0%)  Accuracy:100.00%  Loss: 0.053805\n",
      "Iteration: 344/1718 (20.0%)  Accuracy:96.88%  Loss: 0.133244\n",
      "Iteration: 345/1718 (20.1%)  Accuracy:100.00%  Loss: 0.011257\n",
      "Iteration: 346/1718 (20.1%)  Accuracy:100.00%  Loss: 0.012286\n",
      "Iteration: 347/1718 (20.2%)  Accuracy:96.88%  Loss: 0.133963\n",
      "Iteration: 348/1718 (20.3%)  Accuracy:96.88%  Loss: 0.151222\n",
      "Iteration: 349/1718 (20.3%)  Accuracy:93.75%  Loss: 0.208063\n",
      "Iteration: 350/1718 (20.4%)  Accuracy:96.88%  Loss: 0.038751\n",
      "Iteration: 351/1718 (20.4%)  Accuracy:93.75%  Loss: 0.198070\n",
      "Iteration: 352/1718 (20.5%)  Accuracy:96.88%  Loss: 0.089739\n",
      "Iteration: 353/1718 (20.5%)  Accuracy:100.00%  Loss: 0.020868\n",
      "Iteration: 354/1718 (20.6%)  Accuracy:96.88%  Loss: 0.148125\n",
      "Iteration: 355/1718 (20.7%)  Accuracy:100.00%  Loss: 0.017132\n",
      "Iteration: 356/1718 (20.7%)  Accuracy:87.50%  Loss: 0.134484\n",
      "Iteration: 357/1718 (20.8%)  Accuracy:93.75%  Loss: 0.137190\n",
      "Iteration: 358/1718 (20.8%)  Accuracy:96.88%  Loss: 0.087033\n",
      "Iteration: 359/1718 (20.9%)  Accuracy:100.00%  Loss: 0.029370\n",
      "Iteration: 360/1718 (21.0%)  Accuracy:96.88%  Loss: 0.079033\n",
      "Iteration: 361/1718 (21.0%)  Accuracy:100.00%  Loss: 0.016422\n",
      "Iteration: 362/1718 (21.1%)  Accuracy:100.00%  Loss: 0.030435\n",
      "Iteration: 363/1718 (21.1%)  Accuracy:96.88%  Loss: 0.096004\n",
      "Iteration: 364/1718 (21.2%)  Accuracy:96.88%  Loss: 0.063504\n",
      "Iteration: 365/1718 (21.2%)  Accuracy:100.00%  Loss: 0.028960\n",
      "Iteration: 366/1718 (21.3%)  Accuracy:100.00%  Loss: 0.037337\n",
      "Iteration: 367/1718 (21.4%)  Accuracy:96.88%  Loss: 0.040993\n",
      "Iteration: 368/1718 (21.4%)  Accuracy:90.62%  Loss: 0.318502\n",
      "Iteration: 369/1718 (21.5%)  Accuracy:96.88%  Loss: 0.052134\n",
      "Iteration: 370/1718 (21.5%)  Accuracy:100.00%  Loss: 0.043816\n",
      "Iteration: 371/1718 (21.6%)  Accuracy:100.00%  Loss: 0.037490\n",
      "Iteration: 372/1718 (21.7%)  Accuracy:100.00%  Loss: 0.007934\n",
      "Iteration: 373/1718 (21.7%)  Accuracy:100.00%  Loss: 0.030564\n",
      "Iteration: 374/1718 (21.8%)  Accuracy:96.88%  Loss: 0.104645\n",
      "Iteration: 375/1718 (21.8%)  Accuracy:93.75%  Loss: 0.178281\n",
      "Iteration: 376/1718 (21.9%)  Accuracy:100.00%  Loss: 0.010100\n",
      "Iteration: 377/1718 (21.9%)  Accuracy:96.88%  Loss: 0.063588\n",
      "Iteration: 378/1718 (22.0%)  Accuracy:96.88%  Loss: 0.043686\n",
      "Iteration: 379/1718 (22.1%)  Accuracy:100.00%  Loss: 0.004672\n",
      "Iteration: 380/1718 (22.1%)  Accuracy:96.88%  Loss: 0.043480\n",
      "Iteration: 381/1718 (22.2%)  Accuracy:90.62%  Loss: 0.287197\n",
      "Iteration: 382/1718 (22.2%)  Accuracy:93.75%  Loss: 0.141908\n",
      "Iteration: 383/1718 (22.3%)  Accuracy:96.88%  Loss: 0.051925\n",
      "Iteration: 384/1718 (22.4%)  Accuracy:96.88%  Loss: 0.081307\n",
      "Iteration: 385/1718 (22.4%)  Accuracy:100.00%  Loss: 0.005987\n",
      "Iteration: 386/1718 (22.5%)  Accuracy:100.00%  Loss: 0.007914\n",
      "Iteration: 387/1718 (22.5%)  Accuracy:96.88%  Loss: 0.049235\n",
      "Iteration: 388/1718 (22.6%)  Accuracy:90.62%  Loss: 0.196572\n",
      "Iteration: 389/1718 (22.6%)  Accuracy:100.00%  Loss: 0.022268\n",
      "Iteration: 390/1718 (22.7%)  Accuracy:96.88%  Loss: 0.057661\n",
      "Iteration: 391/1718 (22.8%)  Accuracy:96.88%  Loss: 0.138576\n",
      "Iteration: 392/1718 (22.8%)  Accuracy:96.88%  Loss: 0.115943\n",
      "Iteration: 393/1718 (22.9%)  Accuracy:96.88%  Loss: 0.076864\n",
      "Iteration: 394/1718 (22.9%)  Accuracy:100.00%  Loss: 0.038558\n",
      "Iteration: 395/1718 (23.0%)  Accuracy:93.75%  Loss: 0.104404\n",
      "Iteration: 396/1718 (23.1%)  Accuracy:100.00%  Loss: 0.031802\n",
      "Iteration: 397/1718 (23.1%)  Accuracy:100.00%  Loss: 0.015731\n",
      "Iteration: 398/1718 (23.2%)  Accuracy:93.75%  Loss: 0.120617\n",
      "Iteration: 399/1718 (23.2%)  Accuracy:100.00%  Loss: 0.025573\n",
      "Iteration: 400/1718 (23.3%)  Accuracy:93.75%  Loss: 0.190170\n",
      "Iteration: 401/1718 (23.3%)  Accuracy:93.75%  Loss: 0.248439\n",
      "Iteration: 402/1718 (23.4%)  Accuracy:96.88%  Loss: 0.089788\n",
      "Iteration: 403/1718 (23.5%)  Accuracy:100.00%  Loss: 0.053638\n",
      "Iteration: 404/1718 (23.5%)  Accuracy:93.75%  Loss: 0.187443\n",
      "Iteration: 405/1718 (23.6%)  Accuracy:93.75%  Loss: 0.102836\n",
      "Iteration: 406/1718 (23.6%)  Accuracy:96.88%  Loss: 0.209319\n",
      "Iteration: 407/1718 (23.7%)  Accuracy:100.00%  Loss: 0.030948\n",
      "Iteration: 408/1718 (23.7%)  Accuracy:100.00%  Loss: 0.021071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 409/1718 (23.8%)  Accuracy:96.88%  Loss: 0.114539\n",
      "Iteration: 410/1718 (23.9%)  Accuracy:96.88%  Loss: 0.054682\n",
      "Iteration: 411/1718 (23.9%)  Accuracy:100.00%  Loss: 0.044002\n",
      "Iteration: 412/1718 (24.0%)  Accuracy:90.62%  Loss: 0.220882\n",
      "Iteration: 413/1718 (24.0%)  Accuracy:100.00%  Loss: 0.020920\n",
      "Iteration: 414/1718 (24.1%)  Accuracy:100.00%  Loss: 0.016284\n",
      "Iteration: 415/1718 (24.2%)  Accuracy:96.88%  Loss: 0.075798\n",
      "Iteration: 416/1718 (24.2%)  Accuracy:93.75%  Loss: 0.077084\n",
      "Iteration: 417/1718 (24.3%)  Accuracy:96.88%  Loss: 0.035426\n",
      "Iteration: 418/1718 (24.3%)  Accuracy:100.00%  Loss: 0.018297\n",
      "Iteration: 419/1718 (24.4%)  Accuracy:96.88%  Loss: 0.048786\n",
      "Iteration: 420/1718 (24.4%)  Accuracy:96.88%  Loss: 0.038535\n",
      "Iteration: 421/1718 (24.5%)  Accuracy:90.62%  Loss: 0.140637\n",
      "Iteration: 422/1718 (24.6%)  Accuracy:100.00%  Loss: 0.016586\n",
      "Iteration: 423/1718 (24.6%)  Accuracy:96.88%  Loss: 0.075049\n",
      "Iteration: 424/1718 (24.7%)  Accuracy:100.00%  Loss: 0.006946\n",
      "Iteration: 425/1718 (24.7%)  Accuracy:96.88%  Loss: 0.030452\n",
      "Iteration: 426/1718 (24.8%)  Accuracy:100.00%  Loss: 0.010670\n",
      "Iteration: 427/1718 (24.9%)  Accuracy:100.00%  Loss: 0.016262\n",
      "Iteration: 428/1718 (24.9%)  Accuracy:100.00%  Loss: 0.009915\n",
      "Iteration: 429/1718 (25.0%)  Accuracy:100.00%  Loss: 0.008239\n",
      "Iteration: 430/1718 (25.0%)  Accuracy:96.88%  Loss: 0.040961\n",
      "Iteration: 431/1718 (25.1%)  Accuracy:100.00%  Loss: 0.013526\n",
      "Iteration: 432/1718 (25.1%)  Accuracy:96.88%  Loss: 0.109194\n",
      "Iteration: 433/1718 (25.2%)  Accuracy:93.75%  Loss: 0.167759\n",
      "Iteration: 434/1718 (25.3%)  Accuracy:93.75%  Loss: 0.368704\n",
      "Iteration: 435/1718 (25.3%)  Accuracy:100.00%  Loss: 0.012737\n",
      "Iteration: 436/1718 (25.4%)  Accuracy:96.88%  Loss: 0.075129\n",
      "Iteration: 437/1718 (25.4%)  Accuracy:100.00%  Loss: 0.028509\n",
      "Iteration: 438/1718 (25.5%)  Accuracy:96.88%  Loss: 0.046371\n",
      "Iteration: 439/1718 (25.6%)  Accuracy:93.75%  Loss: 0.155142\n",
      "Iteration: 440/1718 (25.6%)  Accuracy:100.00%  Loss: 0.013107\n",
      "Iteration: 441/1718 (25.7%)  Accuracy:96.88%  Loss: 0.084323\n",
      "Iteration: 442/1718 (25.7%)  Accuracy:100.00%  Loss: 0.037294\n",
      "Iteration: 443/1718 (25.8%)  Accuracy:100.00%  Loss: 0.044673\n",
      "Iteration: 444/1718 (25.8%)  Accuracy:93.75%  Loss: 0.143542\n",
      "Iteration: 445/1718 (25.9%)  Accuracy:96.88%  Loss: 0.166935\n",
      "Iteration: 446/1718 (26.0%)  Accuracy:96.88%  Loss: 0.068141\n",
      "Iteration: 447/1718 (26.0%)  Accuracy:100.00%  Loss: 0.001911\n",
      "Iteration: 448/1718 (26.1%)  Accuracy:100.00%  Loss: 0.031299\n",
      "Iteration: 449/1718 (26.1%)  Accuracy:93.75%  Loss: 0.104249\n",
      "Iteration: 450/1718 (26.2%)  Accuracy:96.88%  Loss: 0.154542\n",
      "Iteration: 451/1718 (26.3%)  Accuracy:96.88%  Loss: 0.073310\n",
      "Iteration: 452/1718 (26.3%)  Accuracy:100.00%  Loss: 0.044924\n",
      "Iteration: 453/1718 (26.4%)  Accuracy:96.88%  Loss: 0.109423\n",
      "Iteration: 454/1718 (26.4%)  Accuracy:96.88%  Loss: 0.105008\n",
      "Iteration: 455/1718 (26.5%)  Accuracy:93.75%  Loss: 0.232235\n",
      "Iteration: 456/1718 (26.5%)  Accuracy:93.75%  Loss: 0.214003\n",
      "Iteration: 457/1718 (26.6%)  Accuracy:96.88%  Loss: 0.048038\n",
      "Iteration: 458/1718 (26.7%)  Accuracy:100.00%  Loss: 0.047031\n",
      "Iteration: 459/1718 (26.7%)  Accuracy:96.88%  Loss: 0.064889\n",
      "Iteration: 460/1718 (26.8%)  Accuracy:96.88%  Loss: 0.095430\n",
      "Iteration: 461/1718 (26.8%)  Accuracy:96.88%  Loss: 0.241056\n",
      "Iteration: 462/1718 (26.9%)  Accuracy:96.88%  Loss: 0.094226\n",
      "Iteration: 463/1718 (26.9%)  Accuracy:93.75%  Loss: 0.237954\n",
      "Iteration: 464/1718 (27.0%)  Accuracy:96.88%  Loss: 0.150332\n",
      "Iteration: 465/1718 (27.1%)  Accuracy:96.88%  Loss: 0.106033\n",
      "Iteration: 466/1718 (27.1%)  Accuracy:96.88%  Loss: 0.225326\n",
      "Iteration: 467/1718 (27.2%)  Accuracy:93.75%  Loss: 0.305445\n",
      "Iteration: 468/1718 (27.2%)  Accuracy:100.00%  Loss: 0.013592\n",
      "Iteration: 469/1718 (27.3%)  Accuracy:100.00%  Loss: 0.022268\n",
      "Iteration: 470/1718 (27.4%)  Accuracy:96.88%  Loss: 0.091420\n",
      "Iteration: 471/1718 (27.4%)  Accuracy:90.62%  Loss: 0.272069\n",
      "Iteration: 472/1718 (27.5%)  Accuracy:100.00%  Loss: 0.017391\n",
      "Iteration: 473/1718 (27.5%)  Accuracy:96.88%  Loss: 0.125835\n",
      "Iteration: 474/1718 (27.6%)  Accuracy:96.88%  Loss: 0.124897\n",
      "Iteration: 475/1718 (27.6%)  Accuracy:93.75%  Loss: 0.091568\n",
      "Iteration: 476/1718 (27.7%)  Accuracy:93.75%  Loss: 0.177152\n",
      "Iteration: 477/1718 (27.8%)  Accuracy:96.88%  Loss: 0.060121\n",
      "Iteration: 478/1718 (27.8%)  Accuracy:100.00%  Loss: 0.016059\n",
      "Iteration: 479/1718 (27.9%)  Accuracy:96.88%  Loss: 0.068825\n",
      "Iteration: 480/1718 (27.9%)  Accuracy:96.88%  Loss: 0.105785\n",
      "Iteration: 481/1718 (28.0%)  Accuracy:100.00%  Loss: 0.016052\n",
      "Iteration: 482/1718 (28.1%)  Accuracy:96.88%  Loss: 0.079154\n",
      "Iteration: 483/1718 (28.1%)  Accuracy:93.75%  Loss: 0.092200\n",
      "Iteration: 484/1718 (28.2%)  Accuracy:93.75%  Loss: 0.118844\n",
      "Iteration: 485/1718 (28.2%)  Accuracy:93.75%  Loss: 0.121025\n",
      "Iteration: 486/1718 (28.3%)  Accuracy:96.88%  Loss: 0.139827\n",
      "Iteration: 487/1718 (28.3%)  Accuracy:100.00%  Loss: 0.056943\n",
      "Iteration: 488/1718 (28.4%)  Accuracy:100.00%  Loss: 0.023526\n",
      "Iteration: 489/1718 (28.5%)  Accuracy:100.00%  Loss: 0.034481\n",
      "Iteration: 490/1718 (28.5%)  Accuracy:100.00%  Loss: 0.003248\n",
      "Iteration: 491/1718 (28.6%)  Accuracy:100.00%  Loss: 0.021522\n",
      "Iteration: 492/1718 (28.6%)  Accuracy:100.00%  Loss: 0.013276\n",
      "Iteration: 493/1718 (28.7%)  Accuracy:100.00%  Loss: 0.033115\n",
      "Iteration: 494/1718 (28.8%)  Accuracy:100.00%  Loss: 0.077907\n",
      "Iteration: 495/1718 (28.8%)  Accuracy:100.00%  Loss: 0.016270\n",
      "Iteration: 496/1718 (28.9%)  Accuracy:100.00%  Loss: 0.011162\n",
      "Iteration: 497/1718 (28.9%)  Accuracy:96.88%  Loss: 0.091650\n",
      "Iteration: 498/1718 (29.0%)  Accuracy:100.00%  Loss: 0.016440\n",
      "Iteration: 499/1718 (29.0%)  Accuracy:96.88%  Loss: 0.246465\n",
      "Iteration: 500/1718 (29.1%)  Accuracy:100.00%  Loss: 0.002009\n",
      "Iteration: 501/1718 (29.2%)  Accuracy:100.00%  Loss: 0.009912\n",
      "Iteration: 502/1718 (29.2%)  Accuracy:96.88%  Loss: 0.061233\n",
      "Iteration: 503/1718 (29.3%)  Accuracy:100.00%  Loss: 0.001983\n",
      "Iteration: 504/1718 (29.3%)  Accuracy:96.88%  Loss: 0.065527\n",
      "Iteration: 505/1718 (29.4%)  Accuracy:100.00%  Loss: 0.016310\n",
      "Iteration: 506/1718 (29.5%)  Accuracy:100.00%  Loss: 0.058861\n",
      "Iteration: 507/1718 (29.5%)  Accuracy:100.00%  Loss: 0.024061\n",
      "Iteration: 508/1718 (29.6%)  Accuracy:100.00%  Loss: 0.025553\n",
      "Iteration: 509/1718 (29.6%)  Accuracy:100.00%  Loss: 0.001239\n",
      "Iteration: 510/1718 (29.7%)  Accuracy:87.50%  Loss: 0.419668\n",
      "Iteration: 511/1718 (29.7%)  Accuracy:93.75%  Loss: 0.118396\n",
      "Iteration: 512/1718 (29.8%)  Accuracy:90.62%  Loss: 0.156084\n",
      "Iteration: 513/1718 (29.9%)  Accuracy:93.75%  Loss: 0.127629\n",
      "Iteration: 514/1718 (29.9%)  Accuracy:93.75%  Loss: 0.298699\n",
      "Iteration: 515/1718 (30.0%)  Accuracy:96.88%  Loss: 0.075651\n",
      "Iteration: 516/1718 (30.0%)  Accuracy:96.88%  Loss: 0.469414\n",
      "Iteration: 517/1718 (30.1%)  Accuracy:100.00%  Loss: 0.013456\n",
      "Iteration: 518/1718 (30.2%)  Accuracy:96.88%  Loss: 0.090447\n",
      "Iteration: 519/1718 (30.2%)  Accuracy:100.00%  Loss: 0.015487\n",
      "Iteration: 520/1718 (30.3%)  Accuracy:96.88%  Loss: 0.223383\n",
      "Iteration: 521/1718 (30.3%)  Accuracy:87.50%  Loss: 0.317740\n",
      "Iteration: 522/1718 (30.4%)  Accuracy:96.88%  Loss: 0.147550\n",
      "Iteration: 523/1718 (30.4%)  Accuracy:93.75%  Loss: 0.157579\n",
      "Iteration: 524/1718 (30.5%)  Accuracy:100.00%  Loss: 0.030190\n",
      "Iteration: 525/1718 (30.6%)  Accuracy:96.88%  Loss: 0.101856\n",
      "Iteration: 526/1718 (30.6%)  Accuracy:96.88%  Loss: 0.118622\n",
      "Iteration: 527/1718 (30.7%)  Accuracy:100.00%  Loss: 0.071752\n",
      "Iteration: 528/1718 (30.7%)  Accuracy:100.00%  Loss: 0.042015\n",
      "Iteration: 529/1718 (30.8%)  Accuracy:96.88%  Loss: 0.237563\n",
      "Iteration: 530/1718 (30.8%)  Accuracy:96.88%  Loss: 0.138502\n",
      "Iteration: 531/1718 (30.9%)  Accuracy:100.00%  Loss: 0.021124\n",
      "Iteration: 532/1718 (31.0%)  Accuracy:100.00%  Loss: 0.012781\n",
      "Iteration: 533/1718 (31.0%)  Accuracy:96.88%  Loss: 0.095339\n",
      "Iteration: 534/1718 (31.1%)  Accuracy:100.00%  Loss: 0.046725\n",
      "Iteration: 535/1718 (31.1%)  Accuracy:100.00%  Loss: 0.028331\n",
      "Iteration: 536/1718 (31.2%)  Accuracy:96.88%  Loss: 0.137397\n",
      "Iteration: 537/1718 (31.3%)  Accuracy:100.00%  Loss: 0.008768\n",
      "Iteration: 538/1718 (31.3%)  Accuracy:93.75%  Loss: 0.146734\n",
      "Iteration: 539/1718 (31.4%)  Accuracy:96.88%  Loss: 0.052922\n",
      "Iteration: 540/1718 (31.4%)  Accuracy:96.88%  Loss: 0.210427\n",
      "Iteration: 541/1718 (31.5%)  Accuracy:93.75%  Loss: 0.097203\n",
      "Iteration: 542/1718 (31.5%)  Accuracy:93.75%  Loss: 0.182627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 543/1718 (31.6%)  Accuracy:93.75%  Loss: 0.304460\n",
      "Iteration: 544/1718 (31.7%)  Accuracy:100.00%  Loss: 0.022066\n",
      "Iteration: 545/1718 (31.7%)  Accuracy:100.00%  Loss: 0.024920\n",
      "Iteration: 546/1718 (31.8%)  Accuracy:96.88%  Loss: 0.064643\n",
      "Iteration: 547/1718 (31.8%)  Accuracy:96.88%  Loss: 0.057440\n",
      "Iteration: 548/1718 (31.9%)  Accuracy:93.75%  Loss: 0.081325\n",
      "Iteration: 549/1718 (32.0%)  Accuracy:100.00%  Loss: 0.024650\n",
      "Iteration: 550/1718 (32.0%)  Accuracy:96.88%  Loss: 0.156788\n",
      "Iteration: 551/1718 (32.1%)  Accuracy:100.00%  Loss: 0.038109\n",
      "Iteration: 552/1718 (32.1%)  Accuracy:96.88%  Loss: 0.075125\n",
      "Iteration: 553/1718 (32.2%)  Accuracy:93.75%  Loss: 0.104867\n",
      "Iteration: 554/1718 (32.2%)  Accuracy:100.00%  Loss: 0.032125\n",
      "Iteration: 555/1718 (32.3%)  Accuracy:96.88%  Loss: 0.074025\n",
      "Iteration: 556/1718 (32.4%)  Accuracy:96.88%  Loss: 0.075813\n",
      "Iteration: 557/1718 (32.4%)  Accuracy:93.75%  Loss: 0.163509\n",
      "Iteration: 558/1718 (32.5%)  Accuracy:90.62%  Loss: 0.120830\n",
      "Iteration: 559/1718 (32.5%)  Accuracy:93.75%  Loss: 0.126085\n",
      "Iteration: 560/1718 (32.6%)  Accuracy:96.88%  Loss: 0.078641\n",
      "Iteration: 561/1718 (32.7%)  Accuracy:100.00%  Loss: 0.009343\n",
      "Iteration: 562/1718 (32.7%)  Accuracy:96.88%  Loss: 0.042799\n",
      "Iteration: 563/1718 (32.8%)  Accuracy:96.88%  Loss: 0.136598\n",
      "Iteration: 564/1718 (32.8%)  Accuracy:100.00%  Loss: 0.017311\n",
      "Iteration: 565/1718 (32.9%)  Accuracy:100.00%  Loss: 0.019484\n",
      "Iteration: 566/1718 (32.9%)  Accuracy:96.88%  Loss: 0.084084\n",
      "Iteration: 567/1718 (33.0%)  Accuracy:100.00%  Loss: 0.037572\n",
      "Iteration: 568/1718 (33.1%)  Accuracy:93.75%  Loss: 0.104081\n",
      "Iteration: 569/1718 (33.1%)  Accuracy:100.00%  Loss: 0.017584\n",
      "Iteration: 570/1718 (33.2%)  Accuracy:100.00%  Loss: 0.024114\n",
      "Iteration: 571/1718 (33.2%)  Accuracy:96.88%  Loss: 0.082722\n",
      "Iteration: 572/1718 (33.3%)  Accuracy:93.75%  Loss: 0.112188\n",
      "Iteration: 573/1718 (33.4%)  Accuracy:100.00%  Loss: 0.037102\n",
      "Iteration: 574/1718 (33.4%)  Accuracy:100.00%  Loss: 0.010675\n",
      "Iteration: 575/1718 (33.5%)  Accuracy:100.00%  Loss: 0.012674\n",
      "Iteration: 576/1718 (33.5%)  Accuracy:100.00%  Loss: 0.008990\n",
      "Iteration: 577/1718 (33.6%)  Accuracy:96.88%  Loss: 0.065307\n",
      "Iteration: 578/1718 (33.6%)  Accuracy:100.00%  Loss: 0.009972\n",
      "Iteration: 579/1718 (33.7%)  Accuracy:93.75%  Loss: 0.187386\n",
      "Iteration: 580/1718 (33.8%)  Accuracy:100.00%  Loss: 0.031694\n",
      "Iteration: 581/1718 (33.8%)  Accuracy:96.88%  Loss: 0.056515\n",
      "Iteration: 582/1718 (33.9%)  Accuracy:96.88%  Loss: 0.066391\n",
      "Iteration: 583/1718 (33.9%)  Accuracy:100.00%  Loss: 0.032834\n",
      "Iteration: 584/1718 (34.0%)  Accuracy:100.00%  Loss: 0.031170\n",
      "Iteration: 585/1718 (34.1%)  Accuracy:100.00%  Loss: 0.069336\n",
      "Iteration: 586/1718 (34.1%)  Accuracy:90.62%  Loss: 0.248291\n",
      "Iteration: 587/1718 (34.2%)  Accuracy:100.00%  Loss: 0.024354\n",
      "Iteration: 588/1718 (34.2%)  Accuracy:93.75%  Loss: 0.126181\n",
      "Iteration: 589/1718 (34.3%)  Accuracy:100.00%  Loss: 0.006392\n",
      "Iteration: 590/1718 (34.3%)  Accuracy:100.00%  Loss: 0.010336\n",
      "Iteration: 591/1718 (34.4%)  Accuracy:100.00%  Loss: 0.014155\n",
      "Iteration: 592/1718 (34.5%)  Accuracy:96.88%  Loss: 0.180431\n",
      "Iteration: 593/1718 (34.5%)  Accuracy:96.88%  Loss: 0.086791\n",
      "Iteration: 594/1718 (34.6%)  Accuracy:93.75%  Loss: 0.230730\n",
      "Iteration: 595/1718 (34.6%)  Accuracy:96.88%  Loss: 0.065939\n",
      "Iteration: 596/1718 (34.7%)  Accuracy:93.75%  Loss: 0.222815\n",
      "Iteration: 597/1718 (34.7%)  Accuracy:93.75%  Loss: 0.177026\n",
      "Iteration: 598/1718 (34.8%)  Accuracy:96.88%  Loss: 0.047437\n",
      "Iteration: 599/1718 (34.9%)  Accuracy:96.88%  Loss: 0.054224\n",
      "Iteration: 600/1718 (34.9%)  Accuracy:96.88%  Loss: 0.090832\n",
      "Iteration: 601/1718 (35.0%)  Accuracy:93.75%  Loss: 0.351793\n",
      "Iteration: 602/1718 (35.0%)  Accuracy:93.75%  Loss: 0.221223\n",
      "Iteration: 603/1718 (35.1%)  Accuracy:100.00%  Loss: 0.005303\n",
      "Iteration: 604/1718 (35.2%)  Accuracy:100.00%  Loss: 0.011987\n",
      "Iteration: 605/1718 (35.2%)  Accuracy:93.75%  Loss: 0.081209\n",
      "Iteration: 606/1718 (35.3%)  Accuracy:100.00%  Loss: 0.007666\n",
      "Iteration: 607/1718 (35.3%)  Accuracy:96.88%  Loss: 0.062607\n",
      "Iteration: 608/1718 (35.4%)  Accuracy:100.00%  Loss: 0.020184\n",
      "Iteration: 609/1718 (35.4%)  Accuracy:100.00%  Loss: 0.014364\n",
      "Iteration: 610/1718 (35.5%)  Accuracy:93.75%  Loss: 0.173942\n",
      "Iteration: 611/1718 (35.6%)  Accuracy:100.00%  Loss: 0.007466\n",
      "Iteration: 612/1718 (35.6%)  Accuracy:96.88%  Loss: 0.040737\n",
      "Iteration: 613/1718 (35.7%)  Accuracy:90.62%  Loss: 0.421526\n",
      "Iteration: 614/1718 (35.7%)  Accuracy:100.00%  Loss: 0.067849\n",
      "Iteration: 615/1718 (35.8%)  Accuracy:100.00%  Loss: 0.004613\n",
      "Iteration: 616/1718 (35.9%)  Accuracy:90.62%  Loss: 0.284270\n",
      "Iteration: 617/1718 (35.9%)  Accuracy:100.00%  Loss: 0.007494\n",
      "Iteration: 618/1718 (36.0%)  Accuracy:100.00%  Loss: 0.032778\n",
      "Iteration: 619/1718 (36.0%)  Accuracy:93.75%  Loss: 0.138212\n",
      "Iteration: 620/1718 (36.1%)  Accuracy:100.00%  Loss: 0.016790\n",
      "Iteration: 621/1718 (36.1%)  Accuracy:96.88%  Loss: 0.151239\n",
      "Iteration: 622/1718 (36.2%)  Accuracy:96.88%  Loss: 0.089557\n",
      "Iteration: 623/1718 (36.3%)  Accuracy:96.88%  Loss: 0.094357\n",
      "Iteration: 624/1718 (36.3%)  Accuracy:100.00%  Loss: 0.039716\n",
      "Iteration: 625/1718 (36.4%)  Accuracy:100.00%  Loss: 0.015577\n",
      "Iteration: 626/1718 (36.4%)  Accuracy:96.88%  Loss: 0.058676\n",
      "Iteration: 627/1718 (36.5%)  Accuracy:100.00%  Loss: 0.047501\n",
      "Iteration: 628/1718 (36.6%)  Accuracy:100.00%  Loss: 0.028012\n",
      "Iteration: 629/1718 (36.6%)  Accuracy:100.00%  Loss: 0.035617\n",
      "Iteration: 630/1718 (36.7%)  Accuracy:100.00%  Loss: 0.040066\n",
      "Iteration: 631/1718 (36.7%)  Accuracy:96.88%  Loss: 0.146423\n",
      "Iteration: 632/1718 (36.8%)  Accuracy:93.75%  Loss: 0.098928\n",
      "Iteration: 633/1718 (36.8%)  Accuracy:96.88%  Loss: 0.087310\n",
      "Iteration: 634/1718 (36.9%)  Accuracy:100.00%  Loss: 0.037262\n",
      "Iteration: 635/1718 (37.0%)  Accuracy:100.00%  Loss: 0.018763\n",
      "Iteration: 636/1718 (37.0%)  Accuracy:100.00%  Loss: 0.026186\n",
      "Iteration: 637/1718 (37.1%)  Accuracy:100.00%  Loss: 0.009717\n",
      "Iteration: 638/1718 (37.1%)  Accuracy:100.00%  Loss: 0.017901\n",
      "Iteration: 639/1718 (37.2%)  Accuracy:96.88%  Loss: 0.108100\n",
      "Iteration: 640/1718 (37.3%)  Accuracy:100.00%  Loss: 0.007929\n",
      "Iteration: 641/1718 (37.3%)  Accuracy:96.88%  Loss: 0.070913\n",
      "Iteration: 642/1718 (37.4%)  Accuracy:100.00%  Loss: 0.004285\n",
      "Iteration: 643/1718 (37.4%)  Accuracy:96.88%  Loss: 0.044568\n",
      "Iteration: 644/1718 (37.5%)  Accuracy:100.00%  Loss: 0.008095\n",
      "Iteration: 645/1718 (37.5%)  Accuracy:90.62%  Loss: 0.198703\n",
      "Iteration: 646/1718 (37.6%)  Accuracy:100.00%  Loss: 0.012975\n",
      "Iteration: 647/1718 (37.7%)  Accuracy:96.88%  Loss: 0.093984\n",
      "Iteration: 648/1718 (37.7%)  Accuracy:96.88%  Loss: 0.034698\n",
      "Iteration: 649/1718 (37.8%)  Accuracy:100.00%  Loss: 0.006716\n",
      "Iteration: 650/1718 (37.8%)  Accuracy:96.88%  Loss: 0.059598\n",
      "Iteration: 651/1718 (37.9%)  Accuracy:100.00%  Loss: 0.045435\n",
      "Iteration: 652/1718 (38.0%)  Accuracy:93.75%  Loss: 0.117734\n",
      "Iteration: 653/1718 (38.0%)  Accuracy:96.88%  Loss: 0.152945\n",
      "Iteration: 654/1718 (38.1%)  Accuracy:96.88%  Loss: 0.112226\n",
      "Iteration: 655/1718 (38.1%)  Accuracy:96.88%  Loss: 0.042712\n",
      "Iteration: 656/1718 (38.2%)  Accuracy:100.00%  Loss: 0.012276\n",
      "Iteration: 657/1718 (38.2%)  Accuracy:100.00%  Loss: 0.010793\n",
      "Iteration: 658/1718 (38.3%)  Accuracy:96.88%  Loss: 0.141103\n",
      "Iteration: 659/1718 (38.4%)  Accuracy:96.88%  Loss: 0.080983\n",
      "Iteration: 660/1718 (38.4%)  Accuracy:100.00%  Loss: 0.028610\n",
      "Iteration: 661/1718 (38.5%)  Accuracy:100.00%  Loss: 0.026807\n",
      "Iteration: 662/1718 (38.5%)  Accuracy:100.00%  Loss: 0.029767\n",
      "Iteration: 663/1718 (38.6%)  Accuracy:96.88%  Loss: 0.075572\n",
      "Iteration: 664/1718 (38.6%)  Accuracy:96.88%  Loss: 0.091134\n",
      "Iteration: 665/1718 (38.7%)  Accuracy:100.00%  Loss: 0.018363\n",
      "Iteration: 666/1718 (38.8%)  Accuracy:93.75%  Loss: 0.388206\n",
      "Iteration: 667/1718 (38.8%)  Accuracy:96.88%  Loss: 0.185230\n",
      "Iteration: 668/1718 (38.9%)  Accuracy:100.00%  Loss: 0.012464\n",
      "Iteration: 669/1718 (38.9%)  Accuracy:96.88%  Loss: 0.055117\n",
      "Iteration: 670/1718 (39.0%)  Accuracy:100.00%  Loss: 0.006495\n",
      "Iteration: 671/1718 (39.1%)  Accuracy:96.88%  Loss: 0.043152\n",
      "Iteration: 672/1718 (39.1%)  Accuracy:96.88%  Loss: 0.102438\n",
      "Iteration: 673/1718 (39.2%)  Accuracy:100.00%  Loss: 0.034063\n",
      "Iteration: 674/1718 (39.2%)  Accuracy:96.88%  Loss: 0.120613\n",
      "Iteration: 675/1718 (39.3%)  Accuracy:96.88%  Loss: 0.071621\n",
      "Iteration: 676/1718 (39.3%)  Accuracy:96.88%  Loss: 0.065738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 677/1718 (39.4%)  Accuracy:100.00%  Loss: 0.023416\n",
      "Iteration: 678/1718 (39.5%)  Accuracy:93.75%  Loss: 0.136332\n",
      "Iteration: 679/1718 (39.5%)  Accuracy:96.88%  Loss: 0.075022\n",
      "Iteration: 680/1718 (39.6%)  Accuracy:93.75%  Loss: 0.164958\n",
      "Iteration: 681/1718 (39.6%)  Accuracy:96.88%  Loss: 0.077789\n",
      "Iteration: 682/1718 (39.7%)  Accuracy:100.00%  Loss: 0.004541\n",
      "Iteration: 683/1718 (39.8%)  Accuracy:93.75%  Loss: 0.171945\n",
      "Iteration: 684/1718 (39.8%)  Accuracy:100.00%  Loss: 0.007209\n",
      "Iteration: 685/1718 (39.9%)  Accuracy:100.00%  Loss: 0.027388\n",
      "Iteration: 686/1718 (39.9%)  Accuracy:96.88%  Loss: 0.079328\n",
      "Iteration: 687/1718 (40.0%)  Accuracy:100.00%  Loss: 0.005024\n",
      "Iteration: 688/1718 (40.0%)  Accuracy:96.88%  Loss: 0.050626\n",
      "Iteration: 689/1718 (40.1%)  Accuracy:90.62%  Loss: 0.450891\n",
      "Iteration: 690/1718 (40.2%)  Accuracy:96.88%  Loss: 0.055956\n",
      "Iteration: 691/1718 (40.2%)  Accuracy:96.88%  Loss: 0.058323\n",
      "Iteration: 692/1718 (40.3%)  Accuracy:96.88%  Loss: 0.065946\n",
      "Iteration: 693/1718 (40.3%)  Accuracy:100.00%  Loss: 0.006893\n",
      "Iteration: 694/1718 (40.4%)  Accuracy:100.00%  Loss: 0.039098\n",
      "Iteration: 695/1718 (40.5%)  Accuracy:96.88%  Loss: 0.208075\n",
      "Iteration: 696/1718 (40.5%)  Accuracy:93.75%  Loss: 0.095956\n",
      "Iteration: 697/1718 (40.6%)  Accuracy:100.00%  Loss: 0.029509\n",
      "Iteration: 698/1718 (40.6%)  Accuracy:93.75%  Loss: 0.167033\n",
      "Iteration: 699/1718 (40.7%)  Accuracy:100.00%  Loss: 0.010931\n",
      "Iteration: 700/1718 (40.7%)  Accuracy:100.00%  Loss: 0.005332\n",
      "Iteration: 701/1718 (40.8%)  Accuracy:100.00%  Loss: 0.005707\n",
      "Iteration: 702/1718 (40.9%)  Accuracy:100.00%  Loss: 0.002594\n",
      "Iteration: 703/1718 (40.9%)  Accuracy:96.88%  Loss: 0.153501\n",
      "Iteration: 704/1718 (41.0%)  Accuracy:96.88%  Loss: 0.064567\n",
      "Iteration: 705/1718 (41.0%)  Accuracy:96.88%  Loss: 0.064810\n",
      "Iteration: 706/1718 (41.1%)  Accuracy:100.00%  Loss: 0.002580\n",
      "Iteration: 707/1718 (41.2%)  Accuracy:100.00%  Loss: 0.031104\n",
      "Iteration: 708/1718 (41.2%)  Accuracy:93.75%  Loss: 0.133901\n",
      "Iteration: 709/1718 (41.3%)  Accuracy:100.00%  Loss: 0.013715\n",
      "Iteration: 710/1718 (41.3%)  Accuracy:100.00%  Loss: 0.023934\n",
      "Iteration: 711/1718 (41.4%)  Accuracy:93.75%  Loss: 0.208028\n",
      "Iteration: 712/1718 (41.4%)  Accuracy:96.88%  Loss: 0.080428\n",
      "Iteration: 713/1718 (41.5%)  Accuracy:96.88%  Loss: 0.042548\n",
      "Iteration: 714/1718 (41.6%)  Accuracy:96.88%  Loss: 0.116582\n",
      "Iteration: 715/1718 (41.6%)  Accuracy:100.00%  Loss: 0.013361\n",
      "Iteration: 716/1718 (41.7%)  Accuracy:100.00%  Loss: 0.005866\n",
      "Iteration: 717/1718 (41.7%)  Accuracy:96.88%  Loss: 0.124964\n",
      "Iteration: 718/1718 (41.8%)  Accuracy:100.00%  Loss: 0.038117\n",
      "Iteration: 719/1718 (41.9%)  Accuracy:93.75%  Loss: 0.188246\n",
      "Iteration: 720/1718 (41.9%)  Accuracy:100.00%  Loss: 0.015775\n",
      "Iteration: 721/1718 (42.0%)  Accuracy:96.88%  Loss: 0.047609\n",
      "Iteration: 722/1718 (42.0%)  Accuracy:100.00%  Loss: 0.019682\n",
      "Iteration: 723/1718 (42.1%)  Accuracy:100.00%  Loss: 0.004398\n",
      "Iteration: 724/1718 (42.1%)  Accuracy:100.00%  Loss: 0.013575\n",
      "Iteration: 725/1718 (42.2%)  Accuracy:100.00%  Loss: 0.037420\n",
      "Iteration: 726/1718 (42.3%)  Accuracy:96.88%  Loss: 0.091636\n",
      "Iteration: 727/1718 (42.3%)  Accuracy:96.88%  Loss: 0.083404\n",
      "Iteration: 728/1718 (42.4%)  Accuracy:100.00%  Loss: 0.028718\n",
      "Iteration: 729/1718 (42.4%)  Accuracy:100.00%  Loss: 0.009076\n",
      "Iteration: 730/1718 (42.5%)  Accuracy:100.00%  Loss: 0.012572\n",
      "Iteration: 731/1718 (42.5%)  Accuracy:100.00%  Loss: 0.013367\n",
      "Iteration: 732/1718 (42.6%)  Accuracy:93.75%  Loss: 0.146041\n",
      "Iteration: 733/1718 (42.7%)  Accuracy:100.00%  Loss: 0.005662\n",
      "Iteration: 734/1718 (42.7%)  Accuracy:96.88%  Loss: 0.067462\n",
      "Iteration: 735/1718 (42.8%)  Accuracy:93.75%  Loss: 0.133890\n",
      "Iteration: 736/1718 (42.8%)  Accuracy:100.00%  Loss: 0.006046\n",
      "Iteration: 737/1718 (42.9%)  Accuracy:100.00%  Loss: 0.002283\n",
      "Iteration: 738/1718 (43.0%)  Accuracy:93.75%  Loss: 0.457439\n",
      "Iteration: 739/1718 (43.0%)  Accuracy:96.88%  Loss: 0.084247\n",
      "Iteration: 740/1718 (43.1%)  Accuracy:96.88%  Loss: 0.036923\n",
      "Iteration: 741/1718 (43.1%)  Accuracy:100.00%  Loss: 0.009346\n",
      "Iteration: 742/1718 (43.2%)  Accuracy:96.88%  Loss: 0.110869\n",
      "Iteration: 743/1718 (43.2%)  Accuracy:100.00%  Loss: 0.026926\n",
      "Iteration: 744/1718 (43.3%)  Accuracy:100.00%  Loss: 0.025653\n",
      "Iteration: 745/1718 (43.4%)  Accuracy:93.75%  Loss: 0.322148\n",
      "Iteration: 746/1718 (43.4%)  Accuracy:100.00%  Loss: 0.009374\n",
      "Iteration: 747/1718 (43.5%)  Accuracy:96.88%  Loss: 0.113821\n",
      "Iteration: 748/1718 (43.5%)  Accuracy:96.88%  Loss: 0.063618\n",
      "Iteration: 749/1718 (43.6%)  Accuracy:100.00%  Loss: 0.074110\n",
      "Iteration: 750/1718 (43.7%)  Accuracy:100.00%  Loss: 0.020141\n",
      "Iteration: 751/1718 (43.7%)  Accuracy:100.00%  Loss: 0.050353\n",
      "Iteration: 752/1718 (43.8%)  Accuracy:100.00%  Loss: 0.021367\n",
      "Iteration: 753/1718 (43.8%)  Accuracy:96.88%  Loss: 0.046512\n",
      "Iteration: 754/1718 (43.9%)  Accuracy:100.00%  Loss: 0.003421\n",
      "Iteration: 755/1718 (43.9%)  Accuracy:96.88%  Loss: 0.085348\n",
      "Iteration: 756/1718 (44.0%)  Accuracy:100.00%  Loss: 0.010200\n",
      "Iteration: 757/1718 (44.1%)  Accuracy:96.88%  Loss: 0.076605\n",
      "Iteration: 758/1718 (44.1%)  Accuracy:90.62%  Loss: 0.252200\n",
      "Iteration: 759/1718 (44.2%)  Accuracy:100.00%  Loss: 0.019648\n",
      "Iteration: 760/1718 (44.2%)  Accuracy:100.00%  Loss: 0.032648\n",
      "Iteration: 761/1718 (44.3%)  Accuracy:96.88%  Loss: 0.078590\n",
      "Iteration: 762/1718 (44.4%)  Accuracy:100.00%  Loss: 0.002770\n",
      "Iteration: 763/1718 (44.4%)  Accuracy:100.00%  Loss: 0.013376\n",
      "Iteration: 764/1718 (44.5%)  Accuracy:96.88%  Loss: 0.072228\n",
      "Iteration: 765/1718 (44.5%)  Accuracy:100.00%  Loss: 0.007387\n",
      "Iteration: 766/1718 (44.6%)  Accuracy:96.88%  Loss: 0.096986\n",
      "Iteration: 767/1718 (44.6%)  Accuracy:100.00%  Loss: 0.002149\n",
      "Iteration: 768/1718 (44.7%)  Accuracy:100.00%  Loss: 0.023001\n",
      "Iteration: 769/1718 (44.8%)  Accuracy:100.00%  Loss: 0.001615\n",
      "Iteration: 770/1718 (44.8%)  Accuracy:96.88%  Loss: 0.116454\n",
      "Iteration: 771/1718 (44.9%)  Accuracy:93.75%  Loss: 0.138067\n",
      "Iteration: 772/1718 (44.9%)  Accuracy:100.00%  Loss: 0.002946\n",
      "Iteration: 773/1718 (45.0%)  Accuracy:96.88%  Loss: 0.128466\n",
      "Iteration: 774/1718 (45.1%)  Accuracy:96.88%  Loss: 0.139108\n",
      "Iteration: 775/1718 (45.1%)  Accuracy:100.00%  Loss: 0.028413\n",
      "Iteration: 776/1718 (45.2%)  Accuracy:100.00%  Loss: 0.008809\n",
      "Iteration: 777/1718 (45.2%)  Accuracy:100.00%  Loss: 0.011660\n",
      "Iteration: 778/1718 (45.3%)  Accuracy:100.00%  Loss: 0.005560\n",
      "Iteration: 779/1718 (45.3%)  Accuracy:96.88%  Loss: 0.046246\n",
      "Iteration: 780/1718 (45.4%)  Accuracy:100.00%  Loss: 0.025506\n",
      "Iteration: 781/1718 (45.5%)  Accuracy:100.00%  Loss: 0.021559\n",
      "Iteration: 782/1718 (45.5%)  Accuracy:96.88%  Loss: 0.088419\n",
      "Iteration: 783/1718 (45.6%)  Accuracy:100.00%  Loss: 0.024317\n",
      "Iteration: 784/1718 (45.6%)  Accuracy:100.00%  Loss: 0.006149\n",
      "Iteration: 785/1718 (45.7%)  Accuracy:100.00%  Loss: 0.003694\n",
      "Iteration: 786/1718 (45.8%)  Accuracy:100.00%  Loss: 0.011058\n",
      "Iteration: 787/1718 (45.8%)  Accuracy:100.00%  Loss: 0.023452\n",
      "Iteration: 788/1718 (45.9%)  Accuracy:93.75%  Loss: 0.158333\n",
      "Iteration: 789/1718 (45.9%)  Accuracy:100.00%  Loss: 0.004861\n",
      "Iteration: 790/1718 (46.0%)  Accuracy:100.00%  Loss: 0.015189\n",
      "Iteration: 791/1718 (46.0%)  Accuracy:96.88%  Loss: 0.028040\n",
      "Iteration: 792/1718 (46.1%)  Accuracy:100.00%  Loss: 0.039052\n",
      "Iteration: 793/1718 (46.2%)  Accuracy:100.00%  Loss: 0.014572\n",
      "Iteration: 794/1718 (46.2%)  Accuracy:96.88%  Loss: 0.079605\n",
      "Iteration: 795/1718 (46.3%)  Accuracy:100.00%  Loss: 0.004796\n",
      "Iteration: 796/1718 (46.3%)  Accuracy:100.00%  Loss: 0.017910\n",
      "Iteration: 797/1718 (46.4%)  Accuracy:100.00%  Loss: 0.020123\n",
      "Iteration: 798/1718 (46.4%)  Accuracy:100.00%  Loss: 0.008435\n",
      "Iteration: 799/1718 (46.5%)  Accuracy:100.00%  Loss: 0.010333\n",
      "Iteration: 800/1718 (46.6%)  Accuracy:100.00%  Loss: 0.005152\n",
      "Iteration: 801/1718 (46.6%)  Accuracy:93.75%  Loss: 0.207768\n",
      "Iteration: 802/1718 (46.7%)  Accuracy:100.00%  Loss: 0.002290\n",
      "Iteration: 803/1718 (46.7%)  Accuracy:100.00%  Loss: 0.011627\n",
      "Iteration: 804/1718 (46.8%)  Accuracy:96.88%  Loss: 0.096636\n",
      "Iteration: 805/1718 (46.9%)  Accuracy:93.75%  Loss: 0.114661\n",
      "Iteration: 806/1718 (46.9%)  Accuracy:93.75%  Loss: 0.124419\n",
      "Iteration: 807/1718 (47.0%)  Accuracy:100.00%  Loss: 0.010064\n",
      "Iteration: 808/1718 (47.0%)  Accuracy:93.75%  Loss: 0.071708\n",
      "Iteration: 809/1718 (47.1%)  Accuracy:100.00%  Loss: 0.011218\n",
      "Iteration: 810/1718 (47.1%)  Accuracy:100.00%  Loss: 0.009859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 811/1718 (47.2%)  Accuracy:100.00%  Loss: 0.001714\n",
      "Iteration: 812/1718 (47.3%)  Accuracy:100.00%  Loss: 0.015205\n",
      "Iteration: 813/1718 (47.3%)  Accuracy:96.88%  Loss: 0.069883\n",
      "Iteration: 814/1718 (47.4%)  Accuracy:100.00%  Loss: 0.002770\n",
      "Iteration: 815/1718 (47.4%)  Accuracy:100.00%  Loss: 0.021269\n",
      "Iteration: 816/1718 (47.5%)  Accuracy:96.88%  Loss: 0.206661\n",
      "Iteration: 817/1718 (47.6%)  Accuracy:93.75%  Loss: 0.187814\n",
      "Iteration: 818/1718 (47.6%)  Accuracy:96.88%  Loss: 0.060640\n",
      "Iteration: 819/1718 (47.7%)  Accuracy:96.88%  Loss: 0.147587\n",
      "Iteration: 820/1718 (47.7%)  Accuracy:100.00%  Loss: 0.046574\n",
      "Iteration: 821/1718 (47.8%)  Accuracy:96.88%  Loss: 0.087244\n",
      "Iteration: 822/1718 (47.8%)  Accuracy:93.75%  Loss: 0.264743\n",
      "Iteration: 823/1718 (47.9%)  Accuracy:96.88%  Loss: 0.078307\n",
      "Iteration: 824/1718 (48.0%)  Accuracy:96.88%  Loss: 0.117156\n",
      "Iteration: 825/1718 (48.0%)  Accuracy:90.62%  Loss: 0.309860\n",
      "Iteration: 826/1718 (48.1%)  Accuracy:93.75%  Loss: 0.214727\n",
      "Iteration: 827/1718 (48.1%)  Accuracy:100.00%  Loss: 0.003557\n",
      "Iteration: 828/1718 (48.2%)  Accuracy:100.00%  Loss: 0.044953\n",
      "Iteration: 829/1718 (48.3%)  Accuracy:100.00%  Loss: 0.013805\n",
      "Iteration: 830/1718 (48.3%)  Accuracy:96.88%  Loss: 0.153382\n",
      "Iteration: 831/1718 (48.4%)  Accuracy:100.00%  Loss: 0.039748\n",
      "Iteration: 832/1718 (48.4%)  Accuracy:100.00%  Loss: 0.012027\n",
      "Iteration: 833/1718 (48.5%)  Accuracy:96.88%  Loss: 0.085202\n",
      "Iteration: 834/1718 (48.5%)  Accuracy:100.00%  Loss: 0.021019\n",
      "Iteration: 835/1718 (48.6%)  Accuracy:96.88%  Loss: 0.157793\n",
      "Iteration: 836/1718 (48.7%)  Accuracy:100.00%  Loss: 0.049831\n",
      "Iteration: 837/1718 (48.7%)  Accuracy:96.88%  Loss: 0.073177\n",
      "Iteration: 838/1718 (48.8%)  Accuracy:100.00%  Loss: 0.016894\n",
      "Iteration: 839/1718 (48.8%)  Accuracy:93.75%  Loss: 0.161526\n",
      "Iteration: 840/1718 (48.9%)  Accuracy:100.00%  Loss: 0.029587\n",
      "Iteration: 841/1718 (49.0%)  Accuracy:96.88%  Loss: 0.088994\n",
      "Iteration: 842/1718 (49.0%)  Accuracy:90.62%  Loss: 0.281593\n",
      "Iteration: 843/1718 (49.1%)  Accuracy:96.88%  Loss: 0.062217\n",
      "Iteration: 844/1718 (49.1%)  Accuracy:93.75%  Loss: 0.153175\n",
      "Iteration: 845/1718 (49.2%)  Accuracy:100.00%  Loss: 0.034657\n",
      "Iteration: 846/1718 (49.2%)  Accuracy:93.75%  Loss: 0.097624\n",
      "Iteration: 847/1718 (49.3%)  Accuracy:96.88%  Loss: 0.124033\n",
      "Iteration: 848/1718 (49.4%)  Accuracy:100.00%  Loss: 0.023226\n",
      "Iteration: 849/1718 (49.4%)  Accuracy:100.00%  Loss: 0.025941\n",
      "Iteration: 850/1718 (49.5%)  Accuracy:100.00%  Loss: 0.040250\n",
      "Iteration: 851/1718 (49.5%)  Accuracy:100.00%  Loss: 0.017529\n",
      "Iteration: 852/1718 (49.6%)  Accuracy:100.00%  Loss: 0.018556\n",
      "Iteration: 853/1718 (49.7%)  Accuracy:96.88%  Loss: 0.052169\n",
      "Iteration: 854/1718 (49.7%)  Accuracy:100.00%  Loss: 0.030750\n",
      "Iteration: 855/1718 (49.8%)  Accuracy:93.75%  Loss: 0.205819\n",
      "Iteration: 856/1718 (49.8%)  Accuracy:100.00%  Loss: 0.002869\n",
      "Iteration: 857/1718 (49.9%)  Accuracy:96.88%  Loss: 0.088401\n",
      "Iteration: 858/1718 (49.9%)  Accuracy:100.00%  Loss: 0.004088\n",
      "Iteration: 859/1718 (50.0%)  Accuracy:100.00%  Loss: 0.013836\n",
      "Iteration: 860/1718 (50.1%)  Accuracy:100.00%  Loss: 0.004318\n",
      "Iteration: 861/1718 (50.1%)  Accuracy:100.00%  Loss: 0.003740\n",
      "Iteration: 862/1718 (50.2%)  Accuracy:96.88%  Loss: 0.095952\n",
      "Iteration: 863/1718 (50.2%)  Accuracy:100.00%  Loss: 0.009090\n",
      "Iteration: 864/1718 (50.3%)  Accuracy:100.00%  Loss: 0.002519\n",
      "Iteration: 865/1718 (50.3%)  Accuracy:100.00%  Loss: 0.036181\n",
      "Iteration: 866/1718 (50.4%)  Accuracy:100.00%  Loss: 0.035157\n",
      "Iteration: 867/1718 (50.5%)  Accuracy:93.75%  Loss: 0.132138\n",
      "Iteration: 868/1718 (50.5%)  Accuracy:96.88%  Loss: 0.050931\n",
      "Iteration: 869/1718 (50.6%)  Accuracy:100.00%  Loss: 0.015958\n",
      "Iteration: 870/1718 (50.6%)  Accuracy:96.88%  Loss: 0.268759\n",
      "Iteration: 871/1718 (50.7%)  Accuracy:90.62%  Loss: 0.224859\n",
      "Iteration: 872/1718 (50.8%)  Accuracy:96.88%  Loss: 0.030404\n",
      "Iteration: 873/1718 (50.8%)  Accuracy:100.00%  Loss: 0.003908\n",
      "Iteration: 874/1718 (50.9%)  Accuracy:93.75%  Loss: 0.094258\n",
      "Iteration: 875/1718 (50.9%)  Accuracy:100.00%  Loss: 0.015634\n",
      "Iteration: 876/1718 (51.0%)  Accuracy:100.00%  Loss: 0.032292\n",
      "Iteration: 877/1718 (51.0%)  Accuracy:96.88%  Loss: 0.059713\n",
      "Iteration: 878/1718 (51.1%)  Accuracy:93.75%  Loss: 0.163327\n",
      "Iteration: 879/1718 (51.2%)  Accuracy:100.00%  Loss: 0.021309\n",
      "Iteration: 880/1718 (51.2%)  Accuracy:100.00%  Loss: 0.016833\n",
      "Iteration: 881/1718 (51.3%)  Accuracy:100.00%  Loss: 0.006554\n",
      "Iteration: 882/1718 (51.3%)  Accuracy:100.00%  Loss: 0.011754\n",
      "Iteration: 883/1718 (51.4%)  Accuracy:100.00%  Loss: 0.035298\n",
      "Iteration: 884/1718 (51.5%)  Accuracy:96.88%  Loss: 0.069377\n",
      "Iteration: 885/1718 (51.5%)  Accuracy:100.00%  Loss: 0.031536\n",
      "Iteration: 886/1718 (51.6%)  Accuracy:96.88%  Loss: 0.062501\n",
      "Iteration: 887/1718 (51.6%)  Accuracy:96.88%  Loss: 0.048218\n",
      "Iteration: 888/1718 (51.7%)  Accuracy:100.00%  Loss: 0.017422\n",
      "Iteration: 889/1718 (51.7%)  Accuracy:96.88%  Loss: 0.044593\n",
      "Iteration: 890/1718 (51.8%)  Accuracy:100.00%  Loss: 0.000950\n",
      "Iteration: 891/1718 (51.9%)  Accuracy:96.88%  Loss: 0.144478\n",
      "Iteration: 892/1718 (51.9%)  Accuracy:96.88%  Loss: 0.041503\n",
      "Iteration: 893/1718 (52.0%)  Accuracy:96.88%  Loss: 0.088783\n",
      "Iteration: 894/1718 (52.0%)  Accuracy:100.00%  Loss: 0.004655\n",
      "Iteration: 895/1718 (52.1%)  Accuracy:93.75%  Loss: 0.100034\n",
      "Iteration: 896/1718 (52.2%)  Accuracy:100.00%  Loss: 0.006090\n",
      "Iteration: 897/1718 (52.2%)  Accuracy:100.00%  Loss: 0.000997\n",
      "Iteration: 898/1718 (52.3%)  Accuracy:100.00%  Loss: 0.014750\n",
      "Iteration: 899/1718 (52.3%)  Accuracy:87.50%  Loss: 0.261866\n",
      "Iteration: 900/1718 (52.4%)  Accuracy:96.88%  Loss: 0.130128\n",
      "Iteration: 901/1718 (52.4%)  Accuracy:96.88%  Loss: 0.069404\n",
      "Iteration: 902/1718 (52.5%)  Accuracy:100.00%  Loss: 0.006278\n",
      "Iteration: 903/1718 (52.6%)  Accuracy:100.00%  Loss: 0.003968\n",
      "Iteration: 904/1718 (52.6%)  Accuracy:100.00%  Loss: 0.004999\n",
      "Iteration: 905/1718 (52.7%)  Accuracy:93.75%  Loss: 0.114036\n",
      "Iteration: 906/1718 (52.7%)  Accuracy:96.88%  Loss: 0.026570\n",
      "Iteration: 907/1718 (52.8%)  Accuracy:100.00%  Loss: 0.029182\n",
      "Iteration: 908/1718 (52.9%)  Accuracy:96.88%  Loss: 0.071293\n",
      "Iteration: 909/1718 (52.9%)  Accuracy:93.75%  Loss: 0.309409\n",
      "Iteration: 910/1718 (53.0%)  Accuracy:100.00%  Loss: 0.001101\n",
      "Iteration: 911/1718 (53.0%)  Accuracy:100.00%  Loss: 0.022540\n",
      "Iteration: 912/1718 (53.1%)  Accuracy:96.88%  Loss: 0.033497\n",
      "Iteration: 913/1718 (53.1%)  Accuracy:96.88%  Loss: 0.151146\n",
      "Iteration: 914/1718 (53.2%)  Accuracy:100.00%  Loss: 0.011715\n",
      "Iteration: 915/1718 (53.3%)  Accuracy:96.88%  Loss: 0.079073\n",
      "Iteration: 916/1718 (53.3%)  Accuracy:100.00%  Loss: 0.003571\n",
      "Iteration: 917/1718 (53.4%)  Accuracy:96.88%  Loss: 0.079003\n",
      "Iteration: 918/1718 (53.4%)  Accuracy:100.00%  Loss: 0.033936\n",
      "Iteration: 919/1718 (53.5%)  Accuracy:90.62%  Loss: 0.345443\n",
      "Iteration: 920/1718 (53.6%)  Accuracy:100.00%  Loss: 0.056046\n",
      "Iteration: 921/1718 (53.6%)  Accuracy:100.00%  Loss: 0.010800\n",
      "Iteration: 922/1718 (53.7%)  Accuracy:96.88%  Loss: 0.148508\n",
      "Iteration: 923/1718 (53.7%)  Accuracy:100.00%  Loss: 0.021486\n",
      "Iteration: 924/1718 (53.8%)  Accuracy:96.88%  Loss: 0.051064\n",
      "Iteration: 925/1718 (53.8%)  Accuracy:100.00%  Loss: 0.032228\n",
      "Iteration: 926/1718 (53.9%)  Accuracy:93.75%  Loss: 0.120216\n",
      "Iteration: 927/1718 (54.0%)  Accuracy:100.00%  Loss: 0.022988\n",
      "Iteration: 928/1718 (54.0%)  Accuracy:93.75%  Loss: 0.148089\n",
      "Iteration: 929/1718 (54.1%)  Accuracy:100.00%  Loss: 0.057582\n",
      "Iteration: 930/1718 (54.1%)  Accuracy:96.88%  Loss: 0.182604\n",
      "Iteration: 931/1718 (54.2%)  Accuracy:90.62%  Loss: 0.190923\n",
      "Iteration: 932/1718 (54.2%)  Accuracy:96.88%  Loss: 0.049804\n",
      "Iteration: 933/1718 (54.3%)  Accuracy:96.88%  Loss: 0.295311\n",
      "Iteration: 934/1718 (54.4%)  Accuracy:93.75%  Loss: 0.165750\n",
      "Iteration: 935/1718 (54.4%)  Accuracy:100.00%  Loss: 0.011101\n",
      "Iteration: 936/1718 (54.5%)  Accuracy:93.75%  Loss: 0.190990\n",
      "Iteration: 937/1718 (54.5%)  Accuracy:100.00%  Loss: 0.004866\n",
      "Iteration: 938/1718 (54.6%)  Accuracy:100.00%  Loss: 0.008154\n",
      "Iteration: 939/1718 (54.7%)  Accuracy:100.00%  Loss: 0.022439\n",
      "Iteration: 940/1718 (54.7%)  Accuracy:100.00%  Loss: 0.055573\n",
      "Iteration: 941/1718 (54.8%)  Accuracy:93.75%  Loss: 0.115219\n",
      "Iteration: 942/1718 (54.8%)  Accuracy:100.00%  Loss: 0.068409\n",
      "Iteration: 943/1718 (54.9%)  Accuracy:100.00%  Loss: 0.009862\n",
      "Iteration: 944/1718 (54.9%)  Accuracy:96.88%  Loss: 0.053866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 945/1718 (55.0%)  Accuracy:90.62%  Loss: 0.225616\n",
      "Iteration: 946/1718 (55.1%)  Accuracy:96.88%  Loss: 0.099419\n",
      "Iteration: 947/1718 (55.1%)  Accuracy:100.00%  Loss: 0.032925\n",
      "Iteration: 948/1718 (55.2%)  Accuracy:90.62%  Loss: 0.248685\n",
      "Iteration: 949/1718 (55.2%)  Accuracy:96.88%  Loss: 0.070777\n",
      "Iteration: 950/1718 (55.3%)  Accuracy:93.75%  Loss: 0.234852\n",
      "Iteration: 951/1718 (55.4%)  Accuracy:100.00%  Loss: 0.017529\n",
      "Iteration: 952/1718 (55.4%)  Accuracy:100.00%  Loss: 0.017164\n",
      "Iteration: 953/1718 (55.5%)  Accuracy:96.88%  Loss: 0.096801\n",
      "Iteration: 954/1718 (55.5%)  Accuracy:93.75%  Loss: 0.111242\n",
      "Iteration: 955/1718 (55.6%)  Accuracy:100.00%  Loss: 0.027138\n",
      "Iteration: 956/1718 (55.6%)  Accuracy:96.88%  Loss: 0.049999\n",
      "Iteration: 957/1718 (55.7%)  Accuracy:96.88%  Loss: 0.069634\n",
      "Iteration: 958/1718 (55.8%)  Accuracy:96.88%  Loss: 0.134890\n",
      "Iteration: 959/1718 (55.8%)  Accuracy:100.00%  Loss: 0.008599\n",
      "Iteration: 960/1718 (55.9%)  Accuracy:100.00%  Loss: 0.014576\n",
      "Iteration: 961/1718 (55.9%)  Accuracy:96.88%  Loss: 0.083721\n",
      "Iteration: 962/1718 (56.0%)  Accuracy:96.88%  Loss: 0.036814\n",
      "Iteration: 963/1718 (56.1%)  Accuracy:96.88%  Loss: 0.028284\n",
      "Iteration: 964/1718 (56.1%)  Accuracy:100.00%  Loss: 0.054994\n",
      "Iteration: 965/1718 (56.2%)  Accuracy:100.00%  Loss: 0.000956\n",
      "Iteration: 966/1718 (56.2%)  Accuracy:100.00%  Loss: 0.027040\n",
      "Iteration: 967/1718 (56.3%)  Accuracy:100.00%  Loss: 0.004770\n",
      "Iteration: 968/1718 (56.3%)  Accuracy:100.00%  Loss: 0.003013\n",
      "Iteration: 969/1718 (56.4%)  Accuracy:96.88%  Loss: 0.112365\n",
      "Iteration: 970/1718 (56.5%)  Accuracy:100.00%  Loss: 0.008884\n",
      "Iteration: 971/1718 (56.5%)  Accuracy:100.00%  Loss: 0.025749\n",
      "Iteration: 972/1718 (56.6%)  Accuracy:100.00%  Loss: 0.004161\n",
      "Iteration: 973/1718 (56.6%)  Accuracy:96.88%  Loss: 0.042546\n",
      "Iteration: 974/1718 (56.7%)  Accuracy:100.00%  Loss: 0.029533\n",
      "Iteration: 975/1718 (56.8%)  Accuracy:100.00%  Loss: 0.000714\n",
      "Iteration: 976/1718 (56.8%)  Accuracy:96.88%  Loss: 0.081274\n",
      "Iteration: 977/1718 (56.9%)  Accuracy:100.00%  Loss: 0.008124\n",
      "Iteration: 978/1718 (56.9%)  Accuracy:96.88%  Loss: 0.026857\n",
      "Iteration: 979/1718 (57.0%)  Accuracy:100.00%  Loss: 0.023857\n",
      "Iteration: 980/1718 (57.0%)  Accuracy:100.00%  Loss: 0.003920\n",
      "Iteration: 981/1718 (57.1%)  Accuracy:100.00%  Loss: 0.025361\n",
      "Iteration: 982/1718 (57.2%)  Accuracy:96.88%  Loss: 0.147466\n",
      "Iteration: 983/1718 (57.2%)  Accuracy:96.88%  Loss: 0.025899\n",
      "Iteration: 984/1718 (57.3%)  Accuracy:96.88%  Loss: 0.033981\n",
      "Iteration: 985/1718 (57.3%)  Accuracy:100.00%  Loss: 0.006914\n",
      "Iteration: 986/1718 (57.4%)  Accuracy:100.00%  Loss: 0.001341\n",
      "Iteration: 987/1718 (57.5%)  Accuracy:96.88%  Loss: 0.093373\n",
      "Iteration: 988/1718 (57.5%)  Accuracy:100.00%  Loss: 0.018345\n",
      "Iteration: 989/1718 (57.6%)  Accuracy:100.00%  Loss: 0.017136\n",
      "Iteration: 990/1718 (57.6%)  Accuracy:96.88%  Loss: 0.044678\n",
      "Iteration: 991/1718 (57.7%)  Accuracy:100.00%  Loss: 0.025647\n",
      "Iteration: 992/1718 (57.7%)  Accuracy:96.88%  Loss: 0.106217\n",
      "Iteration: 993/1718 (57.8%)  Accuracy:93.75%  Loss: 0.103416\n",
      "Iteration: 994/1718 (57.9%)  Accuracy:93.75%  Loss: 0.218739\n",
      "Iteration: 995/1718 (57.9%)  Accuracy:100.00%  Loss: 0.001447\n",
      "Iteration: 996/1718 (58.0%)  Accuracy:93.75%  Loss: 0.090066\n",
      "Iteration: 997/1718 (58.0%)  Accuracy:100.00%  Loss: 0.002699\n",
      "Iteration: 998/1718 (58.1%)  Accuracy:100.00%  Loss: 0.027041\n",
      "Iteration: 999/1718 (58.1%)  Accuracy:96.88%  Loss: 0.074401\n",
      "Iteration: 1000/1718 (58.2%)  Accuracy:96.88%  Loss: 0.172600\n",
      "Iteration: 1001/1718 (58.3%)  Accuracy:96.88%  Loss: 0.128782\n",
      "Iteration: 1002/1718 (58.3%)  Accuracy:87.50%  Loss: 0.390003\n",
      "Iteration: 1003/1718 (58.4%)  Accuracy:93.75%  Loss: 0.123516\n",
      "Iteration: 1004/1718 (58.4%)  Accuracy:100.00%  Loss: 0.003800\n",
      "Iteration: 1005/1718 (58.5%)  Accuracy:100.00%  Loss: 0.011832\n",
      "Iteration: 1006/1718 (58.6%)  Accuracy:100.00%  Loss: 0.001375\n",
      "Iteration: 1007/1718 (58.6%)  Accuracy:90.62%  Loss: 0.211051\n",
      "Iteration: 1008/1718 (58.7%)  Accuracy:100.00%  Loss: 0.021457\n",
      "Iteration: 1009/1718 (58.7%)  Accuracy:100.00%  Loss: 0.035748\n",
      "Iteration: 1010/1718 (58.8%)  Accuracy:93.75%  Loss: 0.211460\n",
      "Iteration: 1011/1718 (58.8%)  Accuracy:100.00%  Loss: 0.005516\n",
      "Iteration: 1012/1718 (58.9%)  Accuracy:100.00%  Loss: 0.020252\n",
      "Iteration: 1013/1718 (59.0%)  Accuracy:96.88%  Loss: 0.081932\n",
      "Iteration: 1014/1718 (59.0%)  Accuracy:96.88%  Loss: 0.268579\n",
      "Iteration: 1015/1718 (59.1%)  Accuracy:96.88%  Loss: 0.049746\n",
      "Iteration: 1016/1718 (59.1%)  Accuracy:96.88%  Loss: 0.052632\n",
      "Iteration: 1017/1718 (59.2%)  Accuracy:100.00%  Loss: 0.011739\n",
      "Iteration: 1018/1718 (59.3%)  Accuracy:100.00%  Loss: 0.012785\n",
      "Iteration: 1019/1718 (59.3%)  Accuracy:96.88%  Loss: 0.127008\n",
      "Iteration: 1020/1718 (59.4%)  Accuracy:100.00%  Loss: 0.001994\n",
      "Iteration: 1021/1718 (59.4%)  Accuracy:96.88%  Loss: 0.096919\n",
      "Iteration: 1022/1718 (59.5%)  Accuracy:96.88%  Loss: 0.087035\n",
      "Iteration: 1023/1718 (59.5%)  Accuracy:93.75%  Loss: 0.089307\n",
      "Iteration: 1024/1718 (59.6%)  Accuracy:96.88%  Loss: 0.079502\n",
      "Iteration: 1025/1718 (59.7%)  Accuracy:93.75%  Loss: 0.107423\n",
      "Iteration: 1026/1718 (59.7%)  Accuracy:100.00%  Loss: 0.005554\n",
      "Iteration: 1027/1718 (59.8%)  Accuracy:100.00%  Loss: 0.022178\n",
      "Iteration: 1028/1718 (59.8%)  Accuracy:100.00%  Loss: 0.037395\n",
      "Iteration: 1029/1718 (59.9%)  Accuracy:100.00%  Loss: 0.035092\n",
      "Iteration: 1030/1718 (60.0%)  Accuracy:93.75%  Loss: 0.182290\n",
      "Iteration: 1031/1718 (60.0%)  Accuracy:100.00%  Loss: 0.008342\n",
      "Iteration: 1032/1718 (60.1%)  Accuracy:100.00%  Loss: 0.027937\n",
      "Iteration: 1033/1718 (60.1%)  Accuracy:100.00%  Loss: 0.002502\n",
      "Iteration: 1034/1718 (60.2%)  Accuracy:100.00%  Loss: 0.025250\n",
      "Iteration: 1035/1718 (60.2%)  Accuracy:100.00%  Loss: 0.016695\n",
      "Iteration: 1036/1718 (60.3%)  Accuracy:96.88%  Loss: 0.120496\n",
      "Iteration: 1037/1718 (60.4%)  Accuracy:100.00%  Loss: 0.020351\n",
      "Iteration: 1038/1718 (60.4%)  Accuracy:93.75%  Loss: 0.077464\n",
      "Iteration: 1039/1718 (60.5%)  Accuracy:96.88%  Loss: 0.047298\n",
      "Iteration: 1040/1718 (60.5%)  Accuracy:96.88%  Loss: 0.063347\n",
      "Iteration: 1041/1718 (60.6%)  Accuracy:96.88%  Loss: 0.084995\n",
      "Iteration: 1042/1718 (60.7%)  Accuracy:100.00%  Loss: 0.008392\n",
      "Iteration: 1043/1718 (60.7%)  Accuracy:96.88%  Loss: 0.044531\n",
      "Iteration: 1044/1718 (60.8%)  Accuracy:100.00%  Loss: 0.020954\n",
      "Iteration: 1045/1718 (60.8%)  Accuracy:96.88%  Loss: 0.036056\n",
      "Iteration: 1046/1718 (60.9%)  Accuracy:93.75%  Loss: 0.157295\n",
      "Iteration: 1047/1718 (60.9%)  Accuracy:100.00%  Loss: 0.021785\n",
      "Iteration: 1048/1718 (61.0%)  Accuracy:100.00%  Loss: 0.014262\n",
      "Iteration: 1049/1718 (61.1%)  Accuracy:100.00%  Loss: 0.006057\n",
      "Iteration: 1050/1718 (61.1%)  Accuracy:100.00%  Loss: 0.001611\n",
      "Iteration: 1051/1718 (61.2%)  Accuracy:100.00%  Loss: 0.030078\n",
      "Iteration: 1052/1718 (61.2%)  Accuracy:93.75%  Loss: 0.164401\n",
      "Iteration: 1053/1718 (61.3%)  Accuracy:100.00%  Loss: 0.019465\n",
      "Iteration: 1054/1718 (61.4%)  Accuracy:93.75%  Loss: 0.177460\n",
      "Iteration: 1055/1718 (61.4%)  Accuracy:100.00%  Loss: 0.004317\n",
      "Iteration: 1056/1718 (61.5%)  Accuracy:96.88%  Loss: 0.048859\n",
      "Iteration: 1057/1718 (61.5%)  Accuracy:100.00%  Loss: 0.002312\n",
      "Iteration: 1058/1718 (61.6%)  Accuracy:96.88%  Loss: 0.044232\n",
      "Iteration: 1059/1718 (61.6%)  Accuracy:96.88%  Loss: 0.088081\n",
      "Iteration: 1060/1718 (61.7%)  Accuracy:96.88%  Loss: 0.233433\n",
      "Iteration: 1061/1718 (61.8%)  Accuracy:100.00%  Loss: 0.006457\n",
      "Iteration: 1062/1718 (61.8%)  Accuracy:100.00%  Loss: 0.009147\n",
      "Iteration: 1063/1718 (61.9%)  Accuracy:93.75%  Loss: 0.160103\n",
      "Iteration: 1064/1718 (61.9%)  Accuracy:100.00%  Loss: 0.003239\n",
      "Iteration: 1065/1718 (62.0%)  Accuracy:100.00%  Loss: 0.001383\n",
      "Iteration: 1066/1718 (62.0%)  Accuracy:100.00%  Loss: 0.004801\n",
      "Iteration: 1067/1718 (62.1%)  Accuracy:96.88%  Loss: 0.103430\n",
      "Iteration: 1068/1718 (62.2%)  Accuracy:96.88%  Loss: 0.045088\n",
      "Iteration: 1069/1718 (62.2%)  Accuracy:96.88%  Loss: 0.186637\n",
      "Iteration: 1070/1718 (62.3%)  Accuracy:96.88%  Loss: 0.066885\n",
      "Iteration: 1071/1718 (62.3%)  Accuracy:96.88%  Loss: 0.038358\n",
      "Iteration: 1072/1718 (62.4%)  Accuracy:100.00%  Loss: 0.018069\n",
      "Iteration: 1073/1718 (62.5%)  Accuracy:96.88%  Loss: 0.076618\n",
      "Iteration: 1074/1718 (62.5%)  Accuracy:90.62%  Loss: 0.283668\n",
      "Iteration: 1075/1718 (62.6%)  Accuracy:93.75%  Loss: 0.093072\n",
      "Iteration: 1076/1718 (62.6%)  Accuracy:100.00%  Loss: 0.033244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1077/1718 (62.7%)  Accuracy:96.88%  Loss: 0.141513\n",
      "Iteration: 1078/1718 (62.7%)  Accuracy:100.00%  Loss: 0.020417\n",
      "Iteration: 1079/1718 (62.8%)  Accuracy:96.88%  Loss: 0.042279\n",
      "Iteration: 1080/1718 (62.9%)  Accuracy:96.88%  Loss: 0.136795\n",
      "Iteration: 1081/1718 (62.9%)  Accuracy:100.00%  Loss: 0.053097\n",
      "Iteration: 1082/1718 (63.0%)  Accuracy:93.75%  Loss: 0.283253\n",
      "Iteration: 1083/1718 (63.0%)  Accuracy:93.75%  Loss: 0.187036\n",
      "Iteration: 1084/1718 (63.1%)  Accuracy:96.88%  Loss: 0.076057\n",
      "Iteration: 1085/1718 (63.2%)  Accuracy:96.88%  Loss: 0.139121\n",
      "Iteration: 1086/1718 (63.2%)  Accuracy:100.00%  Loss: 0.018224\n",
      "Iteration: 1087/1718 (63.3%)  Accuracy:87.50%  Loss: 0.269435\n",
      "Iteration: 1088/1718 (63.3%)  Accuracy:100.00%  Loss: 0.007833\n",
      "Iteration: 1089/1718 (63.4%)  Accuracy:100.00%  Loss: 0.008361\n",
      "Iteration: 1090/1718 (63.4%)  Accuracy:96.88%  Loss: 0.075066\n",
      "Iteration: 1091/1718 (63.5%)  Accuracy:96.88%  Loss: 0.120282\n",
      "Iteration: 1092/1718 (63.6%)  Accuracy:100.00%  Loss: 0.023516\n",
      "Iteration: 1093/1718 (63.6%)  Accuracy:100.00%  Loss: 0.004881\n",
      "Iteration: 1094/1718 (63.7%)  Accuracy:96.88%  Loss: 0.093612\n",
      "Iteration: 1095/1718 (63.7%)  Accuracy:96.88%  Loss: 0.092046\n",
      "Iteration: 1096/1718 (63.8%)  Accuracy:100.00%  Loss: 0.016571\n",
      "Iteration: 1097/1718 (63.9%)  Accuracy:100.00%  Loss: 0.026453\n",
      "Iteration: 1098/1718 (63.9%)  Accuracy:96.88%  Loss: 0.087016\n",
      "Iteration: 1099/1718 (64.0%)  Accuracy:100.00%  Loss: 0.011790\n",
      "Iteration: 1100/1718 (64.0%)  Accuracy:93.75%  Loss: 0.195826\n",
      "Iteration: 1101/1718 (64.1%)  Accuracy:93.75%  Loss: 0.105345\n",
      "Iteration: 1102/1718 (64.1%)  Accuracy:93.75%  Loss: 0.175630\n",
      "Iteration: 1103/1718 (64.2%)  Accuracy:96.88%  Loss: 0.111601\n",
      "Iteration: 1104/1718 (64.3%)  Accuracy:93.75%  Loss: 0.266712\n",
      "Iteration: 1105/1718 (64.3%)  Accuracy:96.88%  Loss: 0.044121\n",
      "Iteration: 1106/1718 (64.4%)  Accuracy:96.88%  Loss: 0.075690\n",
      "Iteration: 1107/1718 (64.4%)  Accuracy:93.75%  Loss: 0.294702\n",
      "Iteration: 1108/1718 (64.5%)  Accuracy:96.88%  Loss: 0.063161\n",
      "Iteration: 1109/1718 (64.6%)  Accuracy:100.00%  Loss: 0.014065\n",
      "Iteration: 1110/1718 (64.6%)  Accuracy:100.00%  Loss: 0.025096\n",
      "Iteration: 1111/1718 (64.7%)  Accuracy:96.88%  Loss: 0.079830\n",
      "Iteration: 1112/1718 (64.7%)  Accuracy:100.00%  Loss: 0.026084\n",
      "Iteration: 1113/1718 (64.8%)  Accuracy:100.00%  Loss: 0.030194\n",
      "Iteration: 1114/1718 (64.8%)  Accuracy:100.00%  Loss: 0.014273\n",
      "Iteration: 1115/1718 (64.9%)  Accuracy:96.88%  Loss: 0.066962\n",
      "Iteration: 1116/1718 (65.0%)  Accuracy:100.00%  Loss: 0.026170\n",
      "Iteration: 1117/1718 (65.0%)  Accuracy:100.00%  Loss: 0.022318\n",
      "Iteration: 1118/1718 (65.1%)  Accuracy:96.88%  Loss: 0.101200\n",
      "Iteration: 1119/1718 (65.1%)  Accuracy:90.62%  Loss: 0.256136\n",
      "Iteration: 1120/1718 (65.2%)  Accuracy:96.88%  Loss: 0.033921\n",
      "Iteration: 1121/1718 (65.3%)  Accuracy:100.00%  Loss: 0.028332\n",
      "Iteration: 1122/1718 (65.3%)  Accuracy:96.88%  Loss: 0.039773\n",
      "Iteration: 1123/1718 (65.4%)  Accuracy:96.88%  Loss: 0.121958\n",
      "Iteration: 1124/1718 (65.4%)  Accuracy:96.88%  Loss: 0.122708\n",
      "Iteration: 1125/1718 (65.5%)  Accuracy:100.00%  Loss: 0.049478\n",
      "Iteration: 1126/1718 (65.5%)  Accuracy:100.00%  Loss: 0.030059\n",
      "Iteration: 1127/1718 (65.6%)  Accuracy:100.00%  Loss: 0.007174\n",
      "Iteration: 1128/1718 (65.7%)  Accuracy:100.00%  Loss: 0.010880\n",
      "Iteration: 1129/1718 (65.7%)  Accuracy:100.00%  Loss: 0.026970\n",
      "Iteration: 1130/1718 (65.8%)  Accuracy:100.00%  Loss: 0.000735\n",
      "Iteration: 1131/1718 (65.8%)  Accuracy:100.00%  Loss: 0.009244\n",
      "Iteration: 1132/1718 (65.9%)  Accuracy:100.00%  Loss: 0.014510\n",
      "Iteration: 1133/1718 (65.9%)  Accuracy:100.00%  Loss: 0.003896\n",
      "Iteration: 1134/1718 (66.0%)  Accuracy:100.00%  Loss: 0.028907\n",
      "Iteration: 1135/1718 (66.1%)  Accuracy:100.00%  Loss: 0.000964\n",
      "Iteration: 1136/1718 (66.1%)  Accuracy:100.00%  Loss: 0.018288\n",
      "Iteration: 1137/1718 (66.2%)  Accuracy:100.00%  Loss: 0.032943\n",
      "Iteration: 1138/1718 (66.2%)  Accuracy:100.00%  Loss: 0.001942\n",
      "Iteration: 1139/1718 (66.3%)  Accuracy:100.00%  Loss: 0.012162\n",
      "Iteration: 1140/1718 (66.4%)  Accuracy:100.00%  Loss: 0.008880\n",
      "Iteration: 1141/1718 (66.4%)  Accuracy:100.00%  Loss: 0.017542\n",
      "Iteration: 1142/1718 (66.5%)  Accuracy:100.00%  Loss: 0.002394\n",
      "Iteration: 1143/1718 (66.5%)  Accuracy:96.88%  Loss: 0.040542\n",
      "Iteration: 1144/1718 (66.6%)  Accuracy:96.88%  Loss: 0.061589\n",
      "Iteration: 1145/1718 (66.6%)  Accuracy:100.00%  Loss: 0.008479\n",
      "Iteration: 1146/1718 (66.7%)  Accuracy:100.00%  Loss: 0.005166\n",
      "Iteration: 1147/1718 (66.8%)  Accuracy:100.00%  Loss: 0.000869\n",
      "Iteration: 1148/1718 (66.8%)  Accuracy:96.88%  Loss: 0.160484\n",
      "Iteration: 1149/1718 (66.9%)  Accuracy:100.00%  Loss: 0.001774\n",
      "Iteration: 1150/1718 (66.9%)  Accuracy:100.00%  Loss: 0.003778\n",
      "Iteration: 1151/1718 (67.0%)  Accuracy:100.00%  Loss: 0.000622\n",
      "Iteration: 1152/1718 (67.1%)  Accuracy:100.00%  Loss: 0.019873\n",
      "Iteration: 1153/1718 (67.1%)  Accuracy:87.50%  Loss: 0.296527\n",
      "Iteration: 1154/1718 (67.2%)  Accuracy:100.00%  Loss: 0.013648\n",
      "Iteration: 1155/1718 (67.2%)  Accuracy:100.00%  Loss: 0.030081\n",
      "Iteration: 1156/1718 (67.3%)  Accuracy:96.88%  Loss: 0.103470\n",
      "Iteration: 1157/1718 (67.3%)  Accuracy:96.88%  Loss: 0.036243\n",
      "Iteration: 1158/1718 (67.4%)  Accuracy:96.88%  Loss: 0.149591\n",
      "Iteration: 1159/1718 (67.5%)  Accuracy:100.00%  Loss: 0.003338\n",
      "Iteration: 1160/1718 (67.5%)  Accuracy:100.00%  Loss: 0.007526\n",
      "Iteration: 1161/1718 (67.6%)  Accuracy:100.00%  Loss: 0.006906\n",
      "Iteration: 1162/1718 (67.6%)  Accuracy:100.00%  Loss: 0.020704\n",
      "Iteration: 1163/1718 (67.7%)  Accuracy:96.88%  Loss: 0.044747\n",
      "Iteration: 1164/1718 (67.8%)  Accuracy:100.00%  Loss: 0.033945\n",
      "Iteration: 1165/1718 (67.8%)  Accuracy:100.00%  Loss: 0.030165\n",
      "Iteration: 1166/1718 (67.9%)  Accuracy:96.88%  Loss: 0.037677\n",
      "Iteration: 1167/1718 (67.9%)  Accuracy:100.00%  Loss: 0.022292\n",
      "Iteration: 1168/1718 (68.0%)  Accuracy:100.00%  Loss: 0.007478\n",
      "Iteration: 1169/1718 (68.0%)  Accuracy:100.00%  Loss: 0.026133\n",
      "Iteration: 1170/1718 (68.1%)  Accuracy:96.88%  Loss: 0.043841\n",
      "Iteration: 1171/1718 (68.2%)  Accuracy:96.88%  Loss: 0.065474\n",
      "Iteration: 1172/1718 (68.2%)  Accuracy:100.00%  Loss: 0.004497\n",
      "Iteration: 1173/1718 (68.3%)  Accuracy:100.00%  Loss: 0.018841\n",
      "Iteration: 1174/1718 (68.3%)  Accuracy:96.88%  Loss: 0.083874\n",
      "Iteration: 1175/1718 (68.4%)  Accuracy:100.00%  Loss: 0.003152\n",
      "Iteration: 1176/1718 (68.5%)  Accuracy:100.00%  Loss: 0.001086\n",
      "Iteration: 1177/1718 (68.5%)  Accuracy:100.00%  Loss: 0.003702\n",
      "Iteration: 1178/1718 (68.6%)  Accuracy:100.00%  Loss: 0.005754\n",
      "Iteration: 1179/1718 (68.6%)  Accuracy:93.75%  Loss: 0.169680\n",
      "Iteration: 1180/1718 (68.7%)  Accuracy:96.88%  Loss: 0.050075\n",
      "Iteration: 1181/1718 (68.7%)  Accuracy:100.00%  Loss: 0.016632\n",
      "Iteration: 1182/1718 (68.8%)  Accuracy:96.88%  Loss: 0.061454\n",
      "Iteration: 1183/1718 (68.9%)  Accuracy:90.62%  Loss: 0.276491\n",
      "Iteration: 1184/1718 (68.9%)  Accuracy:100.00%  Loss: 0.021815\n",
      "Iteration: 1185/1718 (69.0%)  Accuracy:100.00%  Loss: 0.013594\n",
      "Iteration: 1186/1718 (69.0%)  Accuracy:96.88%  Loss: 0.101791\n",
      "Iteration: 1187/1718 (69.1%)  Accuracy:100.00%  Loss: 0.001500\n",
      "Iteration: 1188/1718 (69.2%)  Accuracy:96.88%  Loss: 0.156170\n",
      "Iteration: 1189/1718 (69.2%)  Accuracy:96.88%  Loss: 0.051415\n",
      "Iteration: 1190/1718 (69.3%)  Accuracy:100.00%  Loss: 0.030505\n",
      "Iteration: 1191/1718 (69.3%)  Accuracy:100.00%  Loss: 0.012243\n",
      "Iteration: 1192/1718 (69.4%)  Accuracy:96.88%  Loss: 0.144691\n",
      "Iteration: 1193/1718 (69.4%)  Accuracy:100.00%  Loss: 0.010124\n",
      "Iteration: 1194/1718 (69.5%)  Accuracy:96.88%  Loss: 0.132179\n",
      "Iteration: 1195/1718 (69.6%)  Accuracy:93.75%  Loss: 0.208330\n",
      "Iteration: 1196/1718 (69.6%)  Accuracy:100.00%  Loss: 0.011281\n",
      "Iteration: 1197/1718 (69.7%)  Accuracy:96.88%  Loss: 0.102371\n",
      "Iteration: 1198/1718 (69.7%)  Accuracy:96.88%  Loss: 0.071852\n",
      "Iteration: 1199/1718 (69.8%)  Accuracy:100.00%  Loss: 0.004697\n",
      "Iteration: 1200/1718 (69.8%)  Accuracy:100.00%  Loss: 0.026934\n",
      "Iteration: 1201/1718 (69.9%)  Accuracy:96.88%  Loss: 0.070288\n",
      "Iteration: 1202/1718 (70.0%)  Accuracy:96.88%  Loss: 0.321063\n",
      "Iteration: 1203/1718 (70.0%)  Accuracy:100.00%  Loss: 0.043111\n",
      "Iteration: 1204/1718 (70.1%)  Accuracy:100.00%  Loss: 0.010965\n",
      "Iteration: 1205/1718 (70.1%)  Accuracy:100.00%  Loss: 0.017156\n",
      "Iteration: 1206/1718 (70.2%)  Accuracy:93.75%  Loss: 0.342351\n",
      "Iteration: 1207/1718 (70.3%)  Accuracy:96.88%  Loss: 0.106879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1208/1718 (70.3%)  Accuracy:100.00%  Loss: 0.027131\n",
      "Iteration: 1209/1718 (70.4%)  Accuracy:93.75%  Loss: 0.239752\n",
      "Iteration: 1210/1718 (70.4%)  Accuracy:100.00%  Loss: 0.029692\n",
      "Iteration: 1211/1718 (70.5%)  Accuracy:93.75%  Loss: 0.160925\n",
      "Iteration: 1212/1718 (70.5%)  Accuracy:100.00%  Loss: 0.004079\n",
      "Iteration: 1213/1718 (70.6%)  Accuracy:100.00%  Loss: 0.006671\n",
      "Iteration: 1214/1718 (70.7%)  Accuracy:96.88%  Loss: 0.119867\n",
      "Iteration: 1215/1718 (70.7%)  Accuracy:93.75%  Loss: 0.071981\n",
      "Iteration: 1216/1718 (70.8%)  Accuracy:96.88%  Loss: 0.086375\n",
      "Iteration: 1217/1718 (70.8%)  Accuracy:100.00%  Loss: 0.034959\n",
      "Iteration: 1218/1718 (70.9%)  Accuracy:100.00%  Loss: 0.086549\n",
      "Iteration: 1219/1718 (71.0%)  Accuracy:100.00%  Loss: 0.042380\n",
      "Iteration: 1220/1718 (71.0%)  Accuracy:96.88%  Loss: 0.056778\n",
      "Iteration: 1221/1718 (71.1%)  Accuracy:100.00%  Loss: 0.018513\n",
      "Iteration: 1222/1718 (71.1%)  Accuracy:100.00%  Loss: 0.021195\n",
      "Iteration: 1223/1718 (71.2%)  Accuracy:100.00%  Loss: 0.013857\n",
      "Iteration: 1224/1718 (71.2%)  Accuracy:96.88%  Loss: 0.121056\n",
      "Iteration: 1225/1718 (71.3%)  Accuracy:100.00%  Loss: 0.037815\n",
      "Iteration: 1226/1718 (71.4%)  Accuracy:100.00%  Loss: 0.006715\n",
      "Iteration: 1227/1718 (71.4%)  Accuracy:100.00%  Loss: 0.004853\n",
      "Iteration: 1228/1718 (71.5%)  Accuracy:93.75%  Loss: 0.114156\n",
      "Iteration: 1229/1718 (71.5%)  Accuracy:96.88%  Loss: 0.127739\n",
      "Iteration: 1230/1718 (71.6%)  Accuracy:93.75%  Loss: 0.175975\n",
      "Iteration: 1231/1718 (71.7%)  Accuracy:100.00%  Loss: 0.006739\n",
      "Iteration: 1232/1718 (71.7%)  Accuracy:100.00%  Loss: 0.002123\n",
      "Iteration: 1233/1718 (71.8%)  Accuracy:93.75%  Loss: 0.133860\n",
      "Iteration: 1234/1718 (71.8%)  Accuracy:93.75%  Loss: 0.188245\n",
      "Iteration: 1235/1718 (71.9%)  Accuracy:100.00%  Loss: 0.007731\n",
      "Iteration: 1236/1718 (71.9%)  Accuracy:100.00%  Loss: 0.006011\n",
      "Iteration: 1237/1718 (72.0%)  Accuracy:100.00%  Loss: 0.010194\n",
      "Iteration: 1238/1718 (72.1%)  Accuracy:100.00%  Loss: 0.002413\n",
      "Iteration: 1239/1718 (72.1%)  Accuracy:100.00%  Loss: 0.000612\n",
      "Iteration: 1240/1718 (72.2%)  Accuracy:100.00%  Loss: 0.002387\n",
      "Iteration: 1241/1718 (72.2%)  Accuracy:96.88%  Loss: 0.134490\n",
      "Iteration: 1242/1718 (72.3%)  Accuracy:100.00%  Loss: 0.012846\n",
      "Iteration: 1243/1718 (72.4%)  Accuracy:96.88%  Loss: 0.109750\n",
      "Iteration: 1244/1718 (72.4%)  Accuracy:100.00%  Loss: 0.006809\n",
      "Iteration: 1245/1718 (72.5%)  Accuracy:100.00%  Loss: 0.029550\n",
      "Iteration: 1246/1718 (72.5%)  Accuracy:100.00%  Loss: 0.010099\n",
      "Iteration: 1247/1718 (72.6%)  Accuracy:93.75%  Loss: 0.163228\n",
      "Iteration: 1248/1718 (72.6%)  Accuracy:96.88%  Loss: 0.043775\n",
      "Iteration: 1249/1718 (72.7%)  Accuracy:100.00%  Loss: 0.015403\n",
      "Iteration: 1250/1718 (72.8%)  Accuracy:100.00%  Loss: 0.030324\n",
      "Iteration: 1251/1718 (72.8%)  Accuracy:100.00%  Loss: 0.003405\n",
      "Iteration: 1252/1718 (72.9%)  Accuracy:93.75%  Loss: 0.153282\n",
      "Iteration: 1253/1718 (72.9%)  Accuracy:96.88%  Loss: 0.069468\n",
      "Iteration: 1254/1718 (73.0%)  Accuracy:96.88%  Loss: 0.035886\n",
      "Iteration: 1255/1718 (73.1%)  Accuracy:96.88%  Loss: 0.194954\n",
      "Iteration: 1256/1718 (73.1%)  Accuracy:93.75%  Loss: 0.297690\n",
      "Iteration: 1257/1718 (73.2%)  Accuracy:93.75%  Loss: 0.281437\n",
      "Iteration: 1258/1718 (73.2%)  Accuracy:100.00%  Loss: 0.006127\n",
      "Iteration: 1259/1718 (73.3%)  Accuracy:100.00%  Loss: 0.006400\n",
      "Iteration: 1260/1718 (73.3%)  Accuracy:100.00%  Loss: 0.009159\n",
      "Iteration: 1261/1718 (73.4%)  Accuracy:100.00%  Loss: 0.011404\n",
      "Iteration: 1262/1718 (73.5%)  Accuracy:100.00%  Loss: 0.012005\n",
      "Iteration: 1263/1718 (73.5%)  Accuracy:96.88%  Loss: 0.051205\n",
      "Iteration: 1264/1718 (73.6%)  Accuracy:100.00%  Loss: 0.025725\n",
      "Iteration: 1265/1718 (73.6%)  Accuracy:96.88%  Loss: 0.126667\n",
      "Iteration: 1266/1718 (73.7%)  Accuracy:100.00%  Loss: 0.014118\n",
      "Iteration: 1267/1718 (73.7%)  Accuracy:93.75%  Loss: 0.260628\n",
      "Iteration: 1268/1718 (73.8%)  Accuracy:100.00%  Loss: 0.014023\n",
      "Iteration: 1269/1718 (73.9%)  Accuracy:100.00%  Loss: 0.022046\n",
      "Iteration: 1270/1718 (73.9%)  Accuracy:100.00%  Loss: 0.023667\n",
      "Iteration: 1271/1718 (74.0%)  Accuracy:96.88%  Loss: 0.079273\n",
      "Iteration: 1272/1718 (74.0%)  Accuracy:100.00%  Loss: 0.023883\n",
      "Iteration: 1273/1718 (74.1%)  Accuracy:96.88%  Loss: 0.058651\n",
      "Iteration: 1274/1718 (74.2%)  Accuracy:100.00%  Loss: 0.023721\n",
      "Iteration: 1275/1718 (74.2%)  Accuracy:100.00%  Loss: 0.009992\n",
      "Iteration: 1276/1718 (74.3%)  Accuracy:100.00%  Loss: 0.010517\n",
      "Iteration: 1277/1718 (74.3%)  Accuracy:100.00%  Loss: 0.012036\n",
      "Iteration: 1278/1718 (74.4%)  Accuracy:96.88%  Loss: 0.074268\n",
      "Iteration: 1279/1718 (74.4%)  Accuracy:96.88%  Loss: 0.170557\n",
      "Iteration: 1280/1718 (74.5%)  Accuracy:100.00%  Loss: 0.003051\n",
      "Iteration: 1281/1718 (74.6%)  Accuracy:100.00%  Loss: 0.012427\n",
      "Iteration: 1282/1718 (74.6%)  Accuracy:96.88%  Loss: 0.080632\n",
      "Iteration: 1283/1718 (74.7%)  Accuracy:100.00%  Loss: 0.005288\n",
      "Iteration: 1284/1718 (74.7%)  Accuracy:100.00%  Loss: 0.025697\n",
      "Iteration: 1285/1718 (74.8%)  Accuracy:96.88%  Loss: 0.113253\n",
      "Iteration: 1286/1718 (74.9%)  Accuracy:100.00%  Loss: 0.034500\n",
      "Iteration: 1287/1718 (74.9%)  Accuracy:100.00%  Loss: 0.033564\n",
      "Iteration: 1288/1718 (75.0%)  Accuracy:96.88%  Loss: 0.103857\n",
      "Iteration: 1289/1718 (75.0%)  Accuracy:100.00%  Loss: 0.020834\n",
      "Iteration: 1290/1718 (75.1%)  Accuracy:100.00%  Loss: 0.003117\n",
      "Iteration: 1291/1718 (75.1%)  Accuracy:100.00%  Loss: 0.035030\n",
      "Iteration: 1292/1718 (75.2%)  Accuracy:96.88%  Loss: 0.031219\n",
      "Iteration: 1293/1718 (75.3%)  Accuracy:100.00%  Loss: 0.008626\n",
      "Iteration: 1294/1718 (75.3%)  Accuracy:100.00%  Loss: 0.005280\n",
      "Iteration: 1295/1718 (75.4%)  Accuracy:100.00%  Loss: 0.000733\n",
      "Iteration: 1296/1718 (75.4%)  Accuracy:100.00%  Loss: 0.011545\n",
      "Iteration: 1297/1718 (75.5%)  Accuracy:100.00%  Loss: 0.009565\n",
      "Iteration: 1298/1718 (75.6%)  Accuracy:100.00%  Loss: 0.001938\n",
      "Iteration: 1299/1718 (75.6%)  Accuracy:96.88%  Loss: 0.088517\n",
      "Iteration: 1300/1718 (75.7%)  Accuracy:96.88%  Loss: 0.156847\n",
      "Iteration: 1301/1718 (75.7%)  Accuracy:100.00%  Loss: 0.000662\n",
      "Iteration: 1302/1718 (75.8%)  Accuracy:100.00%  Loss: 0.001878\n",
      "Iteration: 1303/1718 (75.8%)  Accuracy:100.00%  Loss: 0.003417\n",
      "Iteration: 1304/1718 (75.9%)  Accuracy:100.00%  Loss: 0.004076\n",
      "Iteration: 1305/1718 (76.0%)  Accuracy:93.75%  Loss: 0.290857\n",
      "Iteration: 1306/1718 (76.0%)  Accuracy:93.75%  Loss: 0.190480\n",
      "Iteration: 1307/1718 (76.1%)  Accuracy:100.00%  Loss: 0.003054\n",
      "Iteration: 1308/1718 (76.1%)  Accuracy:100.00%  Loss: 0.012406\n",
      "Iteration: 1309/1718 (76.2%)  Accuracy:100.00%  Loss: 0.003988\n",
      "Iteration: 1310/1718 (76.3%)  Accuracy:100.00%  Loss: 0.009816\n",
      "Iteration: 1311/1718 (76.3%)  Accuracy:93.75%  Loss: 0.122642\n",
      "Iteration: 1312/1718 (76.4%)  Accuracy:96.88%  Loss: 0.163412\n",
      "Iteration: 1313/1718 (76.4%)  Accuracy:100.00%  Loss: 0.020214\n",
      "Iteration: 1314/1718 (76.5%)  Accuracy:93.75%  Loss: 0.071750\n",
      "Iteration: 1315/1718 (76.5%)  Accuracy:100.00%  Loss: 0.014541\n",
      "Iteration: 1316/1718 (76.6%)  Accuracy:96.88%  Loss: 0.087167\n",
      "Iteration: 1317/1718 (76.7%)  Accuracy:96.88%  Loss: 0.081331\n",
      "Iteration: 1318/1718 (76.7%)  Accuracy:96.88%  Loss: 0.054429\n",
      "Iteration: 1319/1718 (76.8%)  Accuracy:93.75%  Loss: 0.207957\n",
      "Iteration: 1320/1718 (76.8%)  Accuracy:96.88%  Loss: 0.132429\n",
      "Iteration: 1321/1718 (76.9%)  Accuracy:100.00%  Loss: 0.022205\n",
      "Iteration: 1322/1718 (76.9%)  Accuracy:100.00%  Loss: 0.052321\n",
      "Iteration: 1323/1718 (77.0%)  Accuracy:96.88%  Loss: 0.120334\n",
      "Iteration: 1324/1718 (77.1%)  Accuracy:100.00%  Loss: 0.008219\n",
      "Iteration: 1325/1718 (77.1%)  Accuracy:100.00%  Loss: 0.024198\n",
      "Iteration: 1326/1718 (77.2%)  Accuracy:96.88%  Loss: 0.049449\n",
      "Iteration: 1327/1718 (77.2%)  Accuracy:100.00%  Loss: 0.029706\n",
      "Iteration: 1328/1718 (77.3%)  Accuracy:100.00%  Loss: 0.025819\n",
      "Iteration: 1329/1718 (77.4%)  Accuracy:96.88%  Loss: 0.101123\n",
      "Iteration: 1330/1718 (77.4%)  Accuracy:100.00%  Loss: 0.005838\n",
      "Iteration: 1331/1718 (77.5%)  Accuracy:96.88%  Loss: 0.033314\n",
      "Iteration: 1332/1718 (77.5%)  Accuracy:96.88%  Loss: 0.073664\n",
      "Iteration: 1333/1718 (77.6%)  Accuracy:100.00%  Loss: 0.012884\n",
      "Iteration: 1334/1718 (77.6%)  Accuracy:96.88%  Loss: 0.208541\n",
      "Iteration: 1335/1718 (77.7%)  Accuracy:100.00%  Loss: 0.004362\n",
      "Iteration: 1336/1718 (77.8%)  Accuracy:100.00%  Loss: 0.003578\n",
      "Iteration: 1337/1718 (77.8%)  Accuracy:96.88%  Loss: 0.058783\n",
      "Iteration: 1338/1718 (77.9%)  Accuracy:96.88%  Loss: 0.034168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1339/1718 (77.9%)  Accuracy:100.00%  Loss: 0.012090\n",
      "Iteration: 1340/1718 (78.0%)  Accuracy:100.00%  Loss: 0.013175\n",
      "Iteration: 1341/1718 (78.1%)  Accuracy:96.88%  Loss: 0.075952\n",
      "Iteration: 1342/1718 (78.1%)  Accuracy:100.00%  Loss: 0.005803\n",
      "Iteration: 1343/1718 (78.2%)  Accuracy:100.00%  Loss: 0.015457\n",
      "Iteration: 1344/1718 (78.2%)  Accuracy:93.75%  Loss: 0.151569\n",
      "Iteration: 1345/1718 (78.3%)  Accuracy:100.00%  Loss: 0.000986\n",
      "Iteration: 1346/1718 (78.3%)  Accuracy:96.88%  Loss: 0.176874\n",
      "Iteration: 1347/1718 (78.4%)  Accuracy:96.88%  Loss: 0.050202\n",
      "Iteration: 1348/1718 (78.5%)  Accuracy:96.88%  Loss: 0.245984\n",
      "Iteration: 1349/1718 (78.5%)  Accuracy:100.00%  Loss: 0.026588\n",
      "Iteration: 1350/1718 (78.6%)  Accuracy:93.75%  Loss: 0.083899\n",
      "Iteration: 1351/1718 (78.6%)  Accuracy:100.00%  Loss: 0.000946\n",
      "Iteration: 1352/1718 (78.7%)  Accuracy:100.00%  Loss: 0.011431\n",
      "Iteration: 1353/1718 (78.8%)  Accuracy:96.88%  Loss: 0.055491\n",
      "Iteration: 1354/1718 (78.8%)  Accuracy:100.00%  Loss: 0.027408\n",
      "Iteration: 1355/1718 (78.9%)  Accuracy:100.00%  Loss: 0.013194\n",
      "Iteration: 1356/1718 (78.9%)  Accuracy:100.00%  Loss: 0.014336\n",
      "Iteration: 1357/1718 (79.0%)  Accuracy:100.00%  Loss: 0.015454\n",
      "Iteration: 1358/1718 (79.0%)  Accuracy:96.88%  Loss: 0.127197\n",
      "Iteration: 1359/1718 (79.1%)  Accuracy:100.00%  Loss: 0.031296\n",
      "Iteration: 1360/1718 (79.2%)  Accuracy:100.00%  Loss: 0.024666\n",
      "Iteration: 1361/1718 (79.2%)  Accuracy:96.88%  Loss: 0.124077\n",
      "Iteration: 1362/1718 (79.3%)  Accuracy:93.75%  Loss: 0.136766\n",
      "Iteration: 1363/1718 (79.3%)  Accuracy:100.00%  Loss: 0.002281\n",
      "Iteration: 1364/1718 (79.4%)  Accuracy:100.00%  Loss: 0.016962\n",
      "Iteration: 1365/1718 (79.5%)  Accuracy:100.00%  Loss: 0.021783\n",
      "Iteration: 1366/1718 (79.5%)  Accuracy:96.88%  Loss: 0.108139\n",
      "Iteration: 1367/1718 (79.6%)  Accuracy:100.00%  Loss: 0.004112\n",
      "Iteration: 1368/1718 (79.6%)  Accuracy:100.00%  Loss: 0.003180\n",
      "Iteration: 1369/1718 (79.7%)  Accuracy:96.88%  Loss: 0.096808\n",
      "Iteration: 1370/1718 (79.7%)  Accuracy:100.00%  Loss: 0.027204\n",
      "Iteration: 1371/1718 (79.8%)  Accuracy:96.88%  Loss: 0.089695\n",
      "Iteration: 1372/1718 (79.9%)  Accuracy:100.00%  Loss: 0.016148\n",
      "Iteration: 1373/1718 (79.9%)  Accuracy:100.00%  Loss: 0.012182\n",
      "Iteration: 1374/1718 (80.0%)  Accuracy:100.00%  Loss: 0.035223\n",
      "Iteration: 1375/1718 (80.0%)  Accuracy:100.00%  Loss: 0.000713\n",
      "Iteration: 1376/1718 (80.1%)  Accuracy:100.00%  Loss: 0.003606\n",
      "Iteration: 1377/1718 (80.2%)  Accuracy:96.88%  Loss: 0.106139\n",
      "Iteration: 1378/1718 (80.2%)  Accuracy:100.00%  Loss: 0.013589\n",
      "Iteration: 1379/1718 (80.3%)  Accuracy:96.88%  Loss: 0.032470\n",
      "Iteration: 1380/1718 (80.3%)  Accuracy:100.00%  Loss: 0.003378\n",
      "Iteration: 1381/1718 (80.4%)  Accuracy:96.88%  Loss: 0.068092\n",
      "Iteration: 1382/1718 (80.4%)  Accuracy:96.88%  Loss: 0.124467\n",
      "Iteration: 1383/1718 (80.5%)  Accuracy:96.88%  Loss: 0.136559\n",
      "Iteration: 1384/1718 (80.6%)  Accuracy:100.00%  Loss: 0.023298\n",
      "Iteration: 1385/1718 (80.6%)  Accuracy:100.00%  Loss: 0.015283\n",
      "Iteration: 1386/1718 (80.7%)  Accuracy:96.88%  Loss: 0.052307\n",
      "Iteration: 1387/1718 (80.7%)  Accuracy:96.88%  Loss: 0.042249\n",
      "Iteration: 1388/1718 (80.8%)  Accuracy:100.00%  Loss: 0.005553\n",
      "Iteration: 1389/1718 (80.8%)  Accuracy:96.88%  Loss: 0.037836\n",
      "Iteration: 1390/1718 (80.9%)  Accuracy:96.88%  Loss: 0.075609\n",
      "Iteration: 1391/1718 (81.0%)  Accuracy:100.00%  Loss: 0.014196\n",
      "Iteration: 1392/1718 (81.0%)  Accuracy:100.00%  Loss: 0.002674\n",
      "Iteration: 1393/1718 (81.1%)  Accuracy:96.88%  Loss: 0.060095\n",
      "Iteration: 1394/1718 (81.1%)  Accuracy:100.00%  Loss: 0.001408\n",
      "Iteration: 1395/1718 (81.2%)  Accuracy:100.00%  Loss: 0.010666\n",
      "Iteration: 1396/1718 (81.3%)  Accuracy:93.75%  Loss: 0.095574\n",
      "Iteration: 1397/1718 (81.3%)  Accuracy:93.75%  Loss: 0.119142\n",
      "Iteration: 1398/1718 (81.4%)  Accuracy:100.00%  Loss: 0.008087\n",
      "Iteration: 1399/1718 (81.4%)  Accuracy:100.00%  Loss: 0.005010\n",
      "Iteration: 1400/1718 (81.5%)  Accuracy:100.00%  Loss: 0.005871\n",
      "Iteration: 1401/1718 (81.5%)  Accuracy:93.75%  Loss: 0.179722\n",
      "Iteration: 1402/1718 (81.6%)  Accuracy:100.00%  Loss: 0.020936\n",
      "Iteration: 1403/1718 (81.7%)  Accuracy:96.88%  Loss: 0.050592\n",
      "Iteration: 1404/1718 (81.7%)  Accuracy:100.00%  Loss: 0.003564\n",
      "Iteration: 1405/1718 (81.8%)  Accuracy:100.00%  Loss: 0.013411\n",
      "Iteration: 1406/1718 (81.8%)  Accuracy:96.88%  Loss: 0.169608\n",
      "Iteration: 1407/1718 (81.9%)  Accuracy:96.88%  Loss: 0.048027\n",
      "Iteration: 1408/1718 (82.0%)  Accuracy:100.00%  Loss: 0.017969\n",
      "Iteration: 1409/1718 (82.0%)  Accuracy:93.75%  Loss: 0.179181\n",
      "Iteration: 1410/1718 (82.1%)  Accuracy:100.00%  Loss: 0.016543\n",
      "Iteration: 1411/1718 (82.1%)  Accuracy:100.00%  Loss: 0.021645\n",
      "Iteration: 1412/1718 (82.2%)  Accuracy:96.88%  Loss: 0.121456\n",
      "Iteration: 1413/1718 (82.2%)  Accuracy:100.00%  Loss: 0.010925\n",
      "Iteration: 1414/1718 (82.3%)  Accuracy:100.00%  Loss: 0.002906\n",
      "Iteration: 1415/1718 (82.4%)  Accuracy:100.00%  Loss: 0.029854\n",
      "Iteration: 1416/1718 (82.4%)  Accuracy:96.88%  Loss: 0.040422\n",
      "Iteration: 1417/1718 (82.5%)  Accuracy:100.00%  Loss: 0.002423\n",
      "Iteration: 1418/1718 (82.5%)  Accuracy:100.00%  Loss: 0.005980\n",
      "Iteration: 1419/1718 (82.6%)  Accuracy:96.88%  Loss: 0.125340\n",
      "Iteration: 1420/1718 (82.7%)  Accuracy:96.88%  Loss: 0.038420\n",
      "Iteration: 1421/1718 (82.7%)  Accuracy:93.75%  Loss: 0.076942\n",
      "Iteration: 1422/1718 (82.8%)  Accuracy:100.00%  Loss: 0.024072\n",
      "Iteration: 1423/1718 (82.8%)  Accuracy:100.00%  Loss: 0.021899\n",
      "Iteration: 1424/1718 (82.9%)  Accuracy:100.00%  Loss: 0.000695\n",
      "Iteration: 1425/1718 (82.9%)  Accuracy:100.00%  Loss: 0.013443\n",
      "Iteration: 1426/1718 (83.0%)  Accuracy:100.00%  Loss: 0.019982\n",
      "Iteration: 1427/1718 (83.1%)  Accuracy:96.88%  Loss: 0.127309\n",
      "Iteration: 1428/1718 (83.1%)  Accuracy:96.88%  Loss: 0.038075\n",
      "Iteration: 1429/1718 (83.2%)  Accuracy:93.75%  Loss: 0.168875\n",
      "Iteration: 1430/1718 (83.2%)  Accuracy:90.62%  Loss: 0.437306\n",
      "Iteration: 1431/1718 (83.3%)  Accuracy:96.88%  Loss: 0.073227\n",
      "Iteration: 1432/1718 (83.4%)  Accuracy:100.00%  Loss: 0.011847\n",
      "Iteration: 1433/1718 (83.4%)  Accuracy:96.88%  Loss: 0.344772\n",
      "Iteration: 1434/1718 (83.5%)  Accuracy:100.00%  Loss: 0.012323\n",
      "Iteration: 1435/1718 (83.5%)  Accuracy:100.00%  Loss: 0.021731\n",
      "Iteration: 1436/1718 (83.6%)  Accuracy:100.00%  Loss: 0.001511\n",
      "Iteration: 1437/1718 (83.6%)  Accuracy:96.88%  Loss: 0.039838\n",
      "Iteration: 1438/1718 (83.7%)  Accuracy:100.00%  Loss: 0.045117\n",
      "Iteration: 1439/1718 (83.8%)  Accuracy:96.88%  Loss: 0.076192\n",
      "Iteration: 1440/1718 (83.8%)  Accuracy:96.88%  Loss: 0.233614\n",
      "Iteration: 1441/1718 (83.9%)  Accuracy:100.00%  Loss: 0.002528\n",
      "Iteration: 1442/1718 (83.9%)  Accuracy:96.88%  Loss: 0.112174\n",
      "Iteration: 1443/1718 (84.0%)  Accuracy:100.00%  Loss: 0.016833\n",
      "Iteration: 1444/1718 (84.1%)  Accuracy:93.75%  Loss: 0.161645\n",
      "Iteration: 1445/1718 (84.1%)  Accuracy:100.00%  Loss: 0.020488\n",
      "Iteration: 1446/1718 (84.2%)  Accuracy:96.88%  Loss: 0.040177\n",
      "Iteration: 1447/1718 (84.2%)  Accuracy:100.00%  Loss: 0.014861\n",
      "Iteration: 1448/1718 (84.3%)  Accuracy:100.00%  Loss: 0.009154\n",
      "Iteration: 1449/1718 (84.3%)  Accuracy:96.88%  Loss: 0.106514\n",
      "Iteration: 1450/1718 (84.4%)  Accuracy:100.00%  Loss: 0.011013\n",
      "Iteration: 1451/1718 (84.5%)  Accuracy:100.00%  Loss: 0.018726\n",
      "Iteration: 1452/1718 (84.5%)  Accuracy:96.88%  Loss: 0.043430\n",
      "Iteration: 1453/1718 (84.6%)  Accuracy:100.00%  Loss: 0.018071\n",
      "Iteration: 1454/1718 (84.6%)  Accuracy:90.62%  Loss: 0.119447\n",
      "Iteration: 1455/1718 (84.7%)  Accuracy:100.00%  Loss: 0.033924\n",
      "Iteration: 1456/1718 (84.7%)  Accuracy:96.88%  Loss: 0.028331\n",
      "Iteration: 1457/1718 (84.8%)  Accuracy:100.00%  Loss: 0.011636\n",
      "Iteration: 1458/1718 (84.9%)  Accuracy:100.00%  Loss: 0.022159\n",
      "Iteration: 1459/1718 (84.9%)  Accuracy:100.00%  Loss: 0.023620\n",
      "Iteration: 1460/1718 (85.0%)  Accuracy:96.88%  Loss: 0.093657\n",
      "Iteration: 1461/1718 (85.0%)  Accuracy:96.88%  Loss: 0.077372\n",
      "Iteration: 1462/1718 (85.1%)  Accuracy:96.88%  Loss: 0.060147\n",
      "Iteration: 1463/1718 (85.2%)  Accuracy:100.00%  Loss: 0.062848\n",
      "Iteration: 1464/1718 (85.2%)  Accuracy:96.88%  Loss: 0.025054\n",
      "Iteration: 1465/1718 (85.3%)  Accuracy:100.00%  Loss: 0.021137\n",
      "Iteration: 1466/1718 (85.3%)  Accuracy:93.75%  Loss: 0.068142\n",
      "Iteration: 1467/1718 (85.4%)  Accuracy:96.88%  Loss: 0.049974\n",
      "Iteration: 1468/1718 (85.4%)  Accuracy:100.00%  Loss: 0.004947\n",
      "Iteration: 1469/1718 (85.5%)  Accuracy:96.88%  Loss: 0.063136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1470/1718 (85.6%)  Accuracy:96.88%  Loss: 0.053017\n",
      "Iteration: 1471/1718 (85.6%)  Accuracy:96.88%  Loss: 0.047280\n",
      "Iteration: 1472/1718 (85.7%)  Accuracy:96.88%  Loss: 0.150891\n",
      "Iteration: 1473/1718 (85.7%)  Accuracy:100.00%  Loss: 0.028115\n",
      "Iteration: 1474/1718 (85.8%)  Accuracy:100.00%  Loss: 0.011072\n",
      "Iteration: 1475/1718 (85.9%)  Accuracy:100.00%  Loss: 0.001255\n",
      "Iteration: 1476/1718 (85.9%)  Accuracy:100.00%  Loss: 0.004055\n",
      "Iteration: 1477/1718 (86.0%)  Accuracy:100.00%  Loss: 0.010981\n",
      "Iteration: 1478/1718 (86.0%)  Accuracy:100.00%  Loss: 0.024985\n",
      "Iteration: 1479/1718 (86.1%)  Accuracy:100.00%  Loss: 0.005525\n",
      "Iteration: 1480/1718 (86.1%)  Accuracy:100.00%  Loss: 0.000835\n",
      "Iteration: 1481/1718 (86.2%)  Accuracy:96.88%  Loss: 0.026561\n",
      "Iteration: 1482/1718 (86.3%)  Accuracy:100.00%  Loss: 0.002956\n",
      "Iteration: 1483/1718 (86.3%)  Accuracy:96.88%  Loss: 0.039063\n",
      "Iteration: 1484/1718 (86.4%)  Accuracy:100.00%  Loss: 0.002515\n",
      "Iteration: 1485/1718 (86.4%)  Accuracy:96.88%  Loss: 0.161845\n",
      "Iteration: 1486/1718 (86.5%)  Accuracy:96.88%  Loss: 0.207244\n",
      "Iteration: 1487/1718 (86.6%)  Accuracy:93.75%  Loss: 0.211439\n",
      "Iteration: 1488/1718 (86.6%)  Accuracy:100.00%  Loss: 0.029487\n",
      "Iteration: 1489/1718 (86.7%)  Accuracy:96.88%  Loss: 0.180665\n",
      "Iteration: 1490/1718 (86.7%)  Accuracy:96.88%  Loss: 0.197888\n",
      "Iteration: 1491/1718 (86.8%)  Accuracy:96.88%  Loss: 0.204020\n",
      "Iteration: 1492/1718 (86.8%)  Accuracy:96.88%  Loss: 0.060480\n",
      "Iteration: 1493/1718 (86.9%)  Accuracy:100.00%  Loss: 0.024718\n",
      "Iteration: 1494/1718 (87.0%)  Accuracy:96.88%  Loss: 0.068821\n",
      "Iteration: 1495/1718 (87.0%)  Accuracy:100.00%  Loss: 0.007852\n",
      "Iteration: 1496/1718 (87.1%)  Accuracy:100.00%  Loss: 0.008437\n",
      "Iteration: 1497/1718 (87.1%)  Accuracy:96.88%  Loss: 0.074261\n",
      "Iteration: 1498/1718 (87.2%)  Accuracy:96.88%  Loss: 0.076521\n",
      "Iteration: 1499/1718 (87.3%)  Accuracy:96.88%  Loss: 0.055122\n",
      "Iteration: 1500/1718 (87.3%)  Accuracy:93.75%  Loss: 0.231287\n",
      "Iteration: 1501/1718 (87.4%)  Accuracy:100.00%  Loss: 0.014767\n",
      "Iteration: 1502/1718 (87.4%)  Accuracy:100.00%  Loss: 0.017724\n",
      "Iteration: 1503/1718 (87.5%)  Accuracy:100.00%  Loss: 0.003475\n",
      "Iteration: 1504/1718 (87.5%)  Accuracy:100.00%  Loss: 0.000774\n",
      "Iteration: 1505/1718 (87.6%)  Accuracy:100.00%  Loss: 0.014790\n",
      "Iteration: 1506/1718 (87.7%)  Accuracy:100.00%  Loss: 0.017772\n",
      "Iteration: 1507/1718 (87.7%)  Accuracy:96.88%  Loss: 0.136757\n",
      "Iteration: 1508/1718 (87.8%)  Accuracy:100.00%  Loss: 0.003818\n",
      "Iteration: 1509/1718 (87.8%)  Accuracy:96.88%  Loss: 0.033853\n",
      "Iteration: 1510/1718 (87.9%)  Accuracy:100.00%  Loss: 0.005886\n",
      "Iteration: 1511/1718 (88.0%)  Accuracy:100.00%  Loss: 0.005995\n",
      "Iteration: 1512/1718 (88.0%)  Accuracy:100.00%  Loss: 0.011770\n",
      "Iteration: 1513/1718 (88.1%)  Accuracy:93.75%  Loss: 0.182714\n",
      "Iteration: 1514/1718 (88.1%)  Accuracy:100.00%  Loss: 0.025689\n",
      "Iteration: 1515/1718 (88.2%)  Accuracy:93.75%  Loss: 0.333418\n",
      "Iteration: 1516/1718 (88.2%)  Accuracy:100.00%  Loss: 0.001552\n",
      "Iteration: 1517/1718 (88.3%)  Accuracy:96.88%  Loss: 0.055835\n",
      "Iteration: 1518/1718 (88.4%)  Accuracy:100.00%  Loss: 0.001847\n",
      "Iteration: 1519/1718 (88.4%)  Accuracy:100.00%  Loss: 0.008148\n",
      "Iteration: 1520/1718 (88.5%)  Accuracy:96.88%  Loss: 0.124163\n",
      "Iteration: 1521/1718 (88.5%)  Accuracy:100.00%  Loss: 0.034366\n",
      "Iteration: 1522/1718 (88.6%)  Accuracy:96.88%  Loss: 0.059604\n",
      "Iteration: 1523/1718 (88.6%)  Accuracy:100.00%  Loss: 0.033137\n",
      "Iteration: 1524/1718 (88.7%)  Accuracy:100.00%  Loss: 0.002812\n",
      "Iteration: 1525/1718 (88.8%)  Accuracy:100.00%  Loss: 0.006991\n",
      "Iteration: 1526/1718 (88.8%)  Accuracy:96.88%  Loss: 0.068111\n",
      "Iteration: 1527/1718 (88.9%)  Accuracy:100.00%  Loss: 0.006486\n",
      "Iteration: 1528/1718 (88.9%)  Accuracy:100.00%  Loss: 0.003440\n",
      "Iteration: 1529/1718 (89.0%)  Accuracy:87.50%  Loss: 0.340360\n",
      "Iteration: 1530/1718 (89.1%)  Accuracy:96.88%  Loss: 0.184088\n",
      "Iteration: 1531/1718 (89.1%)  Accuracy:100.00%  Loss: 0.009499\n",
      "Iteration: 1532/1718 (89.2%)  Accuracy:96.88%  Loss: 0.062048\n",
      "Iteration: 1533/1718 (89.2%)  Accuracy:93.75%  Loss: 0.115652\n",
      "Iteration: 1534/1718 (89.3%)  Accuracy:100.00%  Loss: 0.010833\n",
      "Iteration: 1535/1718 (89.3%)  Accuracy:96.88%  Loss: 0.148884\n",
      "Iteration: 1536/1718 (89.4%)  Accuracy:100.00%  Loss: 0.005238\n",
      "Iteration: 1537/1718 (89.5%)  Accuracy:100.00%  Loss: 0.021867\n",
      "Iteration: 1538/1718 (89.5%)  Accuracy:100.00%  Loss: 0.013148\n",
      "Iteration: 1539/1718 (89.6%)  Accuracy:100.00%  Loss: 0.005809\n",
      "Iteration: 1540/1718 (89.6%)  Accuracy:93.75%  Loss: 0.079198\n",
      "Iteration: 1541/1718 (89.7%)  Accuracy:100.00%  Loss: 0.020731\n",
      "Iteration: 1542/1718 (89.8%)  Accuracy:100.00%  Loss: 0.016389\n",
      "Iteration: 1543/1718 (89.8%)  Accuracy:96.88%  Loss: 0.044489\n",
      "Iteration: 1544/1718 (89.9%)  Accuracy:100.00%  Loss: 0.005367\n",
      "Iteration: 1545/1718 (89.9%)  Accuracy:100.00%  Loss: 0.034479\n",
      "Iteration: 1546/1718 (90.0%)  Accuracy:96.88%  Loss: 0.028065\n",
      "Iteration: 1547/1718 (90.0%)  Accuracy:100.00%  Loss: 0.016259\n",
      "Iteration: 1548/1718 (90.1%)  Accuracy:100.00%  Loss: 0.036029\n",
      "Iteration: 1549/1718 (90.2%)  Accuracy:96.88%  Loss: 0.043138\n",
      "Iteration: 1550/1718 (90.2%)  Accuracy:100.00%  Loss: 0.007548\n",
      "Iteration: 1551/1718 (90.3%)  Accuracy:96.88%  Loss: 0.042089\n",
      "Iteration: 1552/1718 (90.3%)  Accuracy:96.88%  Loss: 0.088303\n",
      "Iteration: 1553/1718 (90.4%)  Accuracy:100.00%  Loss: 0.001376\n",
      "Iteration: 1554/1718 (90.5%)  Accuracy:100.00%  Loss: 0.013072\n",
      "Iteration: 1555/1718 (90.5%)  Accuracy:100.00%  Loss: 0.037178\n",
      "Iteration: 1556/1718 (90.6%)  Accuracy:100.00%  Loss: 0.009966\n",
      "Iteration: 1557/1718 (90.6%)  Accuracy:100.00%  Loss: 0.004822\n",
      "Iteration: 1558/1718 (90.7%)  Accuracy:100.00%  Loss: 0.011621\n",
      "Iteration: 1559/1718 (90.7%)  Accuracy:100.00%  Loss: 0.020252\n",
      "Iteration: 1560/1718 (90.8%)  Accuracy:100.00%  Loss: 0.004026\n",
      "Iteration: 1561/1718 (90.9%)  Accuracy:100.00%  Loss: 0.001559\n",
      "Iteration: 1562/1718 (90.9%)  Accuracy:100.00%  Loss: 0.002280\n",
      "Iteration: 1563/1718 (91.0%)  Accuracy:100.00%  Loss: 0.032077\n",
      "Iteration: 1564/1718 (91.0%)  Accuracy:100.00%  Loss: 0.032921\n",
      "Iteration: 1565/1718 (91.1%)  Accuracy:100.00%  Loss: 0.026956\n",
      "Iteration: 1566/1718 (91.2%)  Accuracy:96.88%  Loss: 0.063920\n",
      "Iteration: 1567/1718 (91.2%)  Accuracy:100.00%  Loss: 0.007211\n",
      "Iteration: 1568/1718 (91.3%)  Accuracy:93.75%  Loss: 0.213408\n",
      "Iteration: 1569/1718 (91.3%)  Accuracy:100.00%  Loss: 0.002658\n",
      "Iteration: 1570/1718 (91.4%)  Accuracy:100.00%  Loss: 0.015145\n",
      "Iteration: 1571/1718 (91.4%)  Accuracy:100.00%  Loss: 0.002469\n",
      "Iteration: 1572/1718 (91.5%)  Accuracy:96.88%  Loss: 0.297852\n",
      "Iteration: 1573/1718 (91.6%)  Accuracy:100.00%  Loss: 0.000243\n",
      "Iteration: 1574/1718 (91.6%)  Accuracy:100.00%  Loss: 0.003781\n",
      "Iteration: 1575/1718 (91.7%)  Accuracy:100.00%  Loss: 0.000335\n",
      "Iteration: 1576/1718 (91.7%)  Accuracy:100.00%  Loss: 0.038446\n",
      "Iteration: 1577/1718 (91.8%)  Accuracy:100.00%  Loss: 0.020600\n",
      "Iteration: 1578/1718 (91.9%)  Accuracy:100.00%  Loss: 0.009366\n",
      "Iteration: 1579/1718 (91.9%)  Accuracy:100.00%  Loss: 0.029776\n",
      "Iteration: 1580/1718 (92.0%)  Accuracy:96.88%  Loss: 0.060476\n",
      "Iteration: 1581/1718 (92.0%)  Accuracy:100.00%  Loss: 0.017512\n",
      "Iteration: 1582/1718 (92.1%)  Accuracy:100.00%  Loss: 0.019508\n",
      "Iteration: 1583/1718 (92.1%)  Accuracy:96.88%  Loss: 0.117189\n",
      "Iteration: 1584/1718 (92.2%)  Accuracy:96.88%  Loss: 0.071262\n",
      "Iteration: 1585/1718 (92.3%)  Accuracy:96.88%  Loss: 0.069079\n",
      "Iteration: 1586/1718 (92.3%)  Accuracy:100.00%  Loss: 0.005855\n",
      "Iteration: 1587/1718 (92.4%)  Accuracy:100.00%  Loss: 0.001071\n",
      "Iteration: 1588/1718 (92.4%)  Accuracy:100.00%  Loss: 0.005897\n",
      "Iteration: 1589/1718 (92.5%)  Accuracy:100.00%  Loss: 0.010081\n",
      "Iteration: 1590/1718 (92.5%)  Accuracy:96.88%  Loss: 0.097681\n",
      "Iteration: 1591/1718 (92.6%)  Accuracy:96.88%  Loss: 0.033359\n",
      "Iteration: 1592/1718 (92.7%)  Accuracy:100.00%  Loss: 0.003611\n",
      "Iteration: 1593/1718 (92.7%)  Accuracy:96.88%  Loss: 0.055228\n",
      "Iteration: 1594/1718 (92.8%)  Accuracy:96.88%  Loss: 0.054433\n",
      "Iteration: 1595/1718 (92.8%)  Accuracy:93.75%  Loss: 0.260250\n",
      "Iteration: 1596/1718 (92.9%)  Accuracy:100.00%  Loss: 0.001793\n",
      "Iteration: 1597/1718 (93.0%)  Accuracy:100.00%  Loss: 0.024287\n",
      "Iteration: 1598/1718 (93.0%)  Accuracy:93.75%  Loss: 0.224342\n",
      "Iteration: 1599/1718 (93.1%)  Accuracy:100.00%  Loss: 0.047166\n",
      "Iteration: 1600/1718 (93.1%)  Accuracy:100.00%  Loss: 0.006904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1601/1718 (93.2%)  Accuracy:96.88%  Loss: 0.113470\n",
      "Iteration: 1602/1718 (93.2%)  Accuracy:100.00%  Loss: 0.006834\n",
      "Iteration: 1603/1718 (93.3%)  Accuracy:100.00%  Loss: 0.017752\n",
      "Iteration: 1604/1718 (93.4%)  Accuracy:100.00%  Loss: 0.022639\n",
      "Iteration: 1605/1718 (93.4%)  Accuracy:96.88%  Loss: 0.148666\n",
      "Iteration: 1606/1718 (93.5%)  Accuracy:96.88%  Loss: 0.095933\n",
      "Iteration: 1607/1718 (93.5%)  Accuracy:100.00%  Loss: 0.025290\n",
      "Iteration: 1608/1718 (93.6%)  Accuracy:96.88%  Loss: 0.083364\n",
      "Iteration: 1609/1718 (93.7%)  Accuracy:96.88%  Loss: 0.132223\n",
      "Iteration: 1610/1718 (93.7%)  Accuracy:100.00%  Loss: 0.028685\n",
      "Iteration: 1611/1718 (93.8%)  Accuracy:100.00%  Loss: 0.011765\n",
      "Iteration: 1612/1718 (93.8%)  Accuracy:96.88%  Loss: 0.147179\n",
      "Iteration: 1613/1718 (93.9%)  Accuracy:100.00%  Loss: 0.003174\n",
      "Iteration: 1614/1718 (93.9%)  Accuracy:100.00%  Loss: 0.016208\n",
      "Iteration: 1615/1718 (94.0%)  Accuracy:96.88%  Loss: 0.062963\n",
      "Iteration: 1616/1718 (94.1%)  Accuracy:100.00%  Loss: 0.007928\n",
      "Iteration: 1617/1718 (94.1%)  Accuracy:100.00%  Loss: 0.012004\n",
      "Iteration: 1618/1718 (94.2%)  Accuracy:96.88%  Loss: 0.053390\n",
      "Iteration: 1619/1718 (94.2%)  Accuracy:96.88%  Loss: 0.040973\n",
      "Iteration: 1620/1718 (94.3%)  Accuracy:96.88%  Loss: 0.060992\n",
      "Iteration: 1621/1718 (94.4%)  Accuracy:96.88%  Loss: 0.250424\n",
      "Iteration: 1622/1718 (94.4%)  Accuracy:96.88%  Loss: 0.116683\n",
      "Iteration: 1623/1718 (94.5%)  Accuracy:100.00%  Loss: 0.007460\n",
      "Iteration: 1624/1718 (94.5%)  Accuracy:96.88%  Loss: 0.039953\n",
      "Iteration: 1625/1718 (94.6%)  Accuracy:100.00%  Loss: 0.004348\n",
      "Iteration: 1626/1718 (94.6%)  Accuracy:96.88%  Loss: 0.073050\n",
      "Iteration: 1627/1718 (94.7%)  Accuracy:100.00%  Loss: 0.018738\n",
      "Iteration: 1628/1718 (94.8%)  Accuracy:100.00%  Loss: 0.016171\n",
      "Iteration: 1629/1718 (94.8%)  Accuracy:100.00%  Loss: 0.010608\n",
      "Iteration: 1630/1718 (94.9%)  Accuracy:96.88%  Loss: 0.135629\n",
      "Iteration: 1631/1718 (94.9%)  Accuracy:100.00%  Loss: 0.025057\n",
      "Iteration: 1632/1718 (95.0%)  Accuracy:100.00%  Loss: 0.014042\n",
      "Iteration: 1633/1718 (95.1%)  Accuracy:100.00%  Loss: 0.046858\n",
      "Iteration: 1634/1718 (95.1%)  Accuracy:100.00%  Loss: 0.007807\n",
      "Iteration: 1635/1718 (95.2%)  Accuracy:100.00%  Loss: 0.010434\n",
      "Iteration: 1636/1718 (95.2%)  Accuracy:100.00%  Loss: 0.012647\n",
      "Iteration: 1637/1718 (95.3%)  Accuracy:100.00%  Loss: 0.047950\n",
      "Iteration: 1638/1718 (95.3%)  Accuracy:96.88%  Loss: 0.028783\n",
      "Iteration: 1639/1718 (95.4%)  Accuracy:100.00%  Loss: 0.001550\n",
      "Iteration: 1640/1718 (95.5%)  Accuracy:100.00%  Loss: 0.002854\n",
      "Iteration: 1641/1718 (95.5%)  Accuracy:100.00%  Loss: 0.015042\n",
      "Iteration: 1642/1718 (95.6%)  Accuracy:100.00%  Loss: 0.011404\n",
      "Iteration: 1643/1718 (95.6%)  Accuracy:96.88%  Loss: 0.096095\n",
      "Iteration: 1644/1718 (95.7%)  Accuracy:100.00%  Loss: 0.003245\n",
      "Iteration: 1645/1718 (95.8%)  Accuracy:100.00%  Loss: 0.006478\n",
      "Iteration: 1646/1718 (95.8%)  Accuracy:100.00%  Loss: 0.006359\n",
      "Iteration: 1647/1718 (95.9%)  Accuracy:100.00%  Loss: 0.004254\n",
      "Iteration: 1648/1718 (95.9%)  Accuracy:100.00%  Loss: 0.002540\n",
      "Iteration: 1649/1718 (96.0%)  Accuracy:100.00%  Loss: 0.015735\n",
      "Iteration: 1650/1718 (96.0%)  Accuracy:100.00%  Loss: 0.006682\n",
      "Iteration: 1651/1718 (96.1%)  Accuracy:100.00%  Loss: 0.009251\n",
      "Iteration: 1652/1718 (96.2%)  Accuracy:100.00%  Loss: 0.013856\n",
      "Iteration: 1653/1718 (96.2%)  Accuracy:100.00%  Loss: 0.002992\n",
      "Iteration: 1654/1718 (96.3%)  Accuracy:96.88%  Loss: 0.110556\n",
      "Iteration: 1655/1718 (96.3%)  Accuracy:100.00%  Loss: 0.031965\n",
      "Iteration: 1656/1718 (96.4%)  Accuracy:100.00%  Loss: 0.002215\n",
      "Iteration: 1657/1718 (96.4%)  Accuracy:100.00%  Loss: 0.000777\n",
      "Iteration: 1658/1718 (96.5%)  Accuracy:100.00%  Loss: 0.004134\n",
      "Iteration: 1659/1718 (96.6%)  Accuracy:100.00%  Loss: 0.001681\n",
      "Iteration: 1660/1718 (96.6%)  Accuracy:100.00%  Loss: 0.008010\n",
      "Iteration: 1661/1718 (96.7%)  Accuracy:100.00%  Loss: 0.003882\n",
      "Iteration: 1662/1718 (96.7%)  Accuracy:100.00%  Loss: 0.000582\n",
      "Iteration: 1663/1718 (96.8%)  Accuracy:100.00%  Loss: 0.016663\n",
      "Iteration: 1664/1718 (96.9%)  Accuracy:100.00%  Loss: 0.007230\n",
      "Iteration: 1665/1718 (96.9%)  Accuracy:100.00%  Loss: 0.002321\n",
      "Iteration: 1666/1718 (97.0%)  Accuracy:96.88%  Loss: 0.096027\n",
      "Iteration: 1667/1718 (97.0%)  Accuracy:100.00%  Loss: 0.000838\n",
      "Iteration: 1668/1718 (97.1%)  Accuracy:100.00%  Loss: 0.019133\n",
      "Iteration: 1669/1718 (97.1%)  Accuracy:100.00%  Loss: 0.003256\n",
      "Iteration: 1670/1718 (97.2%)  Accuracy:96.88%  Loss: 0.029869\n",
      "Iteration: 1671/1718 (97.3%)  Accuracy:100.00%  Loss: 0.002745\n",
      "Iteration: 1672/1718 (97.3%)  Accuracy:100.00%  Loss: 0.008823\n",
      "Iteration: 1673/1718 (97.4%)  Accuracy:100.00%  Loss: 0.008669\n",
      "Iteration: 1674/1718 (97.4%)  Accuracy:96.88%  Loss: 0.061563\n",
      "Iteration: 1675/1718 (97.5%)  Accuracy:96.88%  Loss: 0.066927\n",
      "Iteration: 1676/1718 (97.6%)  Accuracy:96.88%  Loss: 0.049875\n",
      "Iteration: 1677/1718 (97.6%)  Accuracy:96.88%  Loss: 0.057766\n",
      "Iteration: 1678/1718 (97.7%)  Accuracy:100.00%  Loss: 0.021053\n",
      "Iteration: 1679/1718 (97.7%)  Accuracy:100.00%  Loss: 0.001820\n",
      "Iteration: 1680/1718 (97.8%)  Accuracy:100.00%  Loss: 0.004804\n",
      "Iteration: 1681/1718 (97.8%)  Accuracy:100.00%  Loss: 0.001588\n",
      "Iteration: 1682/1718 (97.9%)  Accuracy:100.00%  Loss: 0.015407\n",
      "Iteration: 1683/1718 (98.0%)  Accuracy:100.00%  Loss: 0.015220\n",
      "Iteration: 1684/1718 (98.0%)  Accuracy:100.00%  Loss: 0.006992\n",
      "Iteration: 1685/1718 (98.1%)  Accuracy:96.88%  Loss: 0.028475\n",
      "Iteration: 1686/1718 (98.1%)  Accuracy:100.00%  Loss: 0.001525\n",
      "Iteration: 1687/1718 (98.2%)  Accuracy:100.00%  Loss: 0.001136\n",
      "Iteration: 1688/1718 (98.3%)  Accuracy:100.00%  Loss: 0.001408\n",
      "Iteration: 1689/1718 (98.3%)  Accuracy:100.00%  Loss: 0.011780\n",
      "Iteration: 1690/1718 (98.4%)  Accuracy:100.00%  Loss: 0.010924\n",
      "Iteration: 1691/1718 (98.4%)  Accuracy:100.00%  Loss: 0.000400\n",
      "Iteration: 1692/1718 (98.5%)  Accuracy:100.00%  Loss: 0.003141\n",
      "Iteration: 1693/1718 (98.5%)  Accuracy:100.00%  Loss: 0.003177\n",
      "Iteration: 1694/1718 (98.6%)  Accuracy:96.88%  Loss: 0.040143\n",
      "Iteration: 1695/1718 (98.7%)  Accuracy:100.00%  Loss: 0.019446\n",
      "Iteration: 1696/1718 (98.7%)  Accuracy:100.00%  Loss: 0.026302\n",
      "Iteration: 1697/1718 (98.8%)  Accuracy:100.00%  Loss: 0.001893\n",
      "Iteration: 1698/1718 (98.8%)  Accuracy:96.88%  Loss: 0.059375\n",
      "Iteration: 1699/1718 (98.9%)  Accuracy:100.00%  Loss: 0.000506\n",
      "Iteration: 1700/1718 (99.0%)  Accuracy:96.88%  Loss: 0.032258\n",
      "Iteration: 1701/1718 (99.0%)  Accuracy:100.00%  Loss: 0.001791\n",
      "Iteration: 1702/1718 (99.1%)  Accuracy:96.88%  Loss: 0.209221\n",
      "Iteration: 1703/1718 (99.1%)  Accuracy:100.00%  Loss: 0.001295\n",
      "Iteration: 1704/1718 (99.2%)  Accuracy:93.75%  Loss: 0.318962\n",
      "Iteration: 1705/1718 (99.2%)  Accuracy:93.75%  Loss: 0.204878\n",
      "Iteration: 1706/1718 (99.3%)  Accuracy:100.00%  Loss: 0.000164\n",
      "Iteration: 1707/1718 (99.4%)  Accuracy:100.00%  Loss: 0.001978\n",
      "Iteration: 1708/1718 (99.4%)  Accuracy:100.00%  Loss: 0.016671\n",
      "Iteration: 1709/1718 (99.5%)  Accuracy:96.88%  Loss: 0.193212\n",
      "Iteration: 1710/1718 (99.5%)  Accuracy:93.75%  Loss: 0.177915\n",
      "Iteration: 1711/1718 (99.6%)  Accuracy:93.75%  Loss: 0.281328\n",
      "Iteration: 1712/1718 (99.7%)  Accuracy:100.00%  Loss: 0.010886\n",
      "Iteration: 1713/1718 (99.7%)  Accuracy:100.00%  Loss: 0.014912\n",
      "Iteration: 1714/1718 (99.8%)  Accuracy:100.00%  Loss: 0.005956\n",
      "Iteration: 1715/1718 (99.8%)  Accuracy:100.00%  Loss: 0.005511\n",
      "Iteration: 1716/1718 (99.9%)  Accuracy:100.00%  Loss: 0.021563\n",
      "Iteration: 1717/1718 (99.9%)  Accuracy:100.00%  Loss: 0.010180\n",
      "Iteration: 1718/1718 (100.0%)  Accuracy:93.75%  Loss: 0.140998\n",
      "Epoch: 1  Val accuracy: 98.2572%  Loss: 0.063732 (improved)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "restore_checkpoint = True\n",
    "\n",
    "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
    "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"my_cnn/cnn.ckpt\"\n",
    "summary_path = \"my_cnn/\"\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "\n",
    "\n",
    "                # Run the training operation and measure the loss:\n",
    "                _, loss_train, acc = sess.run([training_op, loss, accuracy],\n",
    "                                          feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                     y: y_batch,\n",
    "                                                     prob: 0.5 })\n",
    "\n",
    "                print(\"\\rIteration: {}/{} ({:.1f}%)  Accuracy:{:.2f}%  Loss: {:.6f}\".format(iteration,\n",
    "                                                                          n_iterations_per_epoch,\n",
    "                                                                          iteration * 100 / n_iterations_per_epoch,\n",
    "                                                                          acc * 100,\n",
    "                                                                          loss_train))\n",
    "            # At the end of each epoch,\n",
    "            # measure the validation loss and accuracy:\n",
    "            loss_vals = []\n",
    "            acc_vals = []\n",
    "            for iteration in range(1, n_iterations_validation + 1):\n",
    "                X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "                loss_val, acc_val = sess.run([loss, accuracy],\n",
    "                                             feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                        y: y_batch,\n",
    "                                                        prob: 1.0})\n",
    "                loss_vals.append(loss_val)\n",
    "                acc_vals.append(acc_val)\n",
    "                print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                          iteration, n_iterations_validation,\n",
    "                          iteration * 100 / n_iterations_validation),\n",
    "                      end=\" \" * 10)\n",
    "            loss_val = np.mean(loss_vals)\n",
    "            acc_val = np.mean(acc_vals)\n",
    "            print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "                        epoch + 1, acc_val * 100, loss_val, \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "            # And save the model if it improved:\n",
    "            if loss_val < best_loss_val:\n",
    "                save_path = saver.save(sess, checkpoint_path)\n",
    "                best_loss_val = loss_val\n",
    "\n",
    "                writer = tf.summary.FileWriter(summary_path, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from my_cnn/cnn.ckpt\n",
      "\n",
      "Evaluating the model: 0/312 (0.0%)          \n",
      "Evaluating the model: 1/312 (0.3%)          \n",
      "Evaluating the model: 2/312 (0.6%)          \n",
      "Evaluating the model: 3/312 (1.0%)          \n",
      "Evaluating the model: 4/312 (1.3%)          \n",
      "Evaluating the model: 5/312 (1.6%)          \n",
      "Evaluating the model: 6/312 (1.9%)          \n",
      "Evaluating the model: 7/312 (2.2%)          \n",
      "Evaluating the model: 8/312 (2.6%)          \n",
      "Evaluating the model: 9/312 (2.9%)          \n",
      "Evaluating the model: 10/312 (3.2%)          \n",
      "Evaluating the model: 11/312 (3.5%)          \n",
      "Evaluating the model: 12/312 (3.8%)          \n",
      "Evaluating the model: 13/312 (4.2%)          \n",
      "Evaluating the model: 14/312 (4.5%)          \n",
      "Evaluating the model: 15/312 (4.8%)          \n",
      "Evaluating the model: 16/312 (5.1%)          \n",
      "Evaluating the model: 17/312 (5.4%)          \n",
      "Evaluating the model: 18/312 (5.8%)          \n",
      "Evaluating the model: 19/312 (6.1%)          \n",
      "Evaluating the model: 20/312 (6.4%)          \n",
      "Evaluating the model: 21/312 (6.7%)          \n",
      "Evaluating the model: 22/312 (7.1%)          \n",
      "Evaluating the model: 23/312 (7.4%)          \n",
      "Evaluating the model: 24/312 (7.7%)          \n",
      "Evaluating the model: 25/312 (8.0%)          \n",
      "Evaluating the model: 26/312 (8.3%)          \n",
      "Evaluating the model: 27/312 (8.7%)          \n",
      "Evaluating the model: 28/312 (9.0%)          \n",
      "Evaluating the model: 29/312 (9.3%)          \n",
      "Evaluating the model: 30/312 (9.6%)          \n",
      "Evaluating the model: 31/312 (9.9%)          \n",
      "Evaluating the model: 32/312 (10.3%)          \n",
      "Evaluating the model: 33/312 (10.6%)          \n",
      "Evaluating the model: 34/312 (10.9%)          \n",
      "Evaluating the model: 35/312 (11.2%)          \n",
      "Evaluating the model: 36/312 (11.5%)          \n",
      "Evaluating the model: 37/312 (11.9%)          \n",
      "Evaluating the model: 38/312 (12.2%)          \n",
      "Evaluating the model: 39/312 (12.5%)          \n",
      "Evaluating the model: 40/312 (12.8%)          \n",
      "Evaluating the model: 41/312 (13.1%)          \n",
      "Evaluating the model: 42/312 (13.5%)          \n",
      "Evaluating the model: 43/312 (13.8%)          \n",
      "Evaluating the model: 44/312 (14.1%)          \n",
      "Evaluating the model: 45/312 (14.4%)          \n",
      "Evaluating the model: 46/312 (14.7%)          \n",
      "Evaluating the model: 47/312 (15.1%)          \n",
      "Evaluating the model: 48/312 (15.4%)          \n",
      "Evaluating the model: 49/312 (15.7%)          \n",
      "Evaluating the model: 50/312 (16.0%)          \n",
      "Evaluating the model: 51/312 (16.3%)          \n",
      "Evaluating the model: 52/312 (16.7%)          \n",
      "Evaluating the model: 53/312 (17.0%)          \n",
      "Evaluating the model: 54/312 (17.3%)          \n",
      "Evaluating the model: 55/312 (17.6%)          \n",
      "Evaluating the model: 56/312 (17.9%)          \n",
      "Evaluating the model: 57/312 (18.3%)          \n",
      "Evaluating the model: 58/312 (18.6%)          \n",
      "Evaluating the model: 59/312 (18.9%)          \n",
      "Evaluating the model: 60/312 (19.2%)          \n",
      "Evaluating the model: 61/312 (19.6%)          \n",
      "Evaluating the model: 62/312 (19.9%)          \n",
      "Evaluating the model: 63/312 (20.2%)          \n",
      "Evaluating the model: 64/312 (20.5%)          \n",
      "Evaluating the model: 65/312 (20.8%)          \n",
      "Evaluating the model: 66/312 (21.2%)          \n",
      "Evaluating the model: 67/312 (21.5%)          \n",
      "Evaluating the model: 68/312 (21.8%)          \n",
      "Evaluating the model: 69/312 (22.1%)          \n",
      "Evaluating the model: 70/312 (22.4%)          \n",
      "Evaluating the model: 71/312 (22.8%)          \n",
      "Evaluating the model: 72/312 (23.1%)          \n",
      "Evaluating the model: 73/312 (23.4%)          \n",
      "Evaluating the model: 74/312 (23.7%)          \n",
      "Evaluating the model: 75/312 (24.0%)          \n",
      "Evaluating the model: 76/312 (24.4%)          \n",
      "Evaluating the model: 77/312 (24.7%)          \n",
      "Evaluating the model: 78/312 (25.0%)          \n",
      "Evaluating the model: 79/312 (25.3%)          \n",
      "Evaluating the model: 80/312 (25.6%)          \n",
      "Evaluating the model: 81/312 (26.0%)          \n",
      "Evaluating the model: 82/312 (26.3%)          \n",
      "Evaluating the model: 83/312 (26.6%)          \n",
      "Evaluating the model: 84/312 (26.9%)          \n",
      "Evaluating the model: 85/312 (27.2%)          \n",
      "Evaluating the model: 86/312 (27.6%)          \n",
      "Evaluating the model: 87/312 (27.9%)          \n",
      "Evaluating the model: 88/312 (28.2%)          \n",
      "Evaluating the model: 89/312 (28.5%)          \n",
      "Evaluating the model: 90/312 (28.8%)          \n",
      "Evaluating the model: 91/312 (29.2%)          \n",
      "Evaluating the model: 92/312 (29.5%)          \n",
      "Evaluating the model: 93/312 (29.8%)          \n",
      "Evaluating the model: 94/312 (30.1%)          \n",
      "Evaluating the model: 95/312 (30.4%)          \n",
      "Evaluating the model: 96/312 (30.8%)          \n",
      "Evaluating the model: 97/312 (31.1%)          \n",
      "Evaluating the model: 98/312 (31.4%)          \n",
      "Evaluating the model: 99/312 (31.7%)          \n",
      "Evaluating the model: 100/312 (32.1%)          \n",
      "Evaluating the model: 101/312 (32.4%)          \n",
      "Evaluating the model: 102/312 (32.7%)          \n",
      "Evaluating the model: 103/312 (33.0%)          \n",
      "Evaluating the model: 104/312 (33.3%)          \n",
      "Evaluating the model: 105/312 (33.7%)          \n",
      "Evaluating the model: 106/312 (34.0%)          \n",
      "Evaluating the model: 107/312 (34.3%)          \n",
      "Evaluating the model: 108/312 (34.6%)          \n",
      "Evaluating the model: 109/312 (34.9%)          \n",
      "Evaluating the model: 110/312 (35.3%)          \n",
      "Evaluating the model: 111/312 (35.6%)          \n",
      "Evaluating the model: 112/312 (35.9%)          \n",
      "Evaluating the model: 113/312 (36.2%)          \n",
      "Evaluating the model: 114/312 (36.5%)          \n",
      "Evaluating the model: 115/312 (36.9%)          \n",
      "Evaluating the model: 116/312 (37.2%)          \n",
      "Evaluating the model: 117/312 (37.5%)          \n",
      "Evaluating the model: 118/312 (37.8%)          \n",
      "Evaluating the model: 119/312 (38.1%)          \n",
      "Evaluating the model: 120/312 (38.5%)          \n",
      "Evaluating the model: 121/312 (38.8%)          \n",
      "Evaluating the model: 122/312 (39.1%)          \n",
      "Evaluating the model: 123/312 (39.4%)          \n",
      "Evaluating the model: 124/312 (39.7%)          \n",
      "Evaluating the model: 125/312 (40.1%)          \n",
      "Evaluating the model: 126/312 (40.4%)          \n",
      "Evaluating the model: 127/312 (40.7%)          \n",
      "Evaluating the model: 128/312 (41.0%)          \n",
      "Evaluating the model: 129/312 (41.3%)          \n",
      "Evaluating the model: 130/312 (41.7%)          \n",
      "Evaluating the model: 131/312 (42.0%)          \n",
      "Evaluating the model: 132/312 (42.3%)          \n",
      "Evaluating the model: 133/312 (42.6%)          \n",
      "Evaluating the model: 134/312 (42.9%)          \n",
      "Evaluating the model: 135/312 (43.3%)          \n",
      "Evaluating the model: 136/312 (43.6%)          \n",
      "Evaluating the model: 137/312 (43.9%)          \n",
      "Evaluating the model: 138/312 (44.2%)          \n",
      "Evaluating the model: 139/312 (44.6%)          \n",
      "Evaluating the model: 140/312 (44.9%)          \n",
      "Evaluating the model: 141/312 (45.2%)          \n",
      "Evaluating the model: 142/312 (45.5%)          \n",
      "Evaluating the model: 143/312 (45.8%)          \n",
      "Evaluating the model: 144/312 (46.2%)          \n",
      "Evaluating the model: 145/312 (46.5%)          \n",
      "Evaluating the model: 146/312 (46.8%)          \n",
      "Evaluating the model: 147/312 (47.1%)          \n",
      "Evaluating the model: 148/312 (47.4%)          \n",
      "Evaluating the model: 149/312 (47.8%)          \n",
      "Evaluating the model: 150/312 (48.1%)          \n",
      "Evaluating the model: 151/312 (48.4%)          \n",
      "Evaluating the model: 152/312 (48.7%)          \n",
      "Evaluating the model: 153/312 (49.0%)          \n",
      "Evaluating the model: 154/312 (49.4%)          \n",
      "Evaluating the model: 155/312 (49.7%)          \n",
      "Evaluating the model: 156/312 (50.0%)          \n",
      "Evaluating the model: 157/312 (50.3%)          \n",
      "Evaluating the model: 158/312 (50.6%)          \n",
      "Evaluating the model: 159/312 (51.0%)          \n",
      "Evaluating the model: 160/312 (51.3%)          \n",
      "Evaluating the model: 161/312 (51.6%)          \n",
      "Evaluating the model: 162/312 (51.9%)          \n",
      "Evaluating the model: 163/312 (52.2%)          \n",
      "Evaluating the model: 164/312 (52.6%)          \n",
      "Evaluating the model: 165/312 (52.9%)          \n",
      "Evaluating the model: 166/312 (53.2%)          \n",
      "Evaluating the model: 167/312 (53.5%)          \n",
      "Evaluating the model: 168/312 (53.8%)          \n",
      "Evaluating the model: 169/312 (54.2%)          \n",
      "Evaluating the model: 170/312 (54.5%)          \n",
      "Evaluating the model: 171/312 (54.8%)          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model: 172/312 (55.1%)          \n",
      "Evaluating the model: 173/312 (55.4%)          \n",
      "Evaluating the model: 174/312 (55.8%)          \n",
      "Evaluating the model: 175/312 (56.1%)          \n",
      "Evaluating the model: 176/312 (56.4%)          \n",
      "Evaluating the model: 177/312 (56.7%)          \n",
      "Evaluating the model: 178/312 (57.1%)          \n",
      "Evaluating the model: 179/312 (57.4%)          \n",
      "Evaluating the model: 180/312 (57.7%)          \n",
      "Evaluating the model: 181/312 (58.0%)          \n",
      "Evaluating the model: 182/312 (58.3%)          \n",
      "Evaluating the model: 183/312 (58.7%)          \n",
      "Evaluating the model: 184/312 (59.0%)          \n",
      "Evaluating the model: 185/312 (59.3%)          \n",
      "Evaluating the model: 186/312 (59.6%)          \n",
      "Evaluating the model: 187/312 (59.9%)          \n",
      "Evaluating the model: 188/312 (60.3%)          \n",
      "Evaluating the model: 189/312 (60.6%)          \n",
      "Evaluating the model: 190/312 (60.9%)          \n",
      "Evaluating the model: 191/312 (61.2%)          \n",
      "Evaluating the model: 192/312 (61.5%)          \n",
      "Evaluating the model: 193/312 (61.9%)          \n",
      "Evaluating the model: 194/312 (62.2%)          \n",
      "Evaluating the model: 195/312 (62.5%)          \n",
      "Evaluating the model: 196/312 (62.8%)          \n",
      "Evaluating the model: 197/312 (63.1%)          \n",
      "Evaluating the model: 198/312 (63.5%)          \n",
      "Evaluating the model: 199/312 (63.8%)          \n",
      "Evaluating the model: 200/312 (64.1%)          \n",
      "Evaluating the model: 201/312 (64.4%)          \n",
      "Evaluating the model: 202/312 (64.7%)          \n",
      "Evaluating the model: 203/312 (65.1%)          \n",
      "Evaluating the model: 204/312 (65.4%)          \n",
      "Evaluating the model: 205/312 (65.7%)          \n",
      "Evaluating the model: 206/312 (66.0%)          \n",
      "Evaluating the model: 207/312 (66.3%)          \n",
      "Evaluating the model: 208/312 (66.7%)          \n",
      "Evaluating the model: 209/312 (67.0%)          \n",
      "Evaluating the model: 210/312 (67.3%)          \n",
      "Evaluating the model: 211/312 (67.6%)          \n",
      "Evaluating the model: 212/312 (67.9%)          \n",
      "Evaluating the model: 213/312 (68.3%)          \n",
      "Evaluating the model: 214/312 (68.6%)          \n",
      "Evaluating the model: 215/312 (68.9%)          \n",
      "Evaluating the model: 216/312 (69.2%)          \n",
      "Evaluating the model: 217/312 (69.6%)          \n",
      "Evaluating the model: 218/312 (69.9%)          \n",
      "Evaluating the model: 219/312 (70.2%)          \n",
      "Evaluating the model: 220/312 (70.5%)          \n",
      "Evaluating the model: 221/312 (70.8%)          \n",
      "Evaluating the model: 222/312 (71.2%)          \n",
      "Evaluating the model: 223/312 (71.5%)          \n",
      "Evaluating the model: 224/312 (71.8%)          \n",
      "Evaluating the model: 225/312 (72.1%)          \n",
      "Evaluating the model: 226/312 (72.4%)          \n",
      "Evaluating the model: 227/312 (72.8%)          \n",
      "Evaluating the model: 228/312 (73.1%)          \n",
      "Evaluating the model: 229/312 (73.4%)          \n",
      "Evaluating the model: 230/312 (73.7%)          \n",
      "Evaluating the model: 231/312 (74.0%)          \n",
      "Evaluating the model: 232/312 (74.4%)          \n",
      "Evaluating the model: 233/312 (74.7%)          \n",
      "Evaluating the model: 234/312 (75.0%)          \n",
      "Evaluating the model: 235/312 (75.3%)          \n",
      "Evaluating the model: 236/312 (75.6%)          \n",
      "Evaluating the model: 237/312 (76.0%)          \n",
      "Evaluating the model: 238/312 (76.3%)          \n",
      "Evaluating the model: 239/312 (76.6%)          \n",
      "Evaluating the model: 240/312 (76.9%)          \n",
      "Evaluating the model: 241/312 (77.2%)          \n",
      "Evaluating the model: 242/312 (77.6%)          \n",
      "Evaluating the model: 243/312 (77.9%)          \n",
      "Evaluating the model: 244/312 (78.2%)          \n",
      "Evaluating the model: 245/312 (78.5%)          \n",
      "Evaluating the model: 246/312 (78.8%)          \n",
      "Evaluating the model: 247/312 (79.2%)          \n",
      "Evaluating the model: 248/312 (79.5%)          \n",
      "Evaluating the model: 249/312 (79.8%)          \n",
      "Evaluating the model: 250/312 (80.1%)          \n",
      "Evaluating the model: 251/312 (80.4%)          \n",
      "Evaluating the model: 252/312 (80.8%)          \n",
      "Evaluating the model: 253/312 (81.1%)          \n",
      "Evaluating the model: 254/312 (81.4%)          \n",
      "Evaluating the model: 255/312 (81.7%)          \n",
      "Evaluating the model: 256/312 (82.1%)          \n",
      "Evaluating the model: 257/312 (82.4%)          \n",
      "Evaluating the model: 258/312 (82.7%)          \n",
      "Evaluating the model: 259/312 (83.0%)          \n",
      "Evaluating the model: 260/312 (83.3%)          \n",
      "Evaluating the model: 261/312 (83.7%)          \n",
      "Evaluating the model: 262/312 (84.0%)          \n",
      "Evaluating the model: 263/312 (84.3%)          \n",
      "Evaluating the model: 264/312 (84.6%)          \n",
      "Evaluating the model: 265/312 (84.9%)          \n",
      "Evaluating the model: 266/312 (85.3%)          \n",
      "Evaluating the model: 267/312 (85.6%)          \n",
      "Evaluating the model: 268/312 (85.9%)          \n",
      "Evaluating the model: 269/312 (86.2%)          \n",
      "Evaluating the model: 270/312 (86.5%)          \n",
      "Evaluating the model: 271/312 (86.9%)          \n",
      "Evaluating the model: 272/312 (87.2%)          \n",
      "Evaluating the model: 273/312 (87.5%)          \n",
      "Evaluating the model: 274/312 (87.8%)          \n",
      "Evaluating the model: 275/312 (88.1%)          \n",
      "Evaluating the model: 276/312 (88.5%)          \n",
      "Evaluating the model: 277/312 (88.8%)          \n",
      "Evaluating the model: 278/312 (89.1%)          \n",
      "Evaluating the model: 279/312 (89.4%)          \n",
      "Evaluating the model: 280/312 (89.7%)          \n",
      "Evaluating the model: 281/312 (90.1%)          \n",
      "Evaluating the model: 282/312 (90.4%)          \n",
      "Evaluating the model: 283/312 (90.7%)          \n",
      "Evaluating the model: 284/312 (91.0%)          \n",
      "Evaluating the model: 285/312 (91.3%)          \n",
      "Evaluating the model: 286/312 (91.7%)          \n",
      "Evaluating the model: 287/312 (92.0%)          \n",
      "Evaluating the model: 288/312 (92.3%)          \n",
      "Evaluating the model: 289/312 (92.6%)          \n",
      "Evaluating the model: 290/312 (92.9%)          \n",
      "Evaluating the model: 291/312 (93.3%)          \n",
      "Evaluating the model: 292/312 (93.6%)          \n",
      "Evaluating the model: 293/312 (93.9%)          \n",
      "Evaluating the model: 294/312 (94.2%)          \n",
      "Evaluating the model: 295/312 (94.6%)          \n",
      "Evaluating the model: 296/312 (94.9%)          \n",
      "Evaluating the model: 297/312 (95.2%)          \n",
      "Evaluating the model: 298/312 (95.5%)          \n",
      "Evaluating the model: 299/312 (95.8%)          \n",
      "Evaluating the model: 300/312 (96.2%)          \n",
      "Evaluating the model: 301/312 (96.5%)          \n",
      "Evaluating the model: 302/312 (96.8%)          \n",
      "Evaluating the model: 303/312 (97.1%)          \n",
      "Evaluating the model: 304/312 (97.4%)          \n",
      "Evaluating the model: 305/312 (97.8%)          \n",
      "Evaluating the model: 306/312 (98.1%)          \n",
      "Evaluating the model: 307/312 (98.4%)          \n",
      "Evaluating the model: 308/312 (98.7%)          \n",
      "Evaluating the model: 309/312 (99.0%)          \n",
      "Evaluating the model: 310/312 (99.4%)          \n",
      "Evaluating the model: 311/312 (99.7%)          \n",
      "Final test accuracy:  98.472446%  Loss: 0.051735\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "n_iterations_test = mnist.test.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    \n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    \n",
    "    for iteration in range(n_iterations_test + 1):\n",
    "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "        \n",
    "        loss_test, acc_test = sess.run([loss, accuracy],\n",
    "                                       feed_dict = {X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                                                    y: y_batch,\n",
    "                                                    prob: 1.0})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        \n",
    "        \n",
    "        print(\"\\nEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                    iteration, n_iterations_test, iteration * 100 / n_iterations_test), end=\" \" * 10)\n",
    "        \n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "        \n",
    "    print(\"\\rFinal test accuracy: {: 4f}%  Loss: {:.6f}\".format(acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
